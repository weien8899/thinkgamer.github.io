{"meta":{"title":"文艺与Code | Thinkgamer的博客","subtitle":"机器学习／python／云计算","description":"沈阳航空航天大学计算机院系，就职于京东，主要研究数据挖掘，机器学习，云计算。","author":"Thinkgamer","url":"http://yoursite.com"},"pages":[{"title":"","date":"2018-03-16T15:28:52.500Z","updated":"2018-03-16T15:28:52.500Z","comments":true,"path":"photos/lazyload.min.js","permalink":"http://yoursite.com/photos/lazyload.min.js","excerpt":"","text":""},{"title":"About Me","date":"2018-03-15T14:34:23.000Z","updated":"2018-03-17T16:28:47.487Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"My Name: Thinkgamer【高阳团】 个人信息性 别：男出生点：1993.05出生地：河南-平顶山-汝州毕业于：沈阳航空航天大学-计算机学院-软件工程就职于：京东 技术园地： CSDN：http://blog.csdn.net/gamer_gyt Github：https://github.com/Thinkgamer 知乎: https://www.zhihu.com/people/thinkgamer/activities 微博: https://weibo.com/234654758 简书: http://www.jianshu.com/u/3d4ea7705284 我的微信： 【加我微信，拉你进数据与算法交流群，每天都有技术讨论】 大学经历荣誉奖励 单项一等奖学金 * 2 综合二等奖学金 * 2 单项支援服务标兵 优秀团干 * 2 辽宁省ACM优秀志愿者 校ACM三等奖 沈阳航空航天大学计算机作品大赛二等奖【网站】 辽宁省计算机作品大赛二等奖【博客统计分析系统】 中国大学生作品大赛三等奖【博客统计分析系统】 工作经历 班级团支书 14级助理辅导员 院宣传部干事 活动部部长 实习/工作经历 北京广联达软件有限公司 北京万维星辰科技有限公司 北京京东尚科有限公司 微信扫码，关注微信公众号【数据与算法联盟】"},{"title":"商务合作介绍","date":"2018-03-15T14:34:23.000Z","updated":"2018-03-17T16:29:10.619Z","comments":true,"path":"cooperation/index.html","permalink":"http://yoursite.com/cooperation/index.html","excerpt":"WelCome To “Thinkgamer 小站” 合作范围 Web全栈 数据服务 模型构建 论文算法实现 大数据服务 跟拍摄影 广告接入 全网唯一ID：Thinkgamer，左侧”关于我“关注微信公众号”数据与算法联盟“，可在公众号添加我的微信，本人涉猎范围包括：Python，机器学习，Web开发，大数据云计算，ELK。","text":"WelCome To “Thinkgamer 小站” 合作范围 Web全栈 数据服务 模型构建 论文算法实现 大数据服务 跟拍摄影 广告接入 全网唯一ID：Thinkgamer，左侧”关于我“关注微信公众号”数据与算法联盟“，可在公众号添加我的微信，本人涉猎范围包括：Python，机器学习，Web开发，大数据云计算，ELK。 ▶ Web全栈 如果您在创业，苦于没有额外精力管理一个技术团队；如果您在工作，遇到了一些您解决不了的问题；如果您的网站苦于没有运维；如果您的数据需要备份；如果一切有关Web开发运维的问题。您都可以来找我，我虽不是最厉害的，但绝对会为您提供最优质的服务。 ▶ 数据服务 包含但不局限于以下数据相关的服务： 数据采集（一次性和程序开发） 数据清洗 数据可视化（不限于Web） 数据存储方案设计与实现 … … 本人曾多次向他人提供数据相关的技术服务，积累了一定的经验，相信能够为您提供全方位的数据服务。 ▶ 模型构建 根据对方提供的具体业务场景，进行相关模型选择与构建。当然，如果有荣幸参与您的场景选定和数据准备阶段，也是极好的。 ▶ 论文算法实现 如果您是一个马上要毕业的本科或者研究生，如果您苦于论文的立项与项目实现，如果您没有更好的主意，欢迎您来找我，加我的个人微信，为您的毕业保驾护航。 ▶ 大数据与分布式计算 提供大数据相关的服务，包含但不局限于： 大数据分析平台方案设计 大数据分析平台搭建 基于平台的数据分析Demo实现 海量数据的分布式计算处理 … … &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;欢迎来找我，24小时在线。 ▶ 广告接入 眼前的黑不是黑，Ta们说的白是什么白，也许一直是我们忘了搭一座桥，到对方的心里瞧一瞧。你的品牌，你的知名度为什么那么低，因为你没有使用我的广告接入，那么问题来了，包含但不局限于以下几种情况的，可以加我微信私聊了： 品牌宣传 广告位接入 公众号互相推广 个人网站/社区主页链接 … … ▶ 跟拍摄影 如果您在旅游途中缺少了一个摄影的小跟班；如果您苦于找不到好的角度拍照；如果您是一个人，苦于没有人照出你的美；如果您的照片需要美化与调整。那么请您来找我，保证为您提供最优质的技术与服务。 业务涉及： 跟拍摄影 照片美化与调整 PS技术服务 小本生意&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;诚信经验 大神勿扰&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;自求多福 微信扫码，关注微信公众号【数据与算法联盟】"},{"title":"相册","slug":"photos","date":"2018-03-16T15:28:52.660Z","updated":"2018-03-16T15:28:52.660Z","comments":false,"path":"photos/index.html","permalink":"http://yoursite.com/photos/index.html","excerpt":"","text":"图片来自Thinkgamer，正在加载中… (function() { var loadScript = function(path) { var $script = document.createElement('script') document.getElementsByTagName('body')[0].appendChild($script) $script.setAttribute('src', path) } setTimeout(function() { loadScript('./ins.js') }, 0) })()"},{"title":"","date":"2018-03-16T15:28:52.524Z","updated":"2018-03-16T15:28:52.524Z","comments":true,"path":"photos/ins.css","permalink":"http://yoursite.com/photos/ins.css","excerpt":"","text":"#post-instagram{ padding: 30px; } #post-instagram .article-entry{ padding-right: 0; } .instagram{ position: relative; min-height: 500px; } .instagram img { width: 100%; } .instagram .year { font-size: 16px; } .instagram .open-ins{ padding: 10px 0; color: #cdcdcd; } .instagram .open-ins:hover{ color: #657b83; } .instagram .year{ display: inline; } .instagram .thumb { width: 25%; height: 0; padding-bottom: 25%; position: relative; display: inline-block; text-align: center; background: #ededed; } .instagram .thumb a { position: relative; } .instagram .album h1 em{ font-style: normal; font-size: 14px; margin-left: 10px; } .instagram .album ul{ display: flex; flex-wrap: wrap; clear: both; width: 100%; text-align: left; } .instagram .album li{ list-style: none; display: inline-block; box-sizing: border-box; padding: 0 5px; margin-bottom: -10px; height: 0; width: 25%; position: relative; padding-bottom: 25%; } .instagram .album li:before{ display: none; } .instagram .album div.img-box{ position: absolute; width: 90%; height: 90%; -webkit-box-shadow: 0 1px 0 rgba(255,255,255,0.4), 0 1px 0 1px rgba(255,255,255,0.1); -moz-box-shadow: 0 1px 0 rgba(255,255,255,0.4), 0 1px 0 1px rgba(255,255,255,0.1); box-shadow: 0 1px 0 rgba(255,255,255,0.4), 0 1px 0 1px rgba(255,255,255,0.1); } .instagram .album div.img-box img{ width: 100%; height: 100%; position: absolute; z-index: 2; } .instagram .album div.img-box .img-bg{ position: absolute; top: 0; left: 0; bottom: 0px; width: 100%; margin: -5px; padding: 5px; -webkit-box-shadow: 0 0 0 1px rgba(0,0,0,.04), 0 1px 5px rgba(0,0,0,0.1); -moz-box-shadow: 0 0 0 1px rgba(0,0,0,.04), 0 1px 5px rgba(0,0,0,0.1); box-shadow: 0 0 0 1px rgba(0,0,0,.04), 0 1px 5px rgba(0,0,0,0.1); -webkit-transition: all 0.15s ease-out 0.1s; -moz-transition: all 0.15s ease-out 0.1s; -o-transition: all 0.15s ease-out 0.1s; transition: all 0.15s ease-out 0.1s; opacity: 0.2; cursor: pointer; display: block; z-index: 3; } .instagram .album div.img-box .icon { font-size: 14px; position: absolute; left: 50%; top: 50%; margin-left: -7px; margin-top: -7px; color: #999; z-index: 1; } .instagram .album div.img-box .img-bg:hover{ opacity: 0; } .photos-btn-wrap { border-bottom: 1px solid #e5e5e5; margin-bottom: 20px; } .photos-btn { font-size: 16px; color: #333; margin-bottom: -4px; padding: 5px 8px 3px; } .photos-btn.active { color: #08c; border: 1px solid #e5e5e5; border-bottom: 5px solid #fff; } @media screen and (max-width:600px) { .instagram .thumb { width: 50%; padding-bottom: 50%; } .instagram .album li { width: 100%; position: relative; padding-bottom: 100%; text-align: center; } .instagram .album div.img-box{ margin: 0; width: 90%; height: 90%; } }"},{"title":"","date":"2018-03-16T15:28:52.612Z","updated":"2018-03-16T15:28:52.612Z","comments":true,"path":"photos/ins.js","permalink":"http://yoursite.com/photos/ins.js","excerpt":"","text":"/******/ (function(modules) { // webpackBootstrap /******/ // The module cache /******/ var installedModules = {}; /******/ /******/ // The require function /******/ function __webpack_require__(moduleId) { /******/ /******/ // Check if module is in cache /******/ if (installedModules[moduleId]) /******/ return installedModules[moduleId].exports; /******/ /******/ // Create a new module (and put it into the cache) /******/ var module = installedModules[moduleId] = { /******/ exports: {}, /******/ id: moduleId, /******/ loaded: false /******/ }; /******/ /******/ // Execute the module function /******/ modules[moduleId].call(module.exports, module, module.exports, __webpack_require__); /******/ /******/ // Flag the module as loaded /******/ module.loaded = true; /******/ /******/ // Return the exports of the module /******/ return module.exports; /******/ } /******/ /******/ /******/ // expose the modules object (__webpack_modules__) /******/ __webpack_require__.m = modules; /******/ /******/ // expose the module cache /******/ __webpack_require__.c = installedModules; /******/ /******/ // __webpack_public_path__ /******/ __webpack_require__.p = \"/dist/\"; /******/ /******/ // Load entry module and return exports /******/ return __webpack_require__(0); /******/ }) /************************************************************************/ /******/ ([ /* 0 */ /***/ function(module, exports, __webpack_require__) { 'use strict'; __webpack_require__(1); var _view = __webpack_require__(2); var _view2 = _interopRequireDefault(_view); function _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; } /** * @name impush-client * @description 这个项目让我发家致富… * @date 2016-12-1 */ var _collection = []; var _count = 0; var searchData; function addMask(elem) { var rect = elem.getBoundingClientRect(); var style = getComputedStyle(elem, null); var mask = document.createElement('i'); mask.className = 'icon-film'; mask.style.color = '#fff'; mask.style.fontSize = '26px'; mask.style.position = 'absolute'; mask.style.right = '10px'; mask.style.bottom = '10px'; mask.style.zIndex = 1; elem.parentNode.appendChild(mask); } var createVideoIncon = function createVideoIncon() { var $videoImg = document.querySelectorAll('.thumb a[data-type=\"video\"]'); for (var i = 0, len = $videoImg.length; i < len; i++) { addMask($videoImg[i]); } }; var render = function render(res) { var ulTmpl = \"\"; for (var j = 0, len2 = res.list.length; j < len2; j++) { var data = res.list[j].arr; var liTmpl = \"\"; for (var i = 0, len = data.link.length; i < len; i++) { // var minSrc = 'https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/' + data.link[i] + '.min.jpg'; var minSrc = 'https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/' + data.link[i]; var src = 'https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/photos/' + data.link[i]; var type = data.type[i]; var target = src + (type === 'video' ? '.mp4' : '.jpg'); // src += '.jpg'; liTmpl += '\\ \\ \\ \\ ' + data.text[i] + '\\ '; } ulTmpl = ulTmpl + '' + data.year + '' + data.month + '月\\ ' + liTmpl + '\\ '; } document.querySelector('.instagram').innerHTML = '' + ulTmpl + ''; createVideoIncon(); _view2.default.init(); }; var replacer = function replacer(str) { var arr = str.split(\"/\"); return \"/assets/ins/\" + arr[arr.length - 1]; }; var ctrler = function ctrler(data) { var imgObj = {}; for (var i = 0, len = data.length; i < len; i++) { var y = data[i].y; var m = data[i].m; var src = replacer(data[i].src); var text = data[i].text; var key = y + \"\" + ((m + \"\").length == 1 ? \"0\" + m : m); if (imgObj[key]) { imgObj[key].srclist.push(src); imgObj[key].text.push(text); } else { imgObj[key] = { year: y, month: m, srclist: [src], text: [text] }; } } render(imgObj); }; function loadData(success) { if (!searchData) { var xhr = new XMLHttpRequest(); xhr.open('GET', './ins.json?t=' + +new Date(), true); xhr.onload = function() { if (this.status >= 200 && this.status < 300) { var res = JSON.parse(this.response); searchData = res; success(searchData); } else { console.error(this.statusText); } }; xhr.onerror = function() { console.error(this.statusText); }; xhr.send(); } else { success(searchData); } } var Ins = { init: function init() { loadData(function(data) { render(data); }); } }; Ins.init(); // export default impush; /***/ }, /* 1 */ /***/ function(module, exports, __webpack_require__) { /* WEBPACK VAR INJECTION */ (function(global) { 'use strict'; var inViewport = __webpack_require__(3); var lazyAttrs = ['data-src']; global.lzld = lazyload(); // Provide libs using getAttribute early to get the good src // and not the fake data-src replaceGetAttribute('Image'); replaceGetAttribute('IFrame'); function registerLazyAttr(attr) { if (indexOf.call(lazyAttrs, attr) === -1) { lazyAttrs.push(attr); } } function lazyload(opts) { opts = merge({ 'offset': 333, 'src': 'data-src', 'container': false }, opts || {}); if (typeof opts.src === 'string') { registerLazyAttr(opts.src); } var elts = []; function show(elt) { var src = findRealSrc(elt); if (src) { elt.src = src; } elt.setAttribute('data-lzled', true); elts[indexOf.call(elts, elt)] = null; } function findRealSrc(elt) { if (typeof opts.src === 'function') { return opts.src(elt); } return elt.getAttribute(opts.src); } function register(elt) { elt.onload = null; elt.removeAttribute('onload'); elt.onerror = null; elt.removeAttribute('onerror'); if (indexOf.call(elts, elt) === -1) { inViewport(elt, opts, show); } } return register; } function replaceGetAttribute(elementName) { var fullname = 'HTML' + elementName + 'Element'; if (fullname in global === false) { return; } var original = global[fullname].prototype.getAttribute; global[fullname].prototype.getAttribute = function(name) { if (name === 'src') { var realSrc; for (var i = 0, max = lazyAttrs.length; i < max; i++) { realSrc = original.call(this, lazyAttrs[i]); if (realSrc) { break; } } return realSrc || original.call(this, name); } // our own lazyloader will go through theses lines // because we use getAttribute(opts.src) return original.call(this, name); }; } function merge(defaults, opts) { for (var name in defaults) { if (opts[name] === undefined) { opts[name] = defaults[name]; } } return opts; } // http://webreflection.blogspot.fr/2011/06/partial-polyfills.html function indexOf(value) { for (var i = this.length; i-- && this[i] !== value;) {} return i; } module.exports = lazyload; // export default impush; /* WEBPACK VAR INJECTION */ }.call(exports, (function() { return this; }()))) /***/ }, /* 2 */ /***/ function(module, exports) { 'use strict'; var initPhotoSwipeFromDOM = function initPhotoSwipeFromDOM(gallerySelector) { // parse slide data (url, title, size ...) from DOM elements // (children of gallerySelector) var parseThumbnailElements = function parseThumbnailElements(el) { el = el.parentNode.parentNode; var thumbElements = el.getElementsByClassName('thumb'), numNodes = thumbElements.length, items = [], figureEl, linkEl, size, type, // video or not target, item; for (var i = 0; i < numNodes; i++) { figureEl = thumbElements[i]; // // include only element nodes if (figureEl.nodeType !== 1) { continue; } linkEl = figureEl.children[0]; // size = linkEl.getAttribute('data-size').split('x'); type = linkEl.getAttribute('data-type'); target = linkEl.getAttribute('data-target'); // create slide object item = { src: linkEl.getAttribute('href'), w: parseInt(size[0], 10), h: parseInt(size[1], 10) }; if (figureEl.children.length > 1) { item.title = figureEl.children[1].innerHTML; } if (linkEl.children.length > 0) { item.msrc = linkEl.children[0].getAttribute('src'); item.type = type; item.target = target; item.html = ''; if (type === 'video') { //item.src = null; } } item.el = figureEl; // save link to element for getThumbBoundsFn items.push(item); } return items; }; // find nearest parent element var closest = function closest(el, fn) { return el && (fn(el) ? el : closest(el.parentNode, fn)); }; // triggers when user clicks on thumbnail var onThumbnailsClick = function onThumbnailsClick(e) { e = e || window.event; e.preventDefault ? e.preventDefault() : e.returnValue = false; var eTarget = e.target || e.srcElement; // find root element of slide var clickedListItem = closest(eTarget, function(el) { return el.tagName && el.tagName.toUpperCase() === 'FIGURE'; }); if (!clickedListItem) { return; } // find index of clicked item by looping through all child nodes // alternatively, you may define index via data- attribute var clickedGallery = clickedListItem.parentNode, // childNodes = clickedListItem.parentNode.childNodes, // numChildNodes = childNodes.length, childNodes = document.getElementsByClassName('thumb'), numChildNodes = childNodes.length, nodeIndex = 0, index; for (var i = 0; i < numChildNodes; i++) { if (childNodes[i].nodeType !== 1) { continue; } if (childNodes[i] === clickedListItem) { index = nodeIndex; break; } nodeIndex++; } if (index >= 0) { // open PhotoSwipe if valid index found openPhotoSwipe(index, clickedGallery); } return false; }; // parse picture index and gallery index from URL (#&pid=1&gid=2) var photoswipeParseHash = function photoswipeParseHash() { var hash = window.location.hash.substring(1), params = {}; if (hash.length < 5) { return params; } var vars = hash.split('&'); for (var i = 0; i < vars.length; i++) { if (!vars[i]) { continue; } var pair = vars[i].split('='); if (pair.length < 2) { continue; } params[pair[0]] = pair[1]; } if (params.gid) { params.gid = parseInt(params.gid, 10); } return params; }; var openPhotoSwipe = function openPhotoSwipe(index, galleryElement, disableAnimation, fromURL) { var pswpElement = document.querySelectorAll('.pswp')[0], gallery, options, items; items = parseThumbnailElements(galleryElement); // define options (if needed) options = { // define gallery index (for URL) galleryUID: galleryElement.getAttribute('data-pswp-uid'), getThumbBoundsFn: function getThumbBoundsFn(index) { // See Options -> getThumbBoundsFn section of documentation for more info var thumbnail = items[index].el.getElementsByTagName('img')[0], // find thumbnail pageYScroll = window.pageYOffset || document.documentElement.scrollTop, rect = thumbnail.getBoundingClientRect(); return { x: rect.left, y: rect.top + pageYScroll, w: rect.width }; } }; // PhotoSwipe opened from URL if (fromURL) { if (options.galleryPIDs) { // parse real index when custom PIDs are used // http://photoswipe.com/documentation/faq.html#custom-pid-in-url for (var j = 0; j < items.length; j++) { if (items[j].pid == index) { options.index = j; break; } } } else { // in URL indexes start from 1 options.index = parseInt(index, 10) - 1; } } else { options.index = parseInt(index, 10); } // exit if index not found if (isNaN(options.index)) { return; } if (disableAnimation) { options.showAnimationDuration = 0; } // Pass data to PhotoSwipe and initialize it gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, options); gallery.init(); var $tempVideo; var stopVideoHandle = function stopVideoHandle() { if ($tempVideo) { $tempVideo.remove(); $tempVideo = null; } }; var changeHandle = function changeHandle() { var item = gallery.currItem; stopVideoHandle(); if (item.type === 'video') { var $ctn = item.container; var style = $ctn.getElementsByClassName('pswp__img')[0].style; var $video = document.createElement('video'); $video.setAttribute('autoplay', 'autoplay'); $video.setAttribute('controls', 'controls'); $video.setAttribute('src', item.target); $video.style.width = style.width; $video.style.height = style.height; $video.style.position = 'absolute'; $video.style.zIndex = 2; $tempVideo = $video; $ctn.appendChild($video); } }; gallery.listen('initialZoomIn', changeHandle); gallery.listen('afterChange', changeHandle); gallery.listen('initialZoomOut', stopVideoHandle); }; // loop through all gallery elements and bind events var galleryElements = document.querySelectorAll(gallerySelector); for (var i = 0, l = galleryElements.length; i < l; i++) { galleryElements[i].setAttribute('data-pswp-uid', i + 1); galleryElements[i].onclick = onThumbnailsClick; } // Parse URL and open gallery if it contains #&pid=3&gid=1 var hashData = photoswipeParseHash(); if (hashData.pid && hashData.gid) { openPhotoSwipe(hashData.pid, galleryElements[hashData.gid - 1], true, true); } }; var Viewer = function() { function init() { initPhotoSwipeFromDOM('.photos'); } return { init: init }; }(); module.exports = Viewer; /***/ }, /* 3 */ /***/ function(module, exports) { /* WEBPACK VAR INJECTION */ (function(global) { module.exports = inViewport; var instances = []; var supportsMutationObserver = typeof global.MutationObserver === 'function'; function inViewport(elt, params, cb) { var opts = { container: global.document.body, offset: 0 }; if (params === undefined || typeof params === 'function') { cb = params; params = {}; } var container = opts.container = params.container || opts.container; var offset = opts.offset = params.offset || opts.offset; for (var i = 0; i < instances.length; i++) { if (instances[i].container === container) { return instances[i].isInViewport(elt, offset, cb); } } return instances[ instances.push(createInViewport(container)) - 1 ].isInViewport(elt, offset, cb); } function addEvent(el, type, fn) { if (el.attachEvent) { el.attachEvent('on' + type, fn); } else { el.addEventListener(type, fn, false); } } function debounce(func, wait, immediate) { var timeout; return function() { var context = this, args = arguments; var callNow = immediate && !timeout; clearTimeout(timeout); timeout = setTimeout(later, wait); if (callNow) func.apply(context, args); function later() { timeout = null; if (!immediate) func.apply(context, args); } }; } // https://github.com/jquery/sizzle/blob/3136f48b90e3edc84cbaaa6f6f7734ef03775a07/sizzle.js#L708 var contains = function() { if (!global.document) { return true; } return global.document.documentElement.compareDocumentPosition ? function(a, b) { return !!(a.compareDocumentPosition(b) & 16); } : global.document.documentElement.contains ? function(a, b) { return a !== b && (a.contains ? a.contains(b) : false); } : function(a, b) { while (b = b.parentNode) { if (b === a) { return true; } } return false; }; } function createInViewport(container) { var watches = createWatches(); var scrollContainer = container === global.document.body ? global : container; var debouncedCheck = debounce(watches.checkAll(watchInViewport), 15); addEvent(scrollContainer, 'scroll', debouncedCheck); if (scrollContainer === global) { addEvent(global, 'resize', debouncedCheck); } if (supportsMutationObserver) { observeDOM(watches, container, debouncedCheck); } // failsafe check, every 200ms we check for visible images // usecase: a hidden parent containing eleements // when the parent becomes visible, we have no event that the children // became visible setInterval(debouncedCheck, 150); function isInViewport(elt, offset, cb) { if (!cb) { return isVisible(elt, offset); } var remote = createRemote(elt, offset, cb); remote.watch(); return remote; } function createRemote(elt, offset, cb) { function watch() { watches.add(elt, offset, cb); } function dispose() { watches.remove(elt); } return { watch: watch, dispose: dispose }; } function watchInViewport(elt, offset, cb) { if (isVisible(elt, offset)) { watches.remove(elt); cb(elt); } } function isVisible(elt, offset) { if (!contains(global.document.documentElement, elt) || !contains(global.document.documentElement, container)) { return false; } // Check if the element is visible // https://github.com/jquery/jquery/blob/740e190223d19a114d5373758127285d14d6b71e/src/css/hiddenVisibleSelectors.js if (!elt.offsetWidth || !elt.offsetHeight) { return false; } var eltRect = elt.getBoundingClientRect(); var viewport = {}; if (container === global.document.body) { viewport = { top: -offset, left: -offset, right: global.document.documentElement.clientWidth + offset, bottom: global.document.documentElement.clientHeight + offset }; } else { var containerRect = container.getBoundingClientRect(); viewport = { top: containerRect.top - offset, left: containerRect.left - offset, right: containerRect.right + offset, bottom: containerRect.bottom + offset }; } // The element must overlap with the visible part of the viewport var visible = ( (eltRect.right > viewport.left) && (eltRect.left < viewport.right) && (eltRect.bottom > viewport.top) && (eltRect.top < viewport.bottom) ); return visible; } return { container: container, isInViewport: isInViewport }; } function createWatches() { var watches = []; function add(elt, offset, cb) { if (!isWatched(elt)) { watches.push([elt, offset, cb]); } } function remove(elt) { var pos = indexOf(elt); if (pos !== -1) { watches.splice(pos, 1); } } function indexOf(elt) { for (var i = watches.length - 1; i >= 0; i--) { if (watches[i][0] === elt) { return i; } } return -1; } function isWatched(elt) { return indexOf(elt) !== -1; } function checkAll(cb) { return function() { for (var i = watches.length - 1; i >= 0; i--) { cb.apply(this, watches[i]); } }; } return { add: add, remove: remove, isWatched: isWatched, checkAll: checkAll }; } function observeDOM(watches, container, cb) { var observer = new MutationObserver(watch); var filter = Array.prototype.filter; var concat = Array.prototype.concat; observer.observe(container, { childList: true, subtree: true, // changes like style/width/height/display will be catched attributes: true }); function watch(mutations) { // some new DOM nodes where previously watched // we should check their positions if (mutations.some(knownNodes) === true) { setTimeout(cb, 0); } } function knownNodes(mutation) { var nodes = concat.call([], Array.prototype.slice.call(mutation.addedNodes), mutation.target ); return filter.call(nodes, watches.isWatched).length > 0; } } /* WEBPACK VAR INJECTION */ }.call(exports, (function() { return this; }()))) /***/ } /******/ ]);"},{"title":"","date":"2018-03-16T15:28:52.456Z","updated":"2018-03-16T15:28:52.456Z","comments":true,"path":"photos/ins.json","permalink":"http://yoursite.com/photos/ins.json","excerpt":"","text":"{\"list\":[{\"date\":\"2017-11\",\"arr\":{\"text\":[\"0009\",\"0008\",\"0006\",\"0010\"],\"type\":[\"image\",\"image\",\"image\",\"image\"],\"month\":11,\"link\":[\"2017-11-03_0009.jpg\",\"2017-11-03_0008.jpg\",\"2017-11-03_0006.jpg\",\"2017-11-03_0010.jpg\"],\"year\":2017}},{\"date\":\"2017-10\",\"arr\":{\"text\":[\"0003\",\"0000\",\"0001\",\"0005\",\"0004\"],\"type\":[\"image\",\"image\",\"image\",\"image\",\"image\"],\"month\":10,\"link\":[\"2017-10-22_0003.jpg\",\"2017-10-22_0000.jpg\",\"2017-10-22_0001.jpg\",\"2017-10-22_0005.jpg\",\"2017-10-22_0004.jpg\"],\"year\":2017}},{\"date\":\"2016-08\",\"arr\":{\"text\":[\"0259\",\"0331\",\"1407\"],\"type\":[\"image\",\"image\",\"image\"],\"month\":8,\"link\":[\"2016-08-24_0259.jpg\",\"2016-08-24_0331.jpg\",\"2016-08-24_1407.jpg\"],\"year\":2016}},{\"date\":\"2016-04\",\"arr\":{\"text\":[\"1319\"],\"type\":[\"image\"],\"month\":4,\"link\":[\"2016-04-04_1319.jpg\"],\"year\":2016}}]}"},{"title":"tags","date":"2017-01-17T13:39:14.000Z","updated":"2018-03-17T15:10:44.123Z","comments":true,"path":"tag/index.html","permalink":"http://yoursite.com/tag/index.html","excerpt":"","text":""}],"posts":[{"title":"机器学习中的AUC理解","slug":"机器学习/机器学习中的AUC理解","date":"2018-04-14T19:23:16.000Z","updated":"2018-04-14T19:26:02.655Z","comments":true,"path":"2018/04/15/机器学习/机器学习中的AUC理解/","link":"","permalink":"http://yoursite.com/2018/04/15/机器学习/机器学习中的AUC理解/","excerpt":"","text":"最近在做GBDT模型，里边用到胡模型评价方法就是AUC，刚好趁此机会，好好学习一下。 混淆矩阵(Confusion matrix)混淆矩阵是理解大多数评价指标的基础，毫无疑问也是理解AUC的基础。丰富的资料介绍着混淆矩阵的概念，下边用一个实例讲解什么是混淆矩阵 如有100个样本数据，这些数据分成2类，每类50个。分类结束后得到的混淆矩阵为： 说明：40个为0类别的，预测正确，60个事实是0类别的给预测为1类别的40个为1类别的，预测正确，60个事实是1类别的给预测为0类别的 其对应的混淆矩阵如下： 混淆矩阵包含四部分的信息： True negative(TN)，称为真阴率，表明实际是负样本预测成负样本的样本数 False positive(FP)，称为假阳率，表明实际是负样本预测成正样本的样本数 False negative(FN)，称为假阴率，表明实际是正样本预测成负样本的样本数 True positive(TP)，称为真阳率，表明实际是正样本预测成正样本的样本数 从上边的图可以分析出，对角线带True的为判断对了，斜对角线带False的为判断错了。 像常见的准确率，精准率，召回率，F1-score，AUC都是建立在混淆矩阵上的。 准确率（Accuracy）：判断正确的占总的数目的比例【（TN+TP）/100=(40+40)/100=0.8】精准率（precision）：判断正确的正类占进行判断的数目的比例（针对判别结果而言的，表示预测为正的数据中有多少是真的正确）【TP/(TP+FP) = 40/(40+60 )=0.4】召回率（recall）: 判断正确正类占应该应该判断正确的正类的比例（针对原样本而言，表示样本中的正例有多少被判断正确了）【TP/(TP+FN)=40/(40+60)=0.4】F1-Measure：精确值和召回率的调和均值【2RR/(P+R)=20.40.4/(0.4+0.4)=1】 AUC &amp; ROCAUC是一个模型评价指标，只能用于二分类模型的评价，对于二分类模型，还有损失函数（logloss），正确率（accuracy），准确率（precision），但相比之下AUC和logloss要比accuracy和precision用的多，原因是因为很多的机器学习模型计算结果都是概率的形式，那么对于概率而言，我们就需要去设定一个阈值来判定分类，那么这个阈值的设定就会对我们的正确率和准确率造成一定成都的影响。 AUC(Area under Curve)，表面上意思是曲线下边的面积，这么这条曲线是什么？——ROC曲线（receiver operating characteristic curve，接收者操作特征曲线）。 接下来分析下面这张图（图片来自百度百科）： X轴是假阳率：FP/(FP+TN)Y轴是真阳性：TP(TP+FN)ROC曲线给出的是当阈值(分类器必须提供每个样例被判为阳性或者阴性的可信程度值)变化时假阳率和真阳率的变化情况，左下角的点所对应的是将所有样例判为反例的情况，而右上角的点对应的则是将所有样例判断为正例的情况。ROC曲线不但可以用于比较分类器，还可以基于成本效益分析来做出决策。在理想情况下，最佳的分类器应该尽可能地处于左上角，这就意味着分类器在假阳率很低的同时获得了很高的真阳率。 AUC计算以下部分引用 ROC曲线与AUC计算 中的举例 假设有6次展示记录，有两次被点击了，得到一个展示序列（1:1,2:0,3:1,4:0,5:0,6:0），前面的表示序号，后面的表示点击（1）或没有点击（0）。然后在这6次展示的时候都通过model算出了点击的概率序列。 下面看三种情况。 一、如果概率的序列是（1:0.9,2:0.7,3:0.8,4:0.6,5:0.5,6:0.4）。与原来的序列一起，得到序列（从概率从高到低排）1 0.91 0.80 0.70 0.60 0.50 0.4绘制的步骤是：1）把概率序列从高到低排序，得到顺序（1:0.9,3:0.8,2:0.7,4:0.6,5:0.5,6:0.4）；2）从概率最大开始取一个点作为正类，取到点1，计算得到TPR=0.5，FPR=0.0；3）从概率最大开始，再取一个点作为正类，取到点3，计算得到TPR=1.0，FPR=0.0；4）再从最大开始取一个点作为正类，取到点2，计算得到TPR=1.0，FPR=0.25;5）以此类推，得到6对TPR和FPR。然后把这6对数据组成6个点(0,0.5),(0,1.0),(0.25,1),(0.5,1),(0.75,1),(1.0,1.0)。这6个点在二维坐标系中能绘出来。看看图中，那个就是ROC曲线。 二、如果概率的序列是（1:0.9,2:0.8,3:0.7,4:0.6,5:0.5,6:0.4）。与原来的序列一起，得到序列（从概率从高到低排）1 0.90 0.81 0.70 0.60 0.50 0.4绘制的步骤是：6）把概率序列从高到低排序，得到顺序（1:0.9,2:0.8,3:0.7,4:0.6,5:0.5,6:0.4）；7）从概率最大开始取一个点作为正类，取到点1，计算得到TPR=0.5，FPR=0.0；8）从概率最大开始，再取一个点作为正类，取到点2，计算得到TPR=0.5，FPR=0.25；9）再从最大开始取一个点作为正类，取到点3，计算得到TPR=1.0，FPR=0.25;10） 以此类推，得到6对TPR和FPR。然后把这6对数据组成6个点(0,0.5),(0.25,0.5),(0.25,1),(0.5,1),(0.75,1),(1.0,1.0)。这6个点在二维坐标系中能绘出来。看看图中，那个就是ROC曲线。 三、如果概率的序列是（1:0.4,2:0.6,3:0.5,4:0.7,5:0.8,6:0.9）。与原来的序列一起，得到序列（从概率从高到低排）0 0.90 0.80 0.70 0.61 0.51 0.4绘制的步骤是：11）把概率序列从高到低排序，得到顺序（1:0.4,2:0.6,3:0.5,4:0.7,5:0.8,6:0.9）；12）从概率最大开始取一个点作为正类，取到点6，计算得到TPR=0.0，FPR=0.25；13）从概率最大开始，再取一个点作为正类，取到点5，计算得到TPR=0.0，FPR=0.5；14）再从最大开始取一个点作为正类，取到点4，计算得到TPR=0.0，FPR=0.75;15）以此类推，得到6对TPR和FPR。然后把这6对数据组成6个点(0.25,0.0),(0.5,0.0),(0.75,0.0),(1.0,0.0),(1.0,0.5),(1.0,1.0)。这6个点在二维坐标系中能绘出来。看看图中，那个就是ROC曲线 意义：如上图的例子，总共6个点，2个正样本，4个负样本，取一个正样本和一个负样本的情况总共有8种。 上面的第一种情况，从上往下取，无论怎么取，正样本的概率总在负样本之上，所以分对的概率为1，AUC=1。再看那个ROC曲线，它的积分是什么？也是1，ROC曲线的积分与AUC相等。 上面第二种情况，如果取到了样本2和3，那就分错了，其他情况都分对了；所以分对的概率是0.875，AUC=0.875。再看那个ROC曲线，它的积分也是0.875，ROC曲线的积分与AUC相等。 上面的第三种情况，无论怎么取，都是分错的，所以分对的概率是0，AUC=0.0。再看ROC曲线，它的积分也是0.0，ROC曲线的积分与AUC相等。 很牛吧，其实AUC的意思是——Area Under roc Curve，就是ROC曲线的积分，也是ROC曲线下面的面积。 绘制ROC曲线的意义很明显，不断地把可能分错的情况扣除掉，从概率最高往下取的点，每有一个是负样本，就会导致分错排在它下面的所有正样本，所以要把它下面的正样本数扣除掉（1-TPR，剩下的正样本的比例）。总的ROC曲线绘制出来了，AUC就定了，分对的概率也能求出来了。 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"AUC","slug":"AUC","permalink":"http://yoursite.com/tags/AUC/"}]},{"title":"梯度提升决策树-GBDT","slug":"机器学习/梯度提升决策树-GBDT","date":"2018-04-11T16:43:51.000Z","updated":"2018-04-14T19:27:21.871Z","comments":true,"path":"2018/04/12/机器学习/梯度提升决策树-GBDT/","link":"","permalink":"http://yoursite.com/2018/04/12/机器学习/梯度提升决策树-GBDT/","excerpt":"","text":"研究GBDT的背景是业务中使用到了该模型，用于做推荐场景，当然这里就引出了GBDT的一个应用场景-回归，他的另外一个应用场景便是分类，接下来我会从以下几个方面去学习和研究GBDT的相关知识，当然我也是学习者，只是把我理解到的整理出来。本文参考了网上各路大神的笔记，在此感谢！ Boosting&amp;Bagging集成学习方法不是单独的一个机器学习算法，而是通过构建多个机器学习算法来达到一个强学习器。集成学习可以用来进行分类，回归，特征选取和异常点检测等。随机森林算法就是一个典型的集成学习方法，简单的说就是由一个个弱分类器（决策树）来构建一个强分类器，从而达到比较好的分类效果。 那么如何得到单个的学习器，一般有两种方法： 同质（对于一个强学习器而言，所用的单个弱学习器都是一样的，比如说用的都是决策树，或者都是神经网络） 异质（相对于同质而言，对于一个强学习器而言，所用的单个弱学习器不全是一样的，比如说用的决策树和神经网络的组合） 相对异质而言，同质学习期用的最为广泛，我们平时所讨论的集成学习方法指的就是同质个体学习器，同质个体学习器按照个体学习器之间的依赖关系分为串行（有强依赖关系）和并行（不存在关系或者有很弱的依赖关系），而在串行关系中有代表性的就是boosting系列算法，并行关系中具有代表性的就是bagging和随机森林（random forest） Boosting流程图 Bagging流程图 上边简单的介绍了集成学习方法和boosting&amp;bagging的区别，那么对于单个学习器采用何种策略才能得到一个强学习器呢？ 平均法（加权（个体学习器性能相差较大），简单（性能相近）） 投票法（绝对多数（超过半数标记。否则拒绝预测），相对多数，加权投票） 学习法（通过另一个学习器来进行结合，Stacking算法） Stacking算法：基本思想：先从初始数据集训练出初级学习器，然后生成一个新数据集用于训练次级学习器。在这个新数据集中，初级学习器的输出被当作样例输入特征，而出事样本的标记仍被当作样例标记。注意点：若直接用初级学习器的训练集来产生次级训练集，则过拟合风险会比较大；一般会通过交叉验证等方式，用训练初级学习器未使用的样本来产生次级学习器的训练样本。 Gradient BoostingGradient Boosting是一种Boosting的方法，它主要的思想是，每一次建立模型是在之前建立模型损失函数的梯度下降方向。损失函数是评价模型性能（一般为拟合程度+正则项），认为损失函数越小，性能越好。而让损失函数持续下降，就能使得模型不断改性提升性能，其最好的方法就是使损失函数沿着梯度方向下降（讲道理梯度方向上下降最快）。 Gradient Boost是一个框架，里面可以套入很多不同的算法。 分类树&amp;回归树&amp;分类回归树分类树三种比较常见的分类决策树分支划分方式包括：ID3, C4.5, CART。 以C4.5分类树为例，C4.5分类树在每次分枝时，是穷举每一个feature的每一个阈值，找到使得按照feature&lt;=阈值，和feature&gt;阈值分成的两个分枝的熵最大的阈值(熵最大的概念可理解成尽可能每个分枝的男女比例都远离1:1)，按照该标准分枝得到两个新节点，用同样方法继续分枝直到所有人都被分入性别唯一的叶子节点，或达到预设的终止条件，若最终叶子节点中的性别不唯一，则以多数人的性别作为该叶子节点的性别。 总结：分类树使用信息增益或增益比率来划分节点；每个节点样本的类别情况投票决定测试样本的类别。 回归树 回归树总体流程也是类似，区别在于，回归树的每个节点（不一定是叶子节点）都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化均方差即(每个人的年龄-预测年龄)^2 的总和 / N。也就是被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最可靠的分枝依据。分枝直到每个叶子节点上人的年龄都唯一或者达到预设的终止条件(如叶子个数上限)，若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。 总结：回归树使用最大均方差划分节点；每个节点样本的均值作为测试样本的回归预测值。 分类回归树 Classification And Regression Trees，即既能做分类任务又能做回归任务，CART也是决策树的一种，是一种二分决策树，但是也可以用来做回归，CART同决策树类似，不同于 ID3 与 C4.5 ,分类树采用基尼指数来选择最优的切分特征，而且每次都是二分。至于怎么利用基尼系数进行最优的特征切分，大家可以参考这篇文章的详细介绍 决策树之 CART 损失函数机器学习中的损失函数有很多，常见的有 0-1损失函数（0-1 loss function） L(Y,f(X))=\\left\\{ \\begin{aligned}&1,\\quad Y\\ne f(X)\\\\& 0,\\quad Y=f(X) \\end{aligned} \\right. 该损失函数的意义就是，当预测错误时，损失函数值为1，预测正确时，损失函数值为0。该损失函数不考虑预测值和真实值的误差程度，也就是只要预测错误，预测错误差一点和差很多是一样的。 平方损失函数（quadratic loss function） L(Y,f(X))=(Y-f(X))^2 取预测差距的平方 绝对值损失函数（absolute loss function） L(Y,f(X))=|Y-f(X)| 取预测值与真实值的差值绝对值，差距不会被平方放大 对数损失函数（logarithmic loss function） L(Y,P(Y|X))=-logP(Y|X) 该损失函数用到了极大似然估计的思想。P(Y|X)通俗的解释就是：在当前模型的基础上，对于样本X，其预测值为Y，也就是预测正确的概率。由于概率之间的同时满足需要使用乘法，为了将其转化为加法，我们将其取对数。最后由于是损失函数，所以预测正确的概率越高，其损失值应该是越小，因此再加个负号取个反。 全局损失函数 上面的损失函数仅仅是对于一个样本来说的。而我们的优化目标函数应当是使全局损失函数最小。因此，全局损失函数往往是每个样本的损失函数之和，即： J(w,b)=\\frac{1}{m} \\sum_{i=1}^m L(Y,f(X))对于平方损失函数，为了求导方便，我们可以在前面乘上一个1/2，和平方项求导后的2抵消，即： J(w,b)=\\frac{1}{2m} \\sum_{i=1}^m L(Y,f(X)) 逻辑回归中的损失函数 在逻辑回归中，我们采用的是对数损失函数。由于逻辑回归是服从伯努利分布(0-1分布)的，并且逻辑回归返回的sigmoid值是处于(0,1)区间，不会取到0,1两个端点。因此我们能够将其损失函数写成以下形式： L(\\hat y,y)=-(y\\log{\\hat y}+(1-y)\\log(1-\\hat y)) GBDT思想 以下部分学习于 GBDT算法原理深入解析 ，原文作者讲的很好，照搬过来，毕竟笔者不是推导数学公式的料，哈哈 GBDT 可以看成是由K棵树组成的加法模型： \\hat{y}_i=\\sum_{k=1}^K f_k(x_i), f_k \\in F \\tag 0其中F为所有树组成的函数空间，以回归任务为例，回归树可以看作为一个把特征向量映射为某个score的函数。该模型的参数为：$\\Theta=\\{f_1,f_2, \\cdots, f_K \\}$。于一般的机器学习算法不同的是，加法模型不是学习d维空间中的权重，而是直接学习函数（决策树）集合 上述加法模型的目标函数定义为：$Obj=\\sum_{i=1}^n l(y_i, \\hat{y}_i) + \\sum_{k=1}^K \\Omega(f_k)$，其中$\\Omega$表示决策树的复杂度，那么该如何定义树的复杂度呢？比如，可以考虑树的节点数量、树的深度或者叶子节点所对应的分数的L2范数等等。 如何来学习加法模型呢？ 解这一优化问题，可以用前向分布算法（forward stagewise algorithm）。因为学习的是加法模型，如果能够从前往后，每一步只学习一个基函数及其系数（结构），逐步逼近优化目标函数，那么就可以简化复杂度。这一学习过程称之为Boosting。具体地，我们从一个常量预测开始，每次学习一个新的函数，过程如下： \\begin{split} \\hat{y}_i^0 &= 0 \\\\ \\hat{y}_i^1 &= f_1(x_i) = \\hat{y}_i^0 + f_1(x_i) \\\\ \\hat{y}_i^2 &= f_1(x_i) + f_2(x_i) = \\hat{y}_i^1 + f_2(x_i) \\\\ & \\cdots \\\\ \\hat{y}_i^t &= \\sum_{k=1}^t f_k(x_i) = \\hat{y}_i^{t-1} + f_t(x_i) \\\\ \\end{split}那么，在每一步如何决定哪一个函数$f$被加入呢？指导原则还是最小化目标函数。在第$t$步，模型对$x_i$的预测为：$\\hat{y}_i^t= \\hat{y}_i^{t-1} + f_t(x_i)$，其中$f_t(x_i)$为这一轮我们要学习的函数（决策树）。这个时候目标函数可以写为： \\begin{split} Obj^{(t)} &= \\sum_{i=1}^nl(y_i, \\hat{y}_i^t) + \\sum_{i=i}^t \\Omega(f_i) \\\\ &= \\sum_{i=1}^n l\\left(y_i, \\hat{y}_i^{t-1} + f_t(x_i) \\right) + \\Omega(f_t) + constant \\end{split}\\tag{1}举例说明，假设损失函数为平方损失（square loss），则目标函数为： \\begin{split} Obj^{(t)} &= \\sum_{i=1}^n \\left(y_i - (\\hat{y}_i^{t-1} + f_t(x_i)) \\right)^2 + \\Omega(f_t) + constant \\\\ &= \\sum_{i=1}^n \\left[2(\\hat{y}_i^{t-1} - y_i)f_t(x_i) + f_t(x_i)^2 \\right] + \\Omega(f_t) + constant \\end{split}\\tag{2}其中，$(\\hat{y}_i^{t-1} - y_i)$称之为残差（residual）。因此，使用平方损失函数时，GBDT算法的每一步在生成决策树时只需要拟合前面的模型的残差。 泰勒公式：设$n$是一个正整数，如果定义在一个包含$a$的区间上的函数$f$在点$a$处$n+1$次可导，那么对于这个区间上的任意$x$都有：$\\displaystyle f(x)=\\sum _{n=0}^{N}\\frac{f^{(n)}(a)}{n!}(x-a)^ n+R_ n(x)$，其中的多项式称为函数在$a$处的泰勒展开式，$R_ n(x)$是泰勒公式的余项且是$(x-a)^ n$的高阶无穷小。 根据泰勒公式把函数$f(x+\\Delta x)$在点$x$处二阶展开，可得到如下等式： f(x+\\Delta x) \\approx f(x) + f'(x)\\Delta x + \\frac12 f''(x)\\Delta x^2 \\tag 3由等式(1)可知，目标函数是关于变量$\\hat{y}_i^{t-1} + f_t(x_i)$的函数，若把变量$\\hat{y}_i^{t-1}$看成是等式(3)中的$x$，把变量$f_t(x_i)$看成是等式(3)中的$\\Delta x$，则等式(1)可转化为： Obj^{(t)} = \\sum_{i=1}^n \\left[ l(y_i, \\hat{y}_i^{t-1}) + g_if_t(x_i) + \\frac12h_if_t^2(x_i) \\right] + \\Omega(f_t) + constant \\tag 4其中$g_i$，定义为损失函数的一阶导数，即$g_i=\\partial_{\\hat{y}^{t-1}}l(y_i,\\hat{y}^{t-1})$；$h_i$定义为损失函数的二阶导数，即$h_i=\\partial_{\\hat{y}^{t-1}}^2l(y_i,\\hat{y}^{t-1})$。假设损失函数为平方损失函数，则$g_i=\\partial_{\\hat{y}^{t-1}}(\\hat{y}^{t-1} - y_i)^2 = 2(\\hat{y}^{t-1} - y_i)$，$h_i=\\partial_{\\hat{y}^{t-1}}^2(\\hat{y}^{t-1} - y_i)^2 = 2$，把$g_i$和$h_i$代入等式(4)即得等式(2)。由于函数中的常量在函数最小化的过程中不起作用，因此我们可以从等式(4)中移除掉常量项，得： Obj^{(t)} \\approx \\sum_{i=1}^n \\left[ g_if_t(x_i) + \\frac12h_if_t^2(x_i) \\right] + \\Omega(f_t) \\tag 5由于要学习的函数仅仅依赖于目标函数，从等式(5)可以看出只需为学习任务定义好损失函数，并为每个训练样本计算出损失函数的一阶导数和二阶导数，通过在训练样本集上最小化等式(5)即可求得每步要学习的函数$f(x)$，从而根据加法模型等式(0)可得最终要学习的模型。 GBDT在Scikit-learn中的调用关于GBDT在Scikit-learn中的实现原文在 点击查看GBDT在sklearn中导入的包不一样，分类是 from sklearn.ensemble import GradientBoostingClassifier，回归是 from sklearn.ensemble import GradientBoostingRegressor 参数说明GBDT的参数分为boosting类库参数和弱学习器参数，其中有GBDT的弱学习器为CART，所以弱学习器参数基本为决策树的参数，参考点击阅读 类库参数 loss：损失函数，对于分类模型，有对数似然损失函数”deviance”和指数损失函数”exponential”两者输入选择。默认是对数似然损失函数”deviance”。在原理篇中对这些分类损失函数有详细的介绍。一般来说，推荐使用默认的”deviance”。它对二元分离和多元分类各自都有比较好的优化。而指数损失函数等于把我们带到了Adaboost算法。 learning_rate：即每个弱学习器的权重缩减系数νν，也称作步长。 n_estimators：就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。默认是100。在实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑。 subsample：子采样，取值是[0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间，默认是1.0，即不使用子采样。 init：即我们的初始化的时候的弱学习器，如果不输入，则用训练集样本来做样本集的初始化分类回归预测。否则用init参数提供的学习器做初始化分类回归预测。一般用在我们对数据有先验知识，或者之前做过一些拟合的时候，如果没有的话就不用管这个参数了。 verbose：默认是0，代表启用详细输出，若为1，代表偶尔输出进度信息 warm_start：默认为false random_state：如果int，random_state随机数生成器使用的种子；如果randomstate实例，random_state是随机数发生器；如果没有，随机数生成器使用的np.random的randomstate实例。 presort：默认情况下会在密集的数据上使用，默认是在稀疏数据正常排序。设置对true的稀疏数据将会引起错误。 决策树参数 max_depth：决策树最大深度 criterion：衡量分裂指标的度量方法，支持的是均方误差 min_samples_split：内部节点再划分所需最小样本数。这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。 min_samples_leaf：叶子节点最少样本数。这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。 min_weight_fraction_leaf：叶子节点最小的样本权重和。这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。 默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。 max_features：划分时考虑的最大特征数。可以使用很多种类型的值，默认是”None”,意味着划分时考虑所有的特征数；如果是”log2”意味着划分时最多考虑log2Nlog2N个特征；如果是”sqrt”或者”auto”意味着划分时最多考虑N−−√N个特征。如果是整数，代表考虑的特征绝对数。如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。其中N为样本总特征数。一般来说，如果样本特征数不多，比如小于50，我们用默认的”None”就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。 max_leaf_nodes：最大叶子节点数。通过限制最大叶子节点数，可以防止过拟合，默认是”None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。 min_impurity_split：节点划分最小不纯度。这个值限制了决策树的增长，如果某节点的不纯度(基于基尼系数，均方差)小于这个阈值，则该节点不再生成子节点。即为叶子节点 。一般不推荐改动默认值1e-7。 min_impurity_decrease：默认值为0。如果分裂 分类1234567891011&gt;&gt;&gt; from sklearn.datasets import make_hastie_10_2&gt;&gt;&gt; from sklearn.ensemble import GradientBoostingClassifier&gt;&gt;&gt; X, y = make_hastie_10_2(random_state=0)&gt;&gt;&gt; X_train, X_test = X[:2000], X[2000:]&gt;&gt;&gt; y_train, y_test = y[:2000], y[2000:]&gt;&gt;&gt; clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,... max_depth=1, random_state=0).fit(X_train, y_train)&gt;&gt;&gt; clf.score(X_test, y_test) 0.913... 回归123456789101112&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; from sklearn.metrics import mean_squared_error&gt;&gt;&gt; from sklearn.datasets import make_friedman1&gt;&gt;&gt; from sklearn.ensemble import GradientBoostingRegressor&gt;&gt;&gt; X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)&gt;&gt;&gt; X_train, X_test = X[:200], X[200:]&gt;&gt;&gt; y_train, y_test = y[:200], y[200:]&gt;&gt;&gt; est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,... max_depth=1, random_state=0, loss=&apos;ls&apos;).fit(X_train, y_train)&gt;&gt;&gt; mean_squared_error(y_test, est.predict(X_test)) 5.00... 模型评价123from sklearn import cross_validation, metricsmetrics.accuracy_score(y.values, y_pred) # 准确度metrics.roc_auc_score(y, y_predprob) # AUC大小 参考资料： 集成学习原理小结 Regression Tree 回归树 浅析机器学习中各种损失函数及其含义 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"GBDT","slug":"GBDT","permalink":"http://yoursite.com/tags/GBDT/"},{"name":"Boosting","slug":"Boosting","permalink":"http://yoursite.com/tags/Boosting/"},{"name":"Bagging","slug":"Bagging","permalink":"http://yoursite.com/tags/Bagging/"},{"name":"损失函数","slug":"损失函数","permalink":"http://yoursite.com/tags/损失函数/"}]},{"title":"Softmax-Regression","slug":"机器学习/Softmax-Regression","date":"2018-03-28T15:44:15.000Z","updated":"2018-03-28T16:06:10.665Z","comments":true,"path":"2018/03/28/机器学习/Softmax-Regression/","link":"","permalink":"http://yoursite.com/2018/03/28/机器学习/Softmax-Regression/","excerpt":"","text":"简介在本节中，我们介绍Softmax回归模型，该模型是logistic回归模型在多分类问题上的推广，在多分类问题中，类标签 \\textstyle y 可以取两个以上的值。 Softmax回归模型对于诸如MNIST手写数字分类等问题是很有用的，该问题的目的是辨识10个不同的单个数字。Softmax回归是有监督的，不过后面也会介绍它与深度学习/无监督学习方法的结合。（译者注： MNIST 是一个手写数字识别库，由NYU 的Yann LeCun 等人维护。http://yann.lecun.com/exdb/mnist/ ） 回想一下在 logistic 回归中，我们的训练集由 \\textstyle m 个已标记的样本构成：$ \\{ (x^{(1)}, y^{(1)}), \\ldots, (x^{(m)}, y^{(m)}) \\} $ ，其中输入特征$ x^{(i)} \\in \\Re^{n+1} $（我们对符号的约定如下：特征向量$ \\textstyle x $ 的维度为 \\textstyle n+1，其中 \\textstyle x_0 = 1 对应截距项 。） 由于 logistic 回归是针对二分类问题的，因此类标记 $ y^{(i)} \\in \\{0,1\\}$。假设函数(hypothesis function) 如下： \\begin{align} h_\\theta(x) = \\frac{1}{1+\\exp(-\\theta^Tx)}, \\end{align}我们将训练模型参数$ \\textstyle \\theta $，使其能够最小化代价函数 ： \\begin{align} J(\\theta) = -\\frac{1}{m} \\left[ \\sum_{i=1}^m y^{(i)} \\log h_\\theta(x^{(i)}) + (1-y^{(i)}) \\log (1-h_\\theta(x^{(i)})) \\right] \\end{align}在 softmax回归中，我们解决的是多分类问题（相对于 logistic 回归解决的二分类问题），类标$ \\textstyle y $可以取$ \\textstyle k $ 个不同的值（而不是 2 个）。因此，对于训练集$ \\{ (x^{(1)}, y^{(1)}), \\ldots, (x^{(m)}, y^{(m)}) \\} $，我们有$ y^{(i)} \\in \\{1, 2, \\ldots, k\\} $。（注意此处的类别下标从 1 开始，而不是 0）。例如，在 MNIST 数字识别任务中，我们有$ \\textstyle k=10 $个不同的类别。 对于给定的测试输入$ \\textstyle x$，我们想用假设函数针对每一个类别j估算出概率值$ \\textstyle p(y=j | x)$。也就是说，我们想估计$ \\textstyle x $ 的每一种分类结果出现的概率。因此，我们的假设函数将要输出一个 $ \\textstyle k $维的向量（向量元素的和为1）来表示这$ \\textstyle k$ 个估计的概率值。 具体地说，我们的假设函数$ \\textstyle h_{\\theta}(x) $ 形式如下： \\begin{align} h_\\theta(x^{(i)}) = \\begin{bmatrix} p(y^{(i)} = 1 | x^{(i)}; \\theta) \\\\ p(y^{(i)} = 2 | x^{(i)}; \\theta) \\\\ \\vdots \\\\ p(y^{(i)} = k | x^{(i)}; \\theta) \\end{bmatrix} = \\frac{1}{ \\sum_{j=1}^{k}{e^{ \\theta_j^T x^{(i)} }} } \\begin{bmatrix} e^{ \\theta_1^T x^{(i)} } \\\\ e^{ \\theta_2^T x^{(i)} } \\\\ \\vdots \\\\ e^{ \\theta_k^T x^{(i)} } \\\\ \\end{bmatrix} \\end{align}其中$ \\theta_1, \\theta_2, \\ldots, \\theta_k \\in \\Re^{n+1}$ 是模型的参数。请注意$ \\frac{1}{ \\sum_{j=1}^{k}{e^{ \\theta_j^T x^{(i)} }} } $这一项对概率分布进行归一化，使得所有概率之和为 1 。 为了方便起见，我们同样使用符号$ \\textstyle \\theta$ 来表示全部的模型参数。在实现Softmax回归时，将$ \\textstyle \\theta$ 用一个$ \\textstyle k \\times(n+1) $的矩阵来表示会很方便，该矩阵是将 $ \\theta_1, \\theta_2, \\ldots, \\theta_k $ 按行罗列起来得到的，如下所示： \\theta = \\begin{bmatrix} \\mbox{---} \\theta_1^T \\mbox{---} \\\\ \\mbox{---} \\theta_2^T \\mbox{---} \\\\ \\vdots \\\\ \\mbox{---} \\theta_k^T \\mbox{---} \\\\ \\end{bmatrix}代价函数现在我们来介绍 softmax 回归算法的代价函数。在下面的公式中，$\\textstyle 1\\{\\cdot\\}$ 是示性函数，其取值规则为：$ \\textstyle 1\\{ 值为真的表达式 \\textstyle \\}=1 $ ，$ \\textstyle 1\\{ 值为假的表达式 \\textstyle \\}=0 $。举例来说，表达式$ \\textstyle 1\\{2+2=4\\} $的值为1 ，$ \\textstyle 1\\{1+1=5\\} $的值为 0。我们的代价函数为： \\begin{align} J(\\theta) = - \\frac{1}{m} \\left[ \\sum_{i=1}^{m} \\sum_{j=1}^{k} 1\\left\\{y^{(i)} = j\\right\\} \\log \\frac{e^{\\theta_j^T x^{(i)}}}{\\sum_{l=1}^k e^{ \\theta_l^T x^{(i)} }}\\right] \\end{align}值得注意的是，上述公式是logistic回归代价函数的推广。logistic回归代价函数可以改为： \\begin{align} J(\\theta) &= -\\frac{1}{m} \\left[ \\sum_{i=1}^m (1-y^{(i)}) \\log (1-h_\\theta(x^{(i)})) + y^{(i)} \\log h_\\theta(x^{(i)}) \\right] \\\\ &= - \\frac{1}{m} \\left[ \\sum_{i=1}^{m} \\sum_{j=0}^{1} 1\\left\\{y^{(i)} = j\\right\\} \\log p(y^{(i)} = j | x^{(i)} ; \\theta) \\right] \\end{align}可以看到，Softmax代价函数与logistic 代价函数在形式上非常类似，只是在Softmax损失函数中对类标记的 $\\textstyle k $个可能值进行了累加。注意在Softmax回归中将 \\textstyle x 分类为类别 $\\textstyle j $的概率为： p(y^{(i)} = j | x^{(i)} ; \\theta) = \\frac{e^{\\theta_j^T x^{(i)}}}{\\sum_{l=1}^k e^{ \\theta_l^T x^{(i)}} } .对于$ \\textstyle J(\\theta)$ 的最小化问题，目前还没有闭式解法。因此，我们使用迭代的优化算法（例如梯度下降法，或 L-BFGS）。经过求导，我们得到梯度公式如下： \\begin{align} \\nabla_{\\theta_j} J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m}{ \\left[ x^{(i)} \\left( 1\\{ y^{(i)} = j\\} - p(y^{(i)} = j | x^{(i)}; \\theta) \\right) \\right] } \\end{align}让我们来回顾一下符号 “$ \\textstyle \\nabla_{\\theta_j}$” 的含义。$\\textstyle \\nabla_{\\theta_j} J(\\theta)$ 本身是一个向量，它的第$ \\textstyle l $个元素$ \\textstyle \\frac{\\partial J(\\theta)}{\\partial \\theta_{jl}} $是$ \\textstyle J(\\theta)对\\textstyle \\theta_j $的第 $\\textstyle l $个分量的偏导数。 有了上面的偏导数公式以后，我们就可以将它代入到梯度下降法等算法中，来最小化$ \\textstyle J(\\theta)$。 例如，在梯度下降法的标准实现中，每一次迭代需要进行如下更新: $\\textstyle \\theta_j := \\theta_j - \\alpha \\nabla_{\\theta_j} J(\\theta)(\\textstyle j=1,\\ldots,k）$。 当实现 softmax 回归算法时， 我们通常会使用上述代价函数的一个改进版本。具体来说，就是和权重衰减(weight decay)一起使用。我们接下来介绍使用它的动机和细节。 Softmax回归模型参数化的特点Softmax 回归有一个不寻常的特点：它有一个“冗余”的参数集。为了便于阐述这一特点，假设我们从参数向量 $\\textstyle \\theta_j $中减去了向量 $\\textstyle \\psi$，这时，每一个$ \\textstyle \\theta_j $都变成了 $\\textstyle \\theta_j - \\psi(\\textstyle j=1, \\ldots, k)$。此时假设函数变成了以下的式子： \\begin{align} p(y^{(i)} = j | x^{(i)} ; \\theta) &= \\frac{e^{(\\theta_j-\\psi)^T x^{(i)}}}{\\sum_{l=1}^k e^{ (\\theta_l-\\psi)^T x^{(i)}}} \\\\ &= \\frac{e^{\\theta_j^T x^{(i)}} e^{-\\psi^Tx^{(i)}}}{\\sum_{l=1}^k e^{\\theta_l^T x^{(i)}} e^{-\\psi^Tx^{(i)}}} \\\\ &= \\frac{e^{\\theta_j^T x^{(i)}}}{\\sum_{l=1}^k e^{ \\theta_l^T x^{(i)}}}. \\end{align}换句话说，从$ \\textstyle \\theta_j $中减去$ \\textstyle \\psi$ 完全不影响假设函数的预测结果！这表明前面的 softmax 回归模型中存在冗余的参数。更正式一点来说， Softmax 模型被过度参数化了。对于任意一个用于拟合数据的假设函数，可以求出多组参数值，这些参数得到的是完全相同的假设函数$ \\textstyle h_\\theta$。 进一步而言，如果参数 $\\textstyle (\\theta_1, \\theta_2,\\ldots, \\theta_k) $是代价函数$ \\textstyle J(\\theta)$ 的极小值点，那么 $\\textstyle (\\theta_1 - \\psi, \\theta_2 - \\psi,\\ldots,\\theta_k - \\psi) $同样也是它的极小值点，其中 $\\textstyle \\psi$ 可以为任意向量。因此使 $\\textstyle J(\\theta)$ 最小化的解不是唯一的。（有趣的是，由于$ \\textstyle J(\\theta)$ 仍然是一个凸函数，因此梯度下降时不会遇到局部最优解的问题。但是 Hessian 矩阵是奇异的/不可逆的，这会直接导致采用牛顿法优化就遇到数值计算的问题） 注意，当$ \\textstyle \\psi = \\theta_1$ 时，我们总是可以将 $\\textstyle \\theta_1$替换为$\\textstyle \\theta_1 - \\psi = \\vec{0}$（即替换为全零向量），并且这种变换不会影响假设函数。因此我们可以去掉参数向量 $\\textstyle \\theta_1 $（或者其他 $\\textstyle \\theta_j $中的任意一个）而不影响假设函数的表达能力。实际上，与其优化全部的$ \\textstyle k\\times(n+1) $个参数 $\\textstyle (\\theta_1, \\theta_2,\\ldots, \\theta_k)$ （其中 $\\textstyle \\theta_j \\in \\Re^{n+1}）$，我们可以令$ \\textstyle \\theta_1 =\\vec{0}$，只优化剩余$的 \\textstyle (k-1)\\times(n+1)$ 个参数，这样算法依然能够正常工作。 在实际应用中，为了使算法实现更简单清楚，往往保留所有参数$ \\textstyle (\\theta_1, \\theta_2,\\ldots, \\theta_n)$，而不任意地将某一参数设置为 0。但此时我们需要对代价函数做一个改动：加入权重衰减。权重衰减可以解决 softmax 回归的参数冗余所带来的数值问题。 权重衰减我们通过添加一个权重衰减项$\\textstyle \\frac{\\lambda}{2} \\sum_{i=1}^k \\sum_{j=0}^{n} \\theta_{ij}^2$ 来修改代价函数，这个衰减项会惩罚过大的参数值，现在我们的代价函数变为： \\begin{align} J(\\theta) = - \\frac{1}{m} \\left[ \\sum_{i=1}^{m} \\sum_{j=1}^{k} 1\\left\\{y^{(i)} = j\\right\\} \\log \\frac{e^{\\theta_j^T x^{(i)}}}{\\sum_{l=1}^k e^{ \\theta_l^T x^{(i)} }} \\right] + \\frac{\\lambda}{2} \\sum_{i=1}^k \\sum_{j=0}^n \\theta_{ij}^2 \\end{align}有了这个权重衰减项以后 ($\\textstyle \\lambda &gt; 0$)，代价函数就变成了严格的凸函数，这样就可以保证得到唯一的解了。 此时的 Hessian矩阵变为可逆矩阵，并且因为$\\textstyle J(\\theta)$是凸函数，梯度下降法和 L-BFGS 等算法可以保证收敛到全局最优解。 为了使用优化算法，我们需要求得这个新函数$ \\textstyle J(\\theta)$ 的导数，如下： \\begin{align} \\nabla_{\\theta_j} J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m}{ \\left[ x^{(i)} ( 1\\{ y^{(i)} = j\\} - p(y^{(i)} = j | x^{(i)}; \\theta) ) \\right] } + \\lambda \\theta_j \\end{align} 通过最小化 \\textstyle J(\\theta)，我们就能实现一个可用的 softmax 回归模型。 # Softmax回归与Logistic 回归的关系 当类别数$ \\textstyle k = 2 $时，softmax 回归退化为 logistic 回归。这表明 softmax 回归是 logistic 回归的一般形式。具体地说，当 $\\textstyle k = 2 $时，softmax 回归的假设函数为：\\begin{align}h_\\theta(x) &amp;=\\frac{1}{ e^{\\theta_1^Tx} + e^{ \\theta_2^T x^{(i)} } }\\begin{bmatrix}e^{ \\theta_1^T x } \\\\e^{ \\theta_2^T x }\\end{bmatrix}\\end{align} 利用softmax回归参数冗余的特点，我们令$ \\textstyle \\psi = \\theta_1$，并且从两个参数向量中都减去向量$ \\textstyle \\theta_1$，得到:\\begin{align}h(x) &amp;=\\frac{1}{ e^{\\vec{0}^Tx} + e^{ (\\theta_2-\\theta_1)^T x^{(i)} } }\\begin{bmatrix}e^{ \\vec{0}^T x } \\\\e^{ (\\theta_2-\\theta_1)^T x }\\end{bmatrix} \\\\&amp;=\\begin{bmatrix}\\frac{1}{ 1 + e^{ (\\theta_2-\\theta_1)^T x^{(i)} } } \\\\\\frac{e^{ (\\theta_2-\\theta_1)^T x }}{ 1 + e^{ (\\theta_2-\\theta_1)^T x^{(i)} } }\\end{bmatrix} \\\\&amp;=\\begin{bmatrix}\\frac{1}{ 1 + e^{ (\\theta_2-\\theta_1)^T x^{(i)} } } \\\\1 - \\frac{1}{ 1 + e^{ (\\theta_2-\\theta_1)^T x^{(i)} } } \\\\\\end{bmatrix}\\end{align} $$因此，用$ \\textstyle \\theta$’来表示$\\textstyle \\theta_2-\\theta_1$，我们就会发现 softmax 回归器预测其中一个类别的概率为 $\\textstyle \\frac{1}{ 1 + e^{ (\\theta’)^T x^{(i)} } }$，另一个类别概率的为$ \\textstyle 1 - \\frac{1}{ 1 + e^{ (\\theta’)^T x^{(i)} } }$，这与 logistic回归是一致的。 Softmax 回归 vs. k 个二元分类器如果你在开发一个音乐分类的应用，需要对k种类型的音乐进行识别，那么是选择使用 softmax 分类器呢，还是使用 logistic 回归算法建立 k 个独立的二元分类器呢？ 这一选择取决于你的类别之间是否互斥，例如，如果你有四个类别的音乐，分别为：古典音乐、乡村音乐、摇滚乐和爵士乐，那么你可以假设每个训练样本只会被打上一个标签（即：一首歌只能属于这四种音乐类型的其中一种），此时你应该使用类别数 k = 4 的softmax回归。（如果在你的数据集中，有的歌曲不属于以上四类的其中任何一类，那么你可以添加一个“其他类”，并将类别数 k 设为5。） 如果你的四个类别如下：人声音乐、舞曲、影视原声、流行歌曲，那么这些类别之间并不是互斥的。例如：一首歌曲可以来源于影视原声，同时也包含人声 。这种情况下，使用4个二分类的 logistic 回归分类器更为合适。这样，对于每个新的音乐作品 ，我们的算法可以分别判断它是否属于各个类别。 现在我们来看一个计算视觉领域的例子，你的任务是将图像分到三个不同类别中。(i) 假设这三个类别分别是：室内场景、户外城区场景、户外荒野场景。你会使用sofmax回归还是 3个logistic 回归分类器呢？ (ii) 现在假设这三个类别分别是室内场景、黑白图片、包含人物的图片，你又会选择 softmax 回归还是多个 logistic 回归分类器呢？ 在第一个例子中，三个类别是互斥的，因此更适于选择softmax回归分类器 。而在第二个例子中，建立三个独立的 logistic回归分类器更加合适。 中英文对照 Softmax回归 Softmax Regression 有监督学习 supervised learning 无监督学习 unsupervised learning 深度学习 deep learning logistic回归 logistic regression 截距项 intercept term 二元分类 binary classification 类型标记 class labels 估值函数/估计值 hypothesis 代价函数 cost function 多元分类 multi-class classification 权重衰减 weight decay 原文链接：http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92英文链接：http://ufldl.stanford.edu/wiki/index.php/Softmax_Regression 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"}]},{"title":"推荐系统的一些思考","slug":"机器学习/推荐系统的一些思考","date":"2018-03-25T21:55:44.000Z","updated":"2018-04-14T19:32:51.039Z","comments":true,"path":"2018/03/26/机器学习/推荐系统的一些思考/","link":"","permalink":"http://yoursite.com/2018/03/26/机器学习/推荐系统的一些思考/","excerpt":"","text":"推荐系统一直以来都是电商网站必不可少的一项，在提升用户转化，增加GMV方面可谓功不可没，那么一个好的推荐算法必然会创造更大的价值，刚好最近听了一个关于推荐算法的讲座，写出来一些思考吧，算是分享一下。 学术界的推荐系统其实在大学期间也看过一些推荐的算法，还帮别人实现过关于推荐系统的毕设，但终究都是停留在协同过滤的层面，顶多是加了一些热门推荐来防止冷启动。不得不说，协同过滤打开了我对推荐系统认知的大门，当然在真实环境中这是远远不够的。 传统的推荐系统无非就是评分和排序两种方案，评分即计算出用户对item的可能评分，根据评分的高低进行排序，排序则不关心具体的评分是多少，只是为了得到一个顺序（其实这一点和推荐系统即为相似）。传统的推荐算法典型的有协同过滤和基于内容的过滤。如果你不明白什么是协同过滤算法可以参考：https://blog.csdn.net/gamer_gyt/article/details/51346159 ,如果你不知道协同过滤与基于内容的过滤的区别可参考：https://www.zhihu.com/question/19971859。 大学数学术界关于推荐算法的论文都是对协同过滤的改进，而最终得到一个相对于原先的算法有很大的提升的结果，但是这些都过于理想化了，真实的环境远比实验要复杂的多，网上最有名的推荐系统数据集莫过于那个电影评分数据了，里边只有用户对电影的评分，和电影的一些信息数据。建立在这些数据上的推荐算法其实有点理想化了，他并不能模拟出真实的电商环境，数据的缺乏也是导致协同过滤算法大行其道的原因。 工业界的推荐系统工业界的推荐系统，需要的是明确的价值走向，比如说电商网站的推荐系统是为了增加交易总额，那么在进行推荐的时候是不是应该适当过滤一些极其廉价的商品，是否应该根据用户对不同价格段的需求进行不同价格段商品的推送；如果电商的推荐系统是为了增加用户停留时间，提交CTR，那么推荐系统就不应该考虑过多别的因素，只找到用户最感兴趣的商品或者评论等，当然如何找到用户最感兴趣的也是一个问题，但是有一点不可否认的是这些如果用传统的协同过滤来做是很难满足需求的。这时候就需要开发出新的推荐架构，来适应不同的需求。 目前工业界用的最多的算法莫过于GBDT，LR，DNN等，但所有的推荐算法都会面临一个海量数据的情况，这个时候的做法便是对数据集进行数据召回，得到用户比较感兴趣的一些数据，然后再根据我们的推荐模型进行素材偏好度排序，过滤掉用户已经购买过的类别数据，继而推送给用户。 那么如何进行数据召回呢？这就需要一些基础的模型进行数据准备，比如说用户肖像，用户的价格段偏好，用户购买力水平等。根绝已有的用户数据和特征进行数据召回，适度拉取一些新的数据，保证召回结果的多样性，得到召回池数据之后，便是对模型的训练，其实模型本身难度不大，难度大的是如何选取有效的特征来作为模型的输入数据。你组合得到的有效特征越多，对于模型的训练结果就越准确。 推荐系统的多样性如何保证推荐结果的多样性呢，首先我们要先认识到推荐的可能性，比如说电商网页首页的推荐，商品详细页面的推荐，不同年龄下的推荐，推荐的结果和被评估的指标都是不一样的。这个时候不能单一对所有情况下使用同一种算法或者特征，而是要找到能够区分出不同位置，不同年龄的推荐结果的特征，进行模型训练。 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"推荐系统","slug":"推荐系统","permalink":"http://yoursite.com/tags/推荐系统/"}]},{"title":"Scala解析XML","slug":"Spark/Scala解析XML","date":"2018-02-04T08:32:45.000Z","updated":"2018-04-14T19:38:44.711Z","comments":true,"path":"2018/02/04/Spark/Scala解析XML/","link":"","permalink":"http://yoursite.com/2018/02/04/Spark/Scala解析XML/","excerpt":"在使用Spark时，有时候主函数入口参数过多的时候，会特别复杂，这个时候我们可以将相应的参数写在xml文件中，然后只要将xml文件的路径传进去即可，这里的xml路径可以是本地的，也可以是hdfs上的。","text":"在使用Spark时，有时候主函数入口参数过多的时候，会特别复杂，这个时候我们可以将相应的参数写在xml文件中，然后只要将xml文件的路径传进去即可，这里的xml路径可以是本地的，也可以是hdfs上的。 scala提供了类似于Xpath的语法来解析xml文件，其中很重要的两个操作符是”\\”和 “\\\\” \\ ：根据搜索条件得到下一个节点 \\\\：根据条件获取所有的节点 12345678910&lt;configure&gt; &lt;input&gt; &lt;name&gt;app_feature_goods&lt;/name&gt; &lt;hdfs&gt;/user/path/to/goods&lt;/hdfs&gt; &lt;/input&gt; &lt;input&gt; &lt;name&gt;app_feature_user&lt;/name&gt; &lt;hdfs&gt;/user/path/to/user&lt;/hdfs&gt; &lt;/input&gt;&lt;/configure&gt; 123456789101112131415161718192021222324252627282930val input = args(0)val xml = XML.load(input)// 找到所有的一级节点 inputval input_list = xml\\&quot;input&quot;input_list.foreach(println)// 遍历每个一级节点，得到具体的值for(one &lt;- input_list)&#123; println(one\\&quot;name&quot;) println((one\\&quot;name&quot;).text) println(one\\&quot;hdfs&quot;) println((one\\&quot;hdfs&quot;).text)&#125;// 得到所有的nameval name_list = xml\\\\&quot;name&quot;name_list.map(one =&gt; one.text).foreach(println)// 获取所有hdfsval hdfs_list = xml\\\\&quot;hdfs&quot;hdfs_list.map(one =&gt; one.text).foreach(println)// 获取具有class的值println(xml\\&quot;input&quot;\\&quot;name&quot;\\\\&quot;@class&quot;)// 打印出具有class属性的name值和hdfs值println((xml\\\\&quot;name&quot;).filter(_.attribute(&quot;class&quot;).exists(_.text.equals(&quot;test&quot;))).text)println((xml\\\\&quot;hdfs&quot;).filter(_.attribute(&quot;class&quot;).exists(_.text.equals(&quot;test&quot;))).text) 打印出的信息为：1234567891011121314151617181920212223242526272829303132333435363738394041&lt;input&gt; &lt;name&gt;app_feature_goods&lt;/name&gt; &lt;hdfs&gt;/user/path/to/goods&lt;/hdfs&gt; &lt;/input&gt;&lt;input&gt; &lt;name&gt;app_feature_user&lt;/name&gt; &lt;hdfs&gt;/user/path/to/user&lt;/hdfs&gt; &lt;/input&gt;&lt;input&gt; &lt;name class=&quot;test&quot;&gt;app_feature_user_test&lt;/name&gt; &lt;hdfs class=&quot;test&quot;&gt;/user/path/to/user_test&lt;/hdfs&gt; &lt;/input&gt;-------------&lt;name&gt;app_feature_goods&lt;/name&gt;app_feature_goods&lt;hdfs&gt;/user/path/to/goods&lt;/hdfs&gt;/user/path/to/goods&lt;name&gt;app_feature_user&lt;/name&gt;app_feature_user&lt;hdfs&gt;/user/path/to/user&lt;/hdfs&gt;/user/path/to/user&lt;name class=&quot;test&quot;&gt;app_feature_user_test&lt;/name&gt;app_feature_user_test&lt;hdfs class=&quot;test&quot;&gt;/user/path/to/user_test&lt;/hdfs&gt;/user/path/to/user_test-------------app_feature_goodsapp_feature_userapp_feature_user_test-------------/user/path/to/goods/user/path/to/user/user/path/to/user_test-------------test-------------app_feature_user_test/user/path/to/user_test-------------Process finished with exit code 0 当然还存在一种情况就是XML文件存在于hdfs之上，这时候就不能直接load xml文件里，不过可以通过下面一种方法获得12var rdd = sc.textFile(xml_path)val xml = XML.loadString(rdd.collect().mkString(&quot;\\n&quot;)) 接下来便可以通过上边的方法进行解析了。 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"数据计算","slug":"数据计算","permalink":"http://yoursite.com/categories/数据计算/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"}]},{"title":"Spark求统计量的两种方法","slug":"Spark/Spark求统计量的两种方法","date":"2018-02-04T07:04:27.000Z","updated":"2018-04-14T19:38:55.555Z","comments":true,"path":"2018/02/04/Spark/Spark求统计量的两种方法/","link":"","permalink":"http://yoursite.com/2018/02/04/Spark/Spark求统计量的两种方法/","excerpt":"Spark对于统计量中的最大值，最小值，平均值和方差（均值）的计算都提供了封装，这里小编知道两种计算方法，整理一下分享给大家","text":"Spark对于统计量中的最大值，最小值，平均值和方差（均值）的计算都提供了封装，这里小编知道两种计算方法，整理一下分享给大家 DataFrame形式加载Json数据源example.json文件格式如下123&#123;&quot;name&quot;:&quot;thinkgamer&quot;,&quot;age&quot;:23,&quot;math&quot;:78,&quot;chinese&quot;:78,&quot;english&quot;:95&#125;&#123;&quot;name&quot;:&quot;think&quot;,&quot;age&quot;:25,&quot;math&quot;:95,&quot;chinese&quot;:88,&quot;english&quot;:93&#125;&#123;&quot;name&quot;:&quot;gamer&quot;,&quot;age&quot;:24,&quot;math&quot;:93,&quot;chinese&quot;:68,&quot;english&quot;:88&#125; 1234// persist(StorageLevel.MEMORY_AND_DISK) 当内存不够时cache到磁盘里val df = spark.read.json(&quot;/path/to/example.json&quot;).persist(StorageLevel.MEMORY_AND_DISK)df.show()df.describe() 我们便可以看到如下的形式1234567891011121314151617+---+-------+-------+----+----------+|age|chinese|english|math| name|+---+-------+-------+----+----------+| 23| 78| 95| 78|thinkgamer|| 25| 88| 93| 95| think|| 24| 68| 88| 93| gamer|+---+-------+-------+----+----------++-------+----+-------+-----------------+-----------------+----------+|summary| age|chinese| english| math| name|+-------+----+-------+-----------------+-----------------+----------+| count| 3| 3| 3| 3| 3|| mean|24.0| 78.0| 92.0|88.66666666666667| null|| stddev| 1.0| 10.0|3.605551275463989| 9.29157324317757| null|| min| 23| 68| 88| 78| gamer|| max| 25| 88| 95| 95|thinkgamer|+-------+----+-------+-----------------+-----------------+----------+ 如果是想看某列的通知值的话，可以用下面的方式1df.select(&quot;age&quot;).describe().show() 123456789+-------+----+|summary| age|+-------+----+| count| 3|| mean|24.0|| stddev| 1.0|| min| 23|| max| 25|+-------+----+ RDD形式假设同样还是上边的数据，只不过现在变成按\\t分割的普通文本123thinkgamer 23 78 78 95think 25 95 88 93gamer 24 93 68 88 这里可以将rdd转换成dataframe洗形式，也可以使用rdd计算，转化为df的样例如下1234val new_data = data_txt .map(_.split(&quot;\\\\s+&quot;)) .map(one =&gt; Person(one(0),one(1).toInt,one(2).toDouble,one(3).toDouble,one(4).toDouble)) .toDF() 接下来就是进行和上边df一样的操作了。 那么对于rdd形式的文件如何操作：12345678910111213141516import org.apache.spark.mllib.linalg.Vectorsimport org.apache.spark.mllib.stat.&#123;MultivariateStatisticalSummary, Statistics&#125;val data_txt = SparkSC.spark.sparkContext.textFile(input_txt).persist(StorageLevel.MEMORY_AND_DISK) val new_data = data_txt .map(_.split(&quot;\\\\s+&quot;)) .map(one =&gt; Vectors.dense(one(1).toInt,one(2).toDouble,one(3).toDouble,one(4).toDouble)) val summary: MultivariateStatisticalSummary = Statistics.colStats(new_data) println(&quot;Max:&quot;+summary.max)println(&quot;Min:&quot;+summary.min)println(&quot;Count:&quot;+summary.count)println(&quot;Variance:&quot;+summary.variance)println(&quot;Mean:&quot;+summary.mean)println(&quot;NormL1:&quot;+summary.normL1)println(&quot;Norml2:&quot;+summary.normL2) 输出结果为：1234567Max:[25.0,95.0,88.0,95.0]Min:[23.0,78.0,68.0,88.0]Count:3Variance:[1.0,86.33333333333331,100.0,13.0]Mean:[24.0,88.66666666666667,78.0,92.0]NormL1:[72.0,266.0,234.0,276.0]Norml2:[41.593268686170845,154.1363033162532,135.83813897429545,159.43023552638942] 这里可以得到相关的统计信息，主要区别在于dataframe得到的是标准差，而使用mllib得到的统计值中是方差，但这并不矛盾，两者可以相互转化得到。 当然如果要求四分位数，可以转化成df，使用sql语句进行查询1Select PERCENTILE(col,&lt;0.25,0.75&gt;) from tableName; 自己实现下面是我自己实现的一个方法，传入的参数是一个rdd，返回的是一个字符串1234567891011121314151617181920212223242526272829// 计算最大值，最小值，平均值，方差，标准差，四分位数def getStat(data: RDD[String]):String= &#123; val sort_data = data .filter(one =&gt; Verify.istoDouble(one)) .map(_.toDouble) .sortBy(line=&gt;line) .persist(StorageLevel.MEMORY_AND_DISK) // 默认是true 升序，false为降序 val data_list = sort_data.collect() val len = data_list.length val min = data_list(0) val max = data_list(len-1) val mean = sort_data.reduce((a,b) =&gt; a+b) / len val variance = sort_data.map(one =&gt; math.pow(one-mean,2)).reduce((a,b)=&gt;a+b)/len val stdder = math.sqrt(variance) var quant = &quot;&quot; if(len&lt;4)&#123; val q1 = min val q2 = min val q3 = max quant = q1+&quot;\\t&quot;+q2+&quot;\\t&quot;+q3 &#125;else &#123; val q1 = data_list((len * 0.25).toInt - 1) val q2 = data_list((len * 0.5).toInt - 1) val q3 = data_list((len * 0.75).toInt - 1) quant = q1+&quot;\\t&quot;+q2+&quot;\\t&quot;+q3 &#125; max+&quot;\\t&quot;+min+&quot;\\t&quot;+mean+&quot;\\t&quot;+variance+&quot;\\t&quot;+stdder+&quot;\\t&quot;+quant&#125; 本地碰见的一个错误1：错误11scala.Predef$.refArrayOps([Ljava/lang/Object;)Lscala/collection/mutable/Array 原因是Spark中spark-sql_2.11-2.2.1 ，是用scala 2.11版本上编译的，而我的本地的scala版本为2.12.4，所以就错了，可以在里边把相应的scala版本就行修改就行了 2：错误21java.lang.NoSuchMethodError: scala.Product.$init$(Lscala/Product;)V 原因也是因为我下载安装的scala2.12版本，换成scala2.11版本就可以了 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"数据计算","slug":"数据计算","permalink":"http://yoursite.com/categories/数据计算/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"}]},{"title":"记一次百G数据的聚类算法实施过程","slug":"Spark/记一次百G数据的聚类算法实施过程","date":"2018-01-26T16:08:24.000Z","updated":"2018-04-14T19:38:41.747Z","comments":true,"path":"2018/01/27/Spark/记一次百G数据的聚类算法实施过程/","link":"","permalink":"http://yoursite.com/2018/01/27/Spark/记一次百G数据的聚类算法实施过程/","excerpt":"如题，记一次百G数据的聚类算法实施过程，用的技术都不难，spark和kmeans，我想你会认为这没有什么难度，我接到这个任务的时候也认为没有难度，可是一周之后我发现我错了，数据量100G的确不大，但难度在于我需要对 kmeans 的 train过程执行将近3000次，而且需要高效的完成。那么问题就来了，如何保证高效和准确性。（声明小编对Spark也不是说很熟悉）","text":"如题，记一次百G数据的聚类算法实施过程，用的技术都不难，spark和kmeans，我想你会认为这没有什么难度，我接到这个任务的时候也认为没有难度，可是一周之后我发现我错了，数据量100G的确不大，但难度在于我需要对 kmeans 的 train过程执行将近3000次，而且需要高效的完成。那么问题就来了，如何保证高效和准确性。（声明小编对Spark也不是说很熟悉） 需求数据格式为三列，第一列为类别ID，第二列为商品ID，第三列为价格，数据格式如下12345671000 2000 45.31000 2001 121.31001 2002 4125.31000 2003 225.31001 2004 3415.31000 2005 12245.3 ... ... .... 数据有很多条，数据量为将近100G，存储在hdfs上，第一列品类ID不唯一，每个品类ID下有多个商品ID，商品ID唯一，价格为浮点型数据 现在要对每个品类下的价格进行聚类，得到1~7个价格level（7level的价格要比6level的价格高，以此类推） 第一次尝试第一次尝试很天真，思路也很正常，如下：1：全量加载数据，形成rdd2：数据split之后，按key进行groupby3：针对每个key（也就是类别ID）进行kmeans聚类和预测，并将结果写入hdfs4：加载每个类别的结果，进行聚合形成最终结果 那么开始写代码。papapa写了一堆，发现groupBy之后的数据格式是CompactBuffer，转化成spark kmeans train所需要的格式之后，代码卡着不会动，不明所以（我估计是格式没有转正确，不是kmeans 所需要的格式，但是如果不是kmeans 需要的格式，应该会报错呀），后来当我把代码打包，提交到集群上运行时，提示我kmeans train所在的函数中没有指定master url，可是我明明指定了，后来才发现是因为，我在rdd操作过程中能够，嵌套了函数，函数中又重新使用了rdd，也就是说rdd 不能嵌套rdd使用，具体可参考 Spark 为什么 不允许 RDD 嵌套-如 RDD[RDD[T]]，而我在本地测试时指的都是local，没有进行报错，至此这条路行不通，也就是说不能按这样的思路执行 在该思路的基础上进行改进：既然rdd不能嵌套rdd使用，何不先得到所有的类别id，然后在全量数据总filter单个类别id进行kmeans操作呢？ 该代码，测试，伪代码如下：12leibieID_list = XXXXXleibieID_list.map(one =&gt; kmeans(one,path)) 需要注意的是 leibieID_list.map 操作并不是分布式的，而是for 循环，这样3000个类别id运行完，时间可想而知，是极其耗时的，所以这条路也失败了（不是说行不通，是因为耗时） 第二次尝试经过上边的尝试发现不行，那么我想是不是先全量读取数据，然后按照类别ID，将同一个类别ID的数据写到一个文件（或者文件夹下），然后再对之操作 开始写classify by ID 的代码，这里遇到了问题是如何让同一个类别ID的数据写到一个文件中，上网查了一些资料，可以参考之前整理的笔记 Spark多路径输出和二次排序 这里边有实现的办法，但是还有一个问题，对全量数据（100G）进行shuffle的时候，由于数据量特别大，也特别占用资源，往往会出现一些内存上的错误。 这里采用的策略是将全量数据rdd进行random split，然后for循环遍历split之后的rdd，进行saveAsTestFile，保存的目录这样设计12345/path/split=0//path/split=1//path/split=2//path/split=3/ ... ... 这样的话，就可以避免大量数据 shuffle 耗费资源的问题了，而且也不影响后续数据的使用，同时这一步也会把类别id提取出来，保存在hdfs上，供下一步使用。 经历了上一步的数据准备，开始step 2的开发，第二步的思路：加载第一步保存的类别id list文件，分成5份，启动5个spark任务进行train，至此，思路是正确的，但却忽略了一个很严正的问题：数据倾斜 由于是随机对类别 id 进行分组操作，那么不能保证没组中每个类别id对应的数据条数的大概一致性，也就是存在某个ID 数据条数只有几十条，而有些ID 数据条数千万条，这种情况下就会导致代码在运行过程中，有些task很快运行完了，有些执行了好久也没完事。 第三次尝试有了第二次的经验，想法就是如何将数据条数差不多的分到同一组里，我采用的方法是进行统计，按照10的X次方形式进行分组，比如说12341~10 110~100 2100~1000 3.... 但是这样也有一个问题，就是这样大概符合正太分布，4、5、6这样的组里数据条数比较多，1、2、3和7、8、9这样的数据条数少，这样就会因为4、5、6组的程序运行时间较长，整体任务运行时间也较长。 所以这里采用合并和拆分的策略，比如说将1,2,3合并到一组，4、5、6分别拆成两组，7、8、9合成一组，这样就会保证每组运行的时间是差不多的。（实际情况中，要根据数据的分布进行合理的拆分和合并） 总结至此，问题算是最终解决了，相比原先的MR版本，时间缩减了将近8个小时，在整个优化的过程中，其实对于经验足够的开发者来说，可能很快就会解决，但对于我们这些新手，可能就要耗费些时间，涨涨记性了，在整个过程中对spark也算是有进一步的了解了。 其他的相关笔记： http://blog.csdn.net/gamer_gyt/article/details/79157055 http://blog.csdn.net/gamer_gyt/article/details/79135118 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"数据计算","slug":"数据计算","permalink":"http://yoursite.com/categories/数据计算/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"}]},{"title":"Spark多路径输出和二次排序","slug":"Spark/Spark多路径输出和二次排序","date":"2018-01-24T16:22:03.000Z","updated":"2018-04-14T19:38:48.099Z","comments":true,"path":"2018/01/25/Spark/Spark多路径输出和二次排序/","link":"","permalink":"http://yoursite.com/2018/01/25/Spark/Spark多路径输出和二次排序/","excerpt":"在实际应用场景中，我们对于Spark往往有各式各样的需求，比如说想MR中的二次排序，Top N，多路劲输出等。那么这篇文章我们就来看下这几个问题。","text":"在实际应用场景中，我们对于Spark往往有各式各样的需求，比如说想MR中的二次排序，Top N，多路劲输出等。那么这篇文章我们就来看下这几个问题。 二次排序假设我们的数据是这样的：123456781 21 31 11 61 42 52 82 3 我们想要实现第一列按降序排列，当第一列相同时，第二列按降序排列 定义一个SecondSortKey类：12345678910class SecondSortKey(val first: Int, val second: Int) extends Ordered[SecondSortKey] with Serializable &#123; override def compare(that: SecondSortKey): Int = &#123; if (this.first - that.first == 0) &#123; this.second - that.second &#125; else &#123; this.first - that.first &#125; &#125;&#125; 然后这样去使用1234567val lines = sc.textFile(&quot;test.txt&quot;)val pairs = lines.map &#123; x =&gt; (new SecondSortKey(x.split(&quot;\\\\s+&quot;)(0).toInt, x.split(&quot;\\\\s+&quot;)(1).toInt), x) &#125;val sortedPairs = pairs.sortByKey(false);sortedPairs.map(_._2).foreach(println) 当然这里如果想按第一列升序，当第一列相同时，第二列升序的顺序排列，只需要对SecondSoryKey做如下修改即可12345678910class SecondSortKey(val first: Int, val second: Int) extends Ordered[SecondSortKey] with Serializable &#123; override def compare(that: SecondSortKey): Int = &#123; if (this.first - that.first !== 0) &#123; this.second - that.second &#125; else &#123; this.first - that.first &#125; &#125;&#125; 当时使用的使用去掉1pairs.sortByKey(false) 中的false Top N同样还是上边的数据，假设我们要得到第一列中的前五位123456val lines = sc.textFile(&quot;test.txt&quot;)val rdd = lines .map(x =&gt; x.split(&quot;\\\\s+&quot;)) .map(x =&gt; (x(0),x(1))) .sortByKey()rdd.take(N).foreach(println) 多路径输出自己在使用的过程中，通过搜索发现了两种方法1：调用saveAsHadoopFile函数并自定义一个OutputFormat类 自定义RDDMultipleTextOutputFormat类 RDDMultipleTextOutputFormat类中的generateFileNameForKeyValue函数有三个参数，key和value就是我们RDD的Key和Value，而name参数是每个Reduce的编号。本例中没有使用该参数，而是直接将同一个Key的数据输出到同一个文件中。123456import org.apache.hadoop.mapred.lib.MultipleTextOutputFormat class RDDMultipleTextOutputFormat extends MultipleTextOutputFormat[Any, Any] &#123; override def generateFileNameForKeyValue(key: Any, value: Any, name: String): String = key.asInstanceOf[String] &#125; 调用1234sc.parallelize(List((&quot;w&quot;, &quot;www&quot;), (&quot;b&quot;, &quot;blog&quot;), (&quot;c&quot;, &quot;com&quot;), (&quot;w&quot;, &quot;bt&quot;))) .map(value =&gt; (value._1, value._2 + &quot;Test&quot;)) .partitionBy(new HashPartitioner(3)) .saveAsHadoopFile(&quot;/iteblog&quot;, classOf[String],classOf[String],classOf[RDDMultipleTextOutputFormat]) 这里的1new HashPartitioner(3) 中的3是有key的种类决定的，当然在实际应用场景中，我们可能并不知道有多少k，这个时候就可以通过一个rdd 的 distinct操作来得到唯一key的数目。 2：使用dataframe123people_rdd = sc.parallelize([(1, &quot;alice&quot;), (1, &quot;bob&quot;), (2,&quot;charlie&quot;)])people_df = people_rdd.toDF([&quot;number&quot;, &quot;name&quot;])people_df.write.partitionBy(&quot;number&quot;).format(&quot;text&quot;).save(path ) 当然这两种方法都有一个缺陷，就是当数据量特别大的时候，数据在repartition的过程中特别耗费资源，也会容易出现任务failed的情况，小编采用的解决办法是，适当的对原rdd进行split，然后遍历每个rdd，进行multioutput操作 形似如下：123456val rdd = sc.textFile(input)var split_rdd = rdd.randomSplit(Array(1.0,1.0,1.0,1.0))for (one &lt;- Array(1,2,3,4))&#123; split_rdd(one)XXXX&#125; 参考： Spark学习笔记——二次排序，TopN，TopNByGroup Spark多文件输出(MultipleOutputFormat) scala - Write to multiple outputs by key Spark - one Spark job Write to multiple outputs by key Spark - one Spark job 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群 转载请注明出处：http://blog.csdn.net/gamer_gyt博主微博：http://weibo.com/234654758Github：https://github.com/thinkgamer","categories":[{"name":"数据计算","slug":"数据计算","permalink":"http://yoursite.com/categories/数据计算/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"}]},{"title":"Spark提交参数说明和常见优化","slug":"Spark/Spark提交参数说明和常见优化","date":"2018-01-22T16:30:15.000Z","updated":"2018-04-14T19:38:51.699Z","comments":true,"path":"2018/01/23/Spark/Spark提交参数说明和常见优化/","link":"","permalink":"http://yoursite.com/2018/01/23/Spark/Spark提交参数说明和常见优化/","excerpt":"最近在搞一个价格分类模型，虽说是分类，用的是kmeans算法，求出聚类中心，对每个价格进行级别定级。虽然说起来简单，但做起来却是并没有那么容易，不只是因为数据量大，在执行任务时要不是效率问题就是shuffle报错等。但在这整个过程中对scala编程,Spark rdd 机制，以及海量数据背景下对算法的认知都有很大的提升，这一篇文章主要是总结一些Spark在shell 终端提交jar包任务的时候的相关知识，在后续文章会具体涉及到相关的”实战经历“。","text":"最近在搞一个价格分类模型，虽说是分类，用的是kmeans算法，求出聚类中心，对每个价格进行级别定级。虽然说起来简单，但做起来却是并没有那么容易，不只是因为数据量大，在执行任务时要不是效率问题就是shuffle报错等。但在这整个过程中对scala编程,Spark rdd 机制，以及海量数据背景下对算法的认知都有很大的提升，这一篇文章主要是总结一些Spark在shell 终端提交jar包任务的时候的相关知识，在后续文章会具体涉及到相关的”实战经历“。 对Spark的认识由于之前接触过Hadoop，对Spark也是了解一些皮毛，但中间隔了好久才重新使用spark，期间也产生过一些错误的认识。 之前觉得MapReduce耗费时间，写一个同等效果的Spark程序很快就能执行完，很长一段时间自己都是在本地的单机环境进行测试学习，所以这种错误的认知就会更加深刻，但事实却并非如此，MR之所以慢是因为每一次操作数据都写在了磁盘上，大量的IO造成了时间和资源的浪费，但是Spark是基于内存的计算引擎，相比MR，减少的是大量的IO，但并不是说给一个Spark程序足够的资源，就可以为所欲为了，在提交一个spark程序时，不仅要考虑所在资源队列的总体情况，还要考虑代码本身的高效性，要尽量避免大量的shuffle操作和action操作，尽量使用同一个rdd。 会用spark，会调api和能用好spark是两回事，在进行开发的过程中，不仅要了解运行原理，还要了解业务，将合适的方法和业务场景合适的结合在一起，才能发挥最大的价值。 spark-submit进入spark的home目录，执行以下命令查看帮助1bin/spark-submit --help spark提交任务常见的两种模式1：local/local[K] 本地使用一个worker线程运行spark程序 本地使用K个worker线程运行spark程序 此种模式下适合小批量数据在本地调试代码 2：yarn-client/yarn-cluster yarn-client：以client方式连接到YARN集群，集群的定位由环境变量HADOOP_CONF_DIR定义，该方式driver在client运行。 yarn-cluster：以cluster方式连接到YARN集群，集群的定位由环境变量HADOOP_CONF_DIR定义，该方式driver也在集群中运行。 注意：若使用的是本地文件需要在file路径前加：file:// 在提交任务时的几个重要参数 executor-cores —— 每个executor使用的内核数，默认为1 num-executors —— 启动executors的数量，默认为2 executor-memory —— executor内存大小，默认1G driver-cores —— driver使用内核数，默认为1 driver-memory —— driver内存大小，默认512M 下边给一个提交任务的样式12345678910111213spark-submit \\ --master local[5] \\ --driver-cores 2 \\ --driver-memory 8g \\ --executor-cores 4 \\ --num-executors 10 \\ --executor-memory 8g \\ --class PackageName.ClassName XXXX.jar \\ --name &quot;Spark Job Name&quot; \\ InputPath \\ OutputPath 如果这里通过--queue 指定了队列，那么可以免去写--master 以上就是通过spark-submit来提交一个任务 几个参数的常规设置 executor_cores*num_executors表示的是能够并行执行Task的数目不宜太小或太大！一般不超过总队列 cores 的 25%，比如队列总 cores 400，最大不要超过100，最小不建议低于 40，除非日志量很小。 executor_cores不宜为1！否则 work 进程中线程数过少，一般 2~4 为宜。 executor_memory一般 6~10g 为宜，最大不超过20G，否则会导致GC代价过高，或资源浪费严重。 driver-memorydriver 不做任何计算和存储，只是下发任务与yarn资源管理器和task交互，除非你是 spark-shell，否则一般 1-2g 增加每个executor的内存量，增加了内存量以后，对性能的提升，有三点： 1、如果需要对RDD进行cache，那么更多的内存，就可以缓存更多的数据，将更少的数据写入磁盘，甚至不写入磁盘。减少了磁盘IO。 2、对于shuffle操作，reduce端，会需要内存来存放拉取的数据并进行聚合。如果内存不够，也会写入磁盘。如果给executor分配更多内存以后，就有更少的数据，需要写入磁盘，甚至不需要写入磁盘。减少了磁盘IO，提升了性能。 3、对于task的执行，可能会创建很多对象。如果内存比较小，可能会频繁导致JVM堆内存满了，然后频繁GC，垃圾回收，minor GC和full GC。（速度很慢）。内存加大以后，带来更少的GC，垃圾回收，避免了速度变慢，性能提升。 常规注意事项 预处理数据，丢掉一些不必要的数据 增加Task的数量 过滤掉一些容易导致发生倾斜的key 避免创建重复的RDD 尽可能复用一个RDD 对多次使用的RDD进行持久化 尽量避免使用shuffle算子 在要使用groupByKey算子的时候,尽量用reduceByKey或者aggregateByKey算子替代.因为调用groupByKey时候,按照相同的key进行分组,形成RDD[key,Iterable[value]]的形式,此时所有的键值对都将被重新洗牌,移动,对网络数据传输造成理论上的最大影响. 使用高性能的算子 参考：1：http://www.cnblogs.com/haozhengfei/p/e570f24c43fa15f23ebb97929a1b7fe6.html2：https://www.jianshu.com/p/4c584a3bac7d 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"数据计算","slug":"数据计算","permalink":"http://yoursite.com/categories/数据计算/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"}]},{"title":"用大把的时间仿徨，却用几个瞬间成长","slug":"随手记/用大把的时间仿徨，却用几个瞬间成长","date":"2017-12-31T03:02:00.000Z","updated":"2018-03-28T16:01:41.617Z","comments":true,"path":"2017/12/31/随手记/用大把的时间仿徨，却用几个瞬间成长/","link":"","permalink":"http://yoursite.com/2017/12/31/随手记/用大把的时间仿徨，却用几个瞬间成长/","excerpt":"人总要在特定的阶段去完成特定的事情，然后转身告诉自己，继续往前。 提笔写这篇文章，内心是毫无波澜的，或许是因为曾经的经历让我意识到那一切都是缘的力所能及。以下的内容可能没有章节，没有顺序，但都是心灵能够触及的地方。","text":"人总要在特定的阶段去完成特定的事情，然后转身告诉自己，继续往前。 提笔写这篇文章，内心是毫无波澜的，或许是因为曾经的经历让我意识到那一切都是缘的力所能及。以下的内容可能没有章节，没有顺序，但都是心灵能够触及的地方。 2017年对我个人来讲是比较重要的一年，可以说是完成了自我的一个改变吧，但是脱壳之后，需要的是更加努力的完善自我，这样才能够避免被淘汰。为什么这么说？因为这一年我觉得对我个人影响最大的两件事是：买了首套房（虽然只是付了首付）；在年末之时选择离开，侥幸的入职了京东。一个是为“家庭”做了必要的准备，一个是为“事业”做了点缀。而至于其他的大大小小的事情，则是17年的五味杂粮，并不是那么重要，但却也缺一不可。感谢17年，那些我认识的，或者我不认识的，帮助过我的，或者我帮助过的人。 过往人上了年纪，总爱回忆！ 刚好这两天，“18岁”刷爆了朋友圈和QQ空间，或许这是连腾讯有没有预料到的起死回生或者苟延残喘吧。趁机翻了下我的QQ空间相册，突然发现了，这二十多年来以来，我也是经历颇丰。 从上海到江苏，从沈阳到长春，从南京到天津，最后到北京，这一条路一走便是4年。从初中到高中，从大学到社会，这一条路一走便是11年。 过往的这条路上遇见了很多人，碰见很多事，但终究还是走散了，看淡了，不过庆幸的是那些一直还有联系的，我们还能彼此叫出姓名的人，不管是18岁之前，还是18岁之后，我想我们是幸运的， 年少的我们曾经难免会埋下羞涩的种子，在记忆深处藏着一些不可告人的秘密，直到有一天我们盘膝而坐，三巡酒过，才道出那些现在我们认为可笑的不能再可笑的羞涩，从此，稚嫩青春里的唯一一朵留恋，也该告一段落。 一杯敬明天，一杯敬过往。灵魂不再无处安放。 遇见人有了目标，便爱胡闹！ 2017年，不管是工作，还是自我学习都收获了挺多知识，感谢万维接受了我这个毫无工作经验的“学生”吧，在这里的确收获了挺多，让我明白没有结合业务的技术，只是向别人吹嘘而毫无创造价值的垃圾，不管你在哪，技术都是为了推动业务的增长。 2017年，还是习惯性的写一些学习笔记，只不过是频率明显了降了下来，这只能归结于自己变得懒惰了，从开通博客到现在，所有的表层的收获只能通过这些浅显的数据来表达： 只是没有17年尾的时候访问量达到100W，不过一切随缘吧。 下边这几张图是最近一个月的访问分析情况，不管怎样，你学习了，也帮助其他人了，这就是成长。 2017年，开通了微信公众号【数据与算法联盟】（原名为码农故事多），没有刻意的运营，没有刻意的传播积累用户，一切随缘。当然如果你想加入我们的数据与算法学习交流群的话，欢迎加我的微信，拉你入群，群里有很多大牛，不定时进行“扯淡”。我们的宗旨就是以技术会友，分享与进步！感谢2017年和以往遇见的所有好友！ 长按二维码识别，关注微信公众号【数据与算法联盟】 2017年，很侥幸的加入了JD这个大家庭，未来的一切都是未知数，但你能做的就是向他人学习，围绕着工作进行深度的学习和成长。 新生人过了18，要努力发芽！ 过去的一年里定了太多的目标，结果大部分都没有实现，哎，分析一下，大部门的目标都是盲目的，没有围绕工作的目标（学习规划吧算是），其实实现起来是有难度的，所以新的一年里调整计划，重新出发。 2018不会定太多的目标，主要是想在技术和业务层面提升下自己，不管是学习大数据还是算法，都会围绕功能工作和一条主旋律进行展开。 感谢一路以来，遇见的所有人！ 转载请注明出处：http://blog.csdn.net/gamer_gyt博主微博：http://weibo.com/234654758Github：https://github.com/thinkgamer 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"随手记","slug":"随手记","permalink":"http://yoursite.com/categories/随手记/"}],"tags":[{"name":"随手记","slug":"随手记","permalink":"http://yoursite.com/tags/随手记/"}]},{"title":"Hexo-Yilia加入相册功能","slug":"随手记/Hexo-Yilia加入相册功能","date":"2017-12-14T09:55:29.000Z","updated":"2018-03-28T16:01:55.585Z","comments":true,"path":"2017/12/14/随手记/Hexo-Yilia加入相册功能/","link":"","permalink":"http://yoursite.com/2017/12/14/随手记/Hexo-Yilia加入相册功能/","excerpt":"参考：点击查看 但是其中有一些小问题，自己便重新整理了一下（本文适用于使用github存放照片）","text":"参考：点击查看 但是其中有一些小问题，自己便重新整理了一下（本文适用于使用github存放照片） 主页新建相册链接主题_config.json文件的menu 中加入 相册和对应的链接123456themes/yilia/_config.jsonmenu: 主页: / ... ... 相册: /photos 新建目录并拷贝相应文件使用的是litten 大神的博客 photos文件夹，对应的路径为：https://github.com/litten/BlogBackup/tree/master/source/photos 自己的项目根目录下的source文件夹下新建photos文件夹，将下载的几个文件放在该文件夹中，或者不用新建，直接将下载的photos文件夹放在source目录下。 文件修改 修改 ins.js 文件的 render()函数这个函数是用来渲染数据的修改图片的路径地址.minSrc 小图的路径. src 大图的路径.修改为自己的图片路径(github的路径)例如我的为：12var minSrc = &apos;https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/&apos; + data.link[i] + &apos;.min.jpg&apos;;var src = &apos;https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/photos/&apos; + data.link[i]; 生成json1：下载相应python工具文件 tools.py ImageProcess.py 下载地址：https://github.com/Thinkgamer/GitBlog 2：新建photos和min_photos文件夹在项目根目录下创建，用来存放照片和压缩后的照片12mkdir photosmkdir min_photos 3：py文件和文件夹都放在项目根目录下 4：生成json执行1python tools.py 如果提示：1234Traceback (most recent call last): File &quot;tools.py&quot;, line 13, in &lt;module&gt; from PIL import ImageImportError: No module named PIL 说明你没有安装pillow，执行以下命令安装即可1pip install pillow 如果报错：1ValueError: time data &apos;DSC&apos; does not match format &apos;%Y-%m-%d&apos; 说明你照片的命名方式不合格，这里必须命名为以下这样的格式（当然时间是随意的）12016-10-12_xxx.jpg/png ok，至此会在min_photos文件夹下生成同名的文件，但是大小会小很多 本地预览和部署本地预览项目根目录下执行1hexo s 浏览器4000端口访问，按照上边的方式进行配置，正常情况下你是看不到图片的，通过调试可以发现图片的url中后缀变成了 xxx.jpg.jpg，所以我们要去掉一个jpg 改正方法ins.js/render 函数12345678var minSrc = &apos;https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/&apos; + data.link[i] + &apos;.min.jpg&apos;;换成var minSrc = &apos;https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/&apos; + data.link[i];注释掉该行：src += &apos;.jpg&apos;; 到这里没完，路径都对了，但是在浏览器中还是不能看到图片，调试发现，下载大神的photos文件夹的ins.js中有一行代码，饮用了一张图片，默认情况下，在你的项目中，这张图片是不存在的，改正办法就是在对应目录下放一张图片，并修改相应的名字 1src=&quot;/assets/img/empty.png ok，至此刷新浏览器是可以看到图片的，如果还看不到，应该就是浏览器缓存问题了，如果还有问题，可以加我微信进行沟通：gyt13342445911 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"随手记","slug":"随手记","permalink":"http://yoursite.com/categories/随手记/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"}]},{"title":"梯度算法之批量梯度下降，随机梯度下降和小批量梯度下降","slug":"机器学习/梯度算法之批量梯度下降，随机梯度下降和小批量梯度下降","date":"2017-12-14T06:40:43.000Z","updated":"2018-03-28T15:58:51.317Z","comments":true,"path":"2017/12/14/机器学习/梯度算法之批量梯度下降，随机梯度下降和小批量梯度下降/","link":"","permalink":"http://yoursite.com/2017/12/14/机器学习/梯度算法之批量梯度下降，随机梯度下降和小批量梯度下降/","excerpt":"在机器学习领域，体梯度下降算法分为三种 批量梯度下降算法（BGD，Batch gradient descent algorithm） 随机梯度下降算法（SGD，Stochastic gradient descent algorithm） 小批量梯度下降算法（MBGD，Mini-batch gradient descent algorithm）","text":"在机器学习领域，体梯度下降算法分为三种 批量梯度下降算法（BGD，Batch gradient descent algorithm） 随机梯度下降算法（SGD，Stochastic gradient descent algorithm） 小批量梯度下降算法（MBGD，Mini-batch gradient descent algorithm） 批量梯度下降算法BGD是最原始的梯度下降算法，每一次迭代使用全部的样本，即权重的迭代公式中(公式中用$\\theta$代替$\\theta_i$)， \\jmath (\\theta _0,\\theta _1,...,\\theta _n)=\\sum_{i=0}^{m}( h_\\theta(x_0,x_1,...,x_n)-y_i )^2 \\theta _i = \\theta _i - \\alpha \\frac{\\partial \\jmath (\\theta _1,\\theta _2,...,\\theta _n)}{\\partial \\theta _i} 公式(1)这里的m代表所有的样本，表示从第一个样本遍历到最后一个样本。 特点： 能达到全局最优解，易于并行实现 当样本数目很多时，训练过程缓慢 随机梯度下降算法SGD的思想是更新每一个参数时都使用一个样本来进行更新，即公式（1）中m为1。每次更新参数都只使用一个样本，进行多次更新。这样在样本量很大的情况下，可能只用到其中的一部分样本就能得到最优解了。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。 特点： 训练速度快 准确度下降，并不是最优解，不易于并行实现 小批量梯度下降算法MBGD的算法思想就是在更新每一参数时都使用一部分样本来进行更新，也就是公式（1）中的m的值大于1小于所有样本的数量。 相对于随机梯度下降，Mini-batch梯度下降降低了收敛波动性，即降低了参数更新的方差，使得更新更加稳定。相对于批量梯度下降，其提高了每次学习的速度。并且其不用担心内存瓶颈从而可以利用矩阵运算进行高效计算。一般而言每次更新随机选择[50,256]个样本进行学习，但是也要根据具体问题而选择，实践中可以进行多次试验，选择一个更新速度与更次次数都较适合的样本数。mini-batch梯度下降可以保证收敛性，常用于神经网络中。 补充在样本量较小的情况下，可以使用批量梯度下降算法，样本量较大的情况或者线上，可以使用随机梯度下降算法或者小批量梯度下降算法。 在机器学习中的无约束优化算法，除了梯度下降以外，还有前面提到的最小二乘法，此外还有牛顿法和拟牛顿法。 梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。 梯度下降法和牛顿法/拟牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法/拟牛顿法是用二阶的海森矩阵的逆矩阵或伪逆矩阵求解。相对而言，使用牛顿法/拟牛顿法收敛更快。但是每次迭代的时间比梯度下降法长。 sklearn中的SGDsklearn官网上查了一下，并没有找到BGD和MBGD的相关文档，只是看到可SGD的，感兴趣的可以直接去官网看英文文档，点击SGD查看：SGD，这也有一个中文的 SGD 123456789101112131415161718192021222324In [1]: from sklearn.linear_model import SGDClassifierIn [2]: X = [[0., 0.], [1., 1.]]In [3]: y = [0, 1]In [4]: clf = SGDClassifier(loss=&quot;hinge&quot;, penalty=&quot;l2&quot;)In [5]: clf.fit(X, y)Out[5]: SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15, learning_rate=&apos;optimal&apos;, loss=&apos;hinge&apos;, n_iter=5, n_jobs=1, penalty=&apos;l2&apos;, power_t=0.5, random_state=None, shuffle=True, verbose=0, warm_start=False)In [6]: clf.predict([[2., 2.]])Out[6]: array([1])In [7]: clf.coef_ Out[7]: array([[ 9.91080278, 9.91080278]])In [8]: clf.intercept_ Out[8]: array([-9.97004991]) 参考： https://www.cnblogs.com/pinard/p/5970503.html http://blog.csdn.net/uestc_c2_403/article/details/74910107 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"梯度下降","slug":"梯度下降","permalink":"http://yoursite.com/tags/梯度下降/"}]},{"title":"梯度算法之梯度上升和梯度下降","slug":"机器学习/梯度算法之梯度上升和梯度下降","date":"2017-12-14T06:11:11.000Z","updated":"2018-03-28T15:58:57.277Z","comments":true,"path":"2017/12/14/机器学习/梯度算法之梯度上升和梯度下降/","link":"","permalink":"http://yoursite.com/2017/12/14/机器学习/梯度算法之梯度上升和梯度下降/","excerpt":"第一次看见随机梯度上升算法是看《机器学习实战》这本书，当时也是一知半解，只是大概知道和高等数学中的函数求导有一定的关系。下边我们就好好研究下随机梯度上升（下降）和梯度上升（下降）。","text":"第一次看见随机梯度上升算法是看《机器学习实战》这本书，当时也是一知半解，只是大概知道和高等数学中的函数求导有一定的关系。下边我们就好好研究下随机梯度上升（下降）和梯度上升（下降）。 高数中的导数设导数 y = f(x) 在 $ x_0 $的某个邻域内有定义，当自变量从 $ x_0 $ 变成 x_{0} + \\Delta x函数y=f(x)的增量 \\Delta y = f(x_0 + \\Delta x) - f(x_0)与自变量的增量 $ \\Delta x $ 之比： \\frac{ \\Delta y }{ \\Delta x } = \\frac{ f(x_0 + \\Delta x)-f(x_0) }{ \\Delta x }称为f(x)的平均变化率。如 $ \\Delta x \\rightarrow 0 $ 平均变化率的极限 \\lim_{\\Delta x \\rightarrow 0} \\frac{ \\Delta y }{ \\Delta x } = \\lim_{\\Delta x \\rightarrow 0} \\frac{ f(x_0 + \\Delta x)-f(x_0) }{ \\Delta x }存在，则称极限值为f(x)在$ x_0 $ 处的导数，并说f(x)在$ x_0 $ 处可导或有导数。当平均变化率极限不存在时，就说f(x)在 $ x_0 $ 处不可导或没有导数。 关于导数的说明 1）点导数是因变量在$ x_0 $ 处的变化率，它反映了因变量随自变量的变化而变化的快慢成都 2）如果函数y = f(x)在开区间 I 内的每点都可导，就称f(x)在开区间 I 内可导 3）对于任一 x 属于 I ，都对应着函数f(x)的一个导数，这个函数叫做原来函数f(x)的导函数 4）导函数在x1 处 为 0，若 x&lt;1 时，f’(x) &gt; 0 ，这 f(x) 递增，若f’(x)&lt;0 ，f(x)递减 5）f’(x0) 表示曲线y=f(x)在点 （x0,f($x_0$)）处的切线斜率 偏导数函数z=f(x,y)在点(x0,y0)的某一邻域内有定义，当y固定在y0而x在 $x_0$ 处有增量$ \\Delta x $ 时，相应的有函数增量 f(x_0 + \\Delta x, y_0) - f(x_0,y_0)如果 \\lim_{\\Delta x\\rightarrow 0 } \\frac {f(x_0 + \\Delta x, y_0) - f(x_0,y_0)}{\\Delta x}存在，则称z=f(x,y)在点($x_0$,$y_0$)处对x的偏导数，记为：$ f_x(x_0,y_0) $ 如果函数z=f(x,y)在区域D内任一点(x,y)处对x的偏导数都存在，那么这个偏导数就是x,y的函数，它就称为函数z=f(x,y)对自变量x的偏导数，记做 \\frac{ \\partial z }{ \\partial x } , \\frac{ \\partial f }{ \\partial x } , z_x , f_x(x,y),偏导数的概念可以推广到二元以上的函数，如 u = f(x,y,z)在x,y,z处 f_x(x,y,z)=\\lim_{\\Delta x \\rightarrow 0} \\frac{f(x + \\Delta x,y,z) -f(x,y,z)}{\\Delta x} f_y(x,y,z)=\\lim_{\\Delta y \\rightarrow 0} \\frac{f(x,y + \\Delta y,z) -f(x,y,z)}{\\Delta y} f_z(x,y,z)=\\lim_{\\Delta z \\rightarrow 0} \\frac{f(x,y,z + \\Delta z) -f(x,y,z)}{\\Delta z}可以看出导数与偏导数本质是一致的，都是自变量趋近于0时，函数值的变化与自变量的变化量比值的极限，直观的说，偏导数也就是函数在某一点沿坐标轴正方向的变化率。 区别：导数指的是一元函数中，函数y=f(x)某一点沿x轴正方向的的变化率；偏导数指的是多元函数中，函数y=f(x,y,z)在某一点沿某一坐标轴正方向的变化率。 偏导数的几何意义：偏导数$ z = f_x(x_0,y_0)$表示的是曲面被 $ y=y_0 $ 所截得的曲线在点M处的切线$ M_0T_x $对x轴的斜率偏导数$ z = f_y(x_0,y_0)$表示的是曲面被 $ x=x_0 $ 所截得的曲线在点M处的切线$ M_0T_y $对y轴的斜率 例子：求 $z = x^2 + 3 xy+y^2 $在点(1,2)处的偏导数。 \\frac{ \\partial z}{\\partial x} = 2x +3y \\frac{ \\partial z}{\\partial y} = 2y +3x所以:$z_x(x=1,y=2) = 8$$z_y(x=1,y=2) = 7$ 方向导数 \\frac{ \\partial }{ \\partial l } f(x_0,x_1,...,x_n) = \\lim_{\\rho \\rightarrow 0} \\frac{\\Delta y}{ \\Delta x } = \\lim_{\\rho \\rightarrow 0} \\frac{ f(x_0 + \\Delta x_0,...,x_j + \\Delta x_j,...,x_n + \\Delta x_n)-f(x_0,...,x_j,...,x_n)}{ \\rho } \\rho = \\sqrt{ (\\Delta x_0)^{2} +...+(\\Delta x_j)^{2}+...+(\\Delta x_n)^{2}}前边导数和偏导数的定义中，均是沿坐标轴正方向讨论函数的变化率。那么当讨论函数沿任意方向的变化率时，也就引出了方向导数的定义，即：某一点在某一趋近方向上的导数值。 通俗的解释是： 我们不仅要知道函数在坐标轴正方向上的变化率（即偏导数），而且还要设法求得函数在其他特定方向上的变化率。而方向导数就是函数在其他特定方向上的变化率。 梯度与方向导数有一定的关联，在微积分里面，对多元函数的参数求 $ \\partial $ 偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数f(x,y), 分别对x,y求偏导数，求得的梯度向量就是 $( \\frac{ \\partial f }{ \\partial x },\\frac{ \\partial f }{ \\partial y })^T$ ,简称grad f(x,y)或者 $▽f(x,y)$。对于在点$(x_0,y_0)$的具体梯度向量就是$( \\frac{ \\partial f }{ \\partial x_0 },\\frac{ \\partial f }{ \\partial y_0 })^T$.或者$▽f(x_0,y_0)$，如果是3个参数的向量梯度，就是 $( \\frac{ \\partial f }{ \\partial x },\\frac{ \\partial f }{ \\partial y },\\frac{ \\partial f }{ \\partial z })^T$,以此类推。 那么这个梯度向量求出来有什么意义呢？他的意义从几何意义上讲，就是函数变化增加最快的地方。具体来说，对于函数f(x,y),在点$(x_0,y_0)$，沿着梯度向量的方向就是$( \\frac{ \\partial f }{ \\partial x_0 },\\frac{ \\partial f }{ \\partial y_0 })^T$的方向是f(x,y)增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，也就是 $-( \\frac{ \\partial f }{ \\partial x_0 },\\frac{ \\partial f }{ \\partial y_0 })^T$的方向，梯度减少最快，也就是更加容易找到函数的最小值。 例如：函数 $f(x,y) = \\frac{1}{x^2+y^2} $ ，分别对x，y求偏导数得： \\frac{ \\partial f }{ \\partial x}=-\\frac{2x}{ (x^2+y^2)^2} \\frac{ \\partial f }{ \\partial y}=-\\frac{2y}{ (x^2+y^2)^2}所以 grad( \\frac{1}{x^2+y^2} ) = (-\\frac{2x}{ (x^2+y^2)^2} ,-\\frac{2y}{ (x^2+y^2)^2})函数在某一点的梯度是这样一个向量，它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值。 注意点：1）梯度是一个向量2）梯度的方向是最大方向导数的方向3）梯度的值是最大方向导数的值 梯度下降与梯度上升在机器学习算法中，在最小化损失函数时，可以通过梯度下降思想来求得最小化的损失函数和对应的参数值，反过来，如果要求最大化的损失函数，可以通过梯度上升思想来求取。 梯度下降关于梯度下降的几个概念1）步长（learning rate）：步长决定了在梯度下降迭代过程中，每一步沿梯度负方向前进的长度2）特征（feature）：指的是样本中输入部门，比如样本（x0，y0），（x1，y1），则样本特征为x，样本输出为y3）假设函数（hypothesis function）：在监督学习中，为了拟合输入样本，而使用的假设函数，记为$h_θ(x)$。比如对于样本$（x_i,y_i）(i=1,2,…n)$,可以采用拟合函数如下： $h_θ(x) = θ0+θ1_x$。4）损失函数（loss function）：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。在线性回归中，损失函数通常为样本输出和假设函数的差取平方。比如对于样本（xi,yi）(i=1,2,…n),采用线性回归，损失函数为： \\jmath (\\theta _0,\\theta _1)=\\sum_{i=0}^{m}( h_\\theta(x_i)-y_i )^2其中$x_i$表示样本特征x的第i个元素，$y_i$表示样本输出y的第i个元素，$h_\\theta(x_i)$ 为假设函数。 梯度下降的代数方法描述 先决条件：确定优化模型的假设函数和损失函数这里假定线性回归的假设函数为$h_\\theta(x_1,x_2,…x_n)=\\theta_0+\\theta_1x_1+…+\\theta_nx_n$，其中 $\\theta _i(i=0,1,2…n)$ 为模型参数(公式中用$\\theta$代替)，$x_i(i=0,1,2…n)$为每个样本的n个特征值。 则对应选定得损失函数为： \\jmath (\\theta _0,\\theta _1,...,,\\theta _n)=\\sum_{i=0}^{m}( h_\\theta(x_0,x_1,...,x_n)-y_i )^2 算法相关参数的初始化主要是初始化 $ \\theta _0,\\theta _1…,\\theta _n$，算法终止距离 $\\varepsilon $ 以及步长 $ \\alpha $。在没有任何先验知识的时候，我喜欢将所有的 $\\theta$ 初始化为0， 将步长初始化为1。在调优的时候再优化。 算法过程 1)：确定当前损失函数的梯度，对于$\\theta _i $，其梯度表达式为： \\frac{\\partial }{\\partial \\theta _i}\\jmath (\\theta _1,\\theta _2,...,\\theta _n) 2)：用步长乘以损失函数的梯度，得到当前位置的下降距离，即 \\alpha \\frac{\\partial \\jmath (\\theta _1,\\theta _2,...,\\theta _n)}{\\partial \\theta _i} 3)：确定是否所有的$\\theta _i$ ，梯度下降的距离都小于 $ \\varepsilon $，如果小于$ \\varepsilon $，则算法停止，当前所有的 $\\theta _i(i=1,2,3,…,n)$ 即为最终结果。否则执行下一步。 4)：更新所有的 $\\theta$，对于$\\theta _i $，其更新表达式如下。更新完毕后继续转入步骤1)。 \\theta _i = \\theta _i - \\alpha \\frac{\\partial \\jmath (\\theta _1,\\theta _2,...,\\theta _n)}{\\partial \\theta _i}梯度下降的矩阵方式描述 先决条件：确定优化模型的假设函数和损失函数这里假定线性回归的假设函数为$h_\\theta(x_1,x_2,…x_n)=\\theta_0+\\theta_1x_1+…+\\theta_nx_n$，其中 $\\theta _i(i=0,1,2…n)$ 为模型参数，$x_i(i=0,1,2…n)$为每个样本的n个特征值。假设函数对应的矩阵表示为：$ h_\\theta (x) = X \\theta $，假设函数 $h_\\theta(x)$ 为mx1的向量，$\\theta $ 为nx1的向量，里面有n个代数法的模型参数。X为mxn维的矩阵。m代表样本的个数，n代表样本的特征数。则对应选定得损失函数为： \\jmath (\\theta)=(X \\theta −Y)^T (X \\theta−Y)其中YY是样本的输出向量，维度为m*12.算法相关参数初始化:$\\theta$ 向量可以初始化为默认值，或者调优后的值。算法终止距离 $\\varepsilon $ ，步长 $\\alpha$ 和 “梯度下降的代数方法”描述中一致。3.算法过程 1)：确定当前位置的损失函数的梯度，对于 $ \\theta $ 向量,其梯度表达式如下： \\frac{ \\partial }{\\partial \\theta } \\jmath (\\theta) 2)：用步长乘以损失函数的梯度，得到当前位置下降的距离，即 $\\alpha \\frac{ \\partial }{\\partial \\theta } \\jmath (\\theta)$ 3)：确定 $\\theta$ 向量里面的每个值,梯度下降的距离都小于 $\\varepsilon$，如果小于 $\\varepsilon$ 则算法终止，当前 $\\theta$ 向量即为最终结果。否则进入步骤4) 4)：更新 $\\theta$ 向量，其更新表达式如下。更新完毕后继续转入步骤1) \\theta =\\theta - \\alpha \\frac{ \\partial }{\\partial \\theta } \\jmath (\\theta) 梯度上升梯度上升和梯度下降的分析方式是一致的，只不过把 $ \\theta $ 的更新中 减号变为加号。 梯度下降的算法优化 算法的步长选择。在前面的算法描述中，我提到取步长为1，但是实际上取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。前面说了。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。 算法参数的初始值选择。 初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。 3.归一化。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化，也就是对于每个特征x，求出它的均值 $\\bar{x}$ 和标准差std(x)，然后转化为： \\frac{x - \\bar{x}}{std(x)}这样特征的新期望为0，新方差为1，迭代次数可以大大加快。 http://blog.csdn.net/walilk/article/details/50978864 https://www.zhihu.com/question/24658302 https://www.cnblogs.com/pinard/p/5970503.html http://www.doc88.com/p-7844239247737.html 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"梯度下降","slug":"梯度下降","permalink":"http://yoursite.com/tags/梯度下降/"},{"name":"梯度上升","slug":"梯度上升","permalink":"http://yoursite.com/tags/梯度上升/"}]},{"title":"异常检测之指数平滑（利用elasticsearch来实现）","slug":"ELK/异常检测之指数平滑（利用elasticsearch来实现）","date":"2017-11-20T09:18:54.000Z","updated":"2018-03-28T16:02:09.605Z","comments":true,"path":"2017/11/20/ELK/异常检测之指数平滑（利用elasticsearch来实现）/","link":"","permalink":"http://yoursite.com/2017/11/20/ELK/异常检测之指数平滑（利用elasticsearch来实现）/","excerpt":"指数平滑法是一种特殊的加权平均法，加权的特点是对离预测值较近的历史数据给予较大的权数，对离预测期较远的历史数据给予较小的权数，权数由近到远按指数规律递减，所以，这种预测方法被称为指数平滑法。它可分为一次指数平滑法、二次指数平滑法及更高次指数平滑法。","text":"指数平滑法是一种特殊的加权平均法，加权的特点是对离预测值较近的历史数据给予较大的权数，对离预测期较远的历史数据给予较小的权数，权数由近到远按指数规律递减，所以，这种预测方法被称为指数平滑法。它可分为一次指数平滑法、二次指数平滑法及更高次指数平滑法。 关于指数平滑的得相关资料： ES API接口： https://github.com/IBBD/IBBD.github.io/blob/master/elk/aggregations-pipeline.mdhttps://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-pipeline-movavg-aggregation.html 理论概念 http://blog.sina.com.cn/s/blog_4b9acb5201016nkd.html ES移动平均聚合：Moving Average的四种模型simple就是使用窗口内的值的和除于窗口值，通常窗口值越大，最后的结果越平滑: (a1 + a2 + … + an) / n12345678910111213141516171819202122232425curl -XPOST &apos;localhost:9200/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;my_date_histo&quot;:&#123; &quot;date_histogram&quot;:&#123; &quot;field&quot;:&quot;date&quot;, &quot;interval&quot;:&quot;1M&quot; &#125;, &quot;aggs&quot;:&#123; &quot;the_sum&quot;:&#123; &quot;sum&quot;:&#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;the_movavg&quot;:&#123; &quot;moving_avg&quot;:&#123; &quot;buckets_path&quot;: &quot;the_sum&quot;, &quot;window&quot; : 30, &quot;model&quot; : &quot;simple&quot; &#125; &#125; &#125; &#125; &#125;&#125;&apos; 线性模型：Linear对窗口内的值先做线性变换处理，再求平均：(a1 1 + a2 2 + … + an * n) / (1 + 2 + … + n) 12345678910111213141516171819202122232425curl -XPOST &apos;localhost:9200/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;my_date_histo&quot;:&#123; &quot;date_histogram&quot;:&#123; &quot;field&quot;:&quot;date&quot;, &quot;interval&quot;:&quot;1M&quot; &#125;, &quot;aggs&quot;:&#123; &quot;the_sum&quot;:&#123; &quot;sum&quot;:&#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;the_movavg&quot;: &#123; &quot;moving_avg&quot;:&#123; &quot;buckets_path&quot;: &quot;the_sum&quot;, &quot;window&quot; : 30, &quot;model&quot; : &quot;linear&quot; &#125; &#125; &#125; &#125; &#125;&#125;&apos; 指数平滑模型指数模型：EWMA (Exponentially Weighted)即： 一次指数平滑模型 EWMA模型通常也成为单指数模型（single-exponential）, 和线性模型的思路类似，离当前点越远的点，重要性越低，具体化为数值的指数下降，对应的参数是alpha。 alpha值越小，下降越慢。（估计是用1 - alpha去计算的）默认的alpha=0.3 计算模型：s2 = α x2 + (1 - α) s1 其中α是平滑系数，si是之前i个数据的平滑值，α取值为[0,1]，越接近1，平滑后的值越接近当前时间的数据值，数据越不平滑，α越接近0，平滑后的值越接近前i个数据的平滑值，数据越平滑，α的值通常可以多尝试几次以达到最佳效果。 一次指数平滑算法进行预测的公式为：xi+h=si，其中i为当前最后的一个数据记录的坐标，亦即预测的时间序列为一条直线，不能反映时间序列的趋势和季节性。 12345678910111213141516171819202122232425262728curl -XPOST &apos;localhost:9200/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;my_date_histo&quot;:&#123; &quot;date_histogram&quot;:&#123; &quot;field&quot;:&quot;date&quot;, &quot;interval&quot;:&quot;1M&quot; &#125;, &quot;aggs&quot;:&#123; &quot;the_sum&quot;:&#123; &quot;sum&quot;:&#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;the_movavg&quot;: &#123; &quot;moving_avg&quot;:&#123; &quot;buckets_path&quot;: &quot;the_sum&quot;, &quot;window&quot; : 30, &quot;model&quot; : &quot;ewma&quot;, &quot;settings&quot; : &#123; &quot;alpha&quot; : 0.5 &#125; &#125; &#125; &#125; &#125; &#125;&#125;&apos; 二次指数平滑模型: Holt-Linear计算模型： s2 = α x2 + (1 - α) (s1 + t1) t2 = ß (s2 - s1) + (1 - ß) t1 默认alpha = 0.3 and beta = 0.1 二次指数平滑保留了趋势的信息，使得预测的时间序列可以包含之前数据的趋势。二次指数平滑的预测公式为 xi+h=si+hti 二次指数平滑的预测结果是一条斜的直线。 1234567891011121314151617181920212223242526272829curl -XPOST &apos;localhost:9200/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;my_date_histo&quot;:&#123; &quot;date_histogram&quot;:&#123; &quot;field&quot;:&quot;date&quot;, &quot;interval&quot;:&quot;1M&quot; &#125;, &quot;aggs&quot;:&#123; &quot;the_sum&quot;:&#123; &quot;sum&quot;:&#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;the_movavg&quot;: &#123; &quot;moving_avg&quot;:&#123; &quot;buckets_path&quot;: &quot;the_sum&quot;, &quot;window&quot; : 30, &quot;model&quot; : &quot;holt&quot;, &quot;settings&quot; : &#123; &quot;alpha&quot; : 0.5, &quot;beta&quot; : 0.5 &#125; &#125; &#125; &#125; &#125; &#125;&#125;&apos; 三次指数平滑模型：Holt-Winters无季节模型三次指数平滑在二次指数平滑的基础上保留了季节性的信息，使得其可以预测带有季节性的时间序列。三次指数平滑添加了一个新的参数p来表示平滑后的趋势。 1: Additive Holt-Winters：Holt-Winters加法模型 下面是累加的三次指数平滑123si=α(xi-pi-k)+(1-α)(si-1+ti-1)ti=ß(si-si-1)+(1-ß)ti-1pi=γ(xi-si)+(1-γ)pi-k 其中k为周期 累加三次指数平滑的预测公式为： xi+h=si+hti+pi-k+(h mod k) 1234567891011121314151617181920212223242526272829303132curl -XPOST &apos;localhost:9200/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;my_date_histo&quot;:&#123; &quot;date_histogram&quot;:&#123; &quot;field&quot;:&quot;date&quot;, &quot;interval&quot;:&quot;1M&quot; &#125;, &quot;aggs&quot;:&#123; &quot;the_sum&quot;:&#123; &quot;sum&quot;:&#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;the_movavg&quot;: &#123; &quot;moving_avg&quot;:&#123; &quot;buckets_path&quot;: &quot;the_sum&quot;, &quot;window&quot; : 30, &quot;model&quot; : &quot;holt_winters&quot;, &quot;settings&quot; : &#123; &quot;type&quot; : &quot;add&quot;, &quot;alpha&quot; : 0.5, &quot;beta&quot; : 0.5, &quot;gamma&quot; : 0.5, &quot;period&quot; : 7 &#125; &#125; &#125; &#125; &#125; &#125;&#125;&apos; 2: Multiplicative Holt-Winters：Holt-Winters乘法模型 下式为累乘的三次指数平滑：123si=αxi/pi-k+(1-α)(si-1+ti-1)ti=ß(si-si-1)+(1-ß)ti-1pi=γxi/si+(1-γ)pi-k 其中k为周期 累乘三次指数平滑的预测公式为： xi+h=(si+hti)pi-k+(h mod k) α，ß，γ的值都位于[0,1]之间，可以多试验几次以达到最佳效果。 s,t,p初始值的选取对于算法整体的影响不是特别大，通常的取值为s0=x0,t0=x1-x0,累加时p=0,累乘时p=1. 123456789101112131415161718192021222324252627282930313233curl -XPOST &apos;localhost:9200/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;my_date_histo&quot;:&#123; &quot;date_histogram&quot;:&#123; &quot;field&quot;:&quot;date&quot;, &quot;interval&quot;:&quot;1M&quot; &#125;, &quot;aggs&quot;:&#123; &quot;the_sum&quot;:&#123; &quot;sum&quot;:&#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;the_movavg&quot;: &#123; &quot;moving_avg&quot;:&#123; &quot;buckets_path&quot;: &quot;the_sum&quot;, &quot;window&quot; : 30, &quot;model&quot; : &quot;holt_winters&quot;, &quot;settings&quot; : &#123; &quot;type&quot; : &quot;mult&quot;, &quot;alpha&quot; : 0.5, &quot;beta&quot; : 0.5, &quot;gamma&quot; : 0.5, &quot;period&quot; : 7, &quot;pad&quot; : true &#125; &#125; &#125; &#125; &#125; &#125;&#125;&apos; 预测模型：Prediction使用当前值减去前一个值，其实就是环比增长 1234567891011121314151617181920212223242526curl -XPOST &apos;localhost:9200/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;my_date_histo&quot;:&#123; &quot;date_histogram&quot;:&#123; &quot;field&quot;:&quot;date&quot;, &quot;interval&quot;:&quot;1M&quot; &#125;, &quot;aggs&quot;:&#123; &quot;the_sum&quot;:&#123; &quot;sum&quot;:&#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;the_movavg&quot;: &#123; &quot;moving_avg&quot;:&#123; &quot;buckets_path&quot;: &quot;the_sum&quot;, &quot;window&quot; : 30, &quot;model&quot; : &quot;simple&quot;, &quot;predict&quot; : 10 &#125; &#125; &#125; &#125; &#125;&#125;&apos; 最小化：Minimization某些模型（EWMA，Holt-Linear，Holt-Winters）需要配置一个或多个参数。参数选择可能会非常棘手，有时不直观。此外，这些参数的小偏差有时会对输出移动平均线产生剧烈的影响。 出于这个原因，三个“可调”模型可以在算法上最小化。最小化是一个参数调整的过程，直到模型生成的预测与输出数据紧密匹配为止。最小化并不是完全防护的，并且可能容易过度配合，但是它往往比手动调整有更好的结果。 ewma和holt_linear默认情况下禁用最小化，而holt_winters默认启用最小化。 Holt-Winters最小化是最有用的，因为它有助于提高预测的准确性。 EWMA和Holt-Linear不是很好的预测指标，主要用于平滑数据，所以最小化对于这些模型来说不太有用。 通过最小化参数启用/禁用最小化：”minimize” : true 原始数据数据为SSH login数据其中 IP／user已处理1234567891011121314151617181920212223242526272829303132&#123; &quot;_index&quot;: &quot;logstash-sshlogin-others-success-2017-10&quot;, &quot;_type&quot;: &quot;sshlogin&quot;, &quot;_id&quot;: &quot;AV-weLF8c2nHCDojUbat&quot;, &quot;_version&quot;: 2, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;srcip&quot;: &quot;222.221.238.162&quot;, &quot;dstport&quot;: &quot;&quot;, &quot;pid&quot;: &quot;20604&quot;, &quot;program&quot;: &quot;sshd&quot;, &quot;message&quot;: &quot;dwasw-ibb01:Oct 19 23:38:02 176.231.228.130 sshd[20604]: Accepted publickey for nmuser from 222.221.238.162 port 49484 ssh2&quot;, &quot;type&quot;: &quot;zhongcai-sshlogin&quot;, &quot;ssh_type&quot;: &quot;ssh_successful_login&quot;, &quot;forwarded&quot;: &quot;false&quot;, &quot;manufacturer&quot;: &quot;others&quot;, &quot;IndexTime&quot;: &quot;2017-10&quot;, &quot;path&quot;: &quot;/home/logstash/log/logstash_data/audit10/sshlogin/11.txt&quot;, &quot;number&quot;: 1, &quot;hostname&quot;: &quot;176.231.228.130&quot;, &quot;protocol&quot;: &quot;ssh2&quot;, &quot;@timestamp&quot;: &quot;2017-10-19T15:38:02.000Z&quot;, &quot;ssh_method&quot;: &quot;publickey&quot;, &quot;_hostname&quot;: &quot;dwasw-ibb01&quot;, &quot;@version&quot;: &quot;1&quot;, &quot;host&quot;: &quot;localhost&quot;, &quot;srcport&quot;: &quot;49484&quot;, &quot;dstip&quot;: &quot;&quot;, &quot;category&quot;: &quot;sshlogin&quot;, &quot;user&quot;: &quot;nmuser&quot; &#125;&#125; 利用ES API接口去调用查询数据“interval”: “hour”: hour为单位，这里可以是分钟，小时，天，周，月 “format”: “yyyy-MM-dd HH”: 聚合结果得日期格式 12345&quot;the_sum&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;number&quot; &#125;&#125; number为要聚合得字段 123456789101112131415161718192021222324252627282930313233343536curl -POST &apos;localhost:9200/logstash-sshlogin-others-success-2017-10/sshlogin/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;size&quot;: 0, &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;ssh_type&quot;: &quot;ssh_successful_login&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;hour_sum&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;@timestamp&quot;, &quot;interval&quot;: &quot;hour&quot;, &quot;format&quot;: &quot;yyyy-MM-dd HH&quot; &#125;, &quot;aggs&quot;: &#123; &quot;the_sum&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;number&quot; &#125; &#125;, &quot;the_movavg&quot;: &#123; &quot;moving_avg&quot;: &#123; &quot;buckets_path&quot;: &quot;the_sum&quot;, &quot;window&quot;: 30, &quot;model&quot;: &quot;holt&quot;, &quot;settings&quot;: &#123; &quot;alpha&quot;: 0.5, &quot;beta&quot;: 0.7 &#125; &#125; &#125; &#125; &#125; &#125;&#125;&apos; 得到的结果形式为：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&#123; &quot;took&quot; : 35, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : 206821, &quot;max_score&quot; : 0.0, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;hour_sum&quot; : &#123; &quot;buckets&quot; : [ &#123; &quot;key_as_string&quot; : &quot;2017-09-30 16&quot;, &quot;key&quot; : 1506787200000, &quot;doc_count&quot; : 227, &quot;the_sum&quot; : &#123; &quot;value&quot; : 227.0 &#125; &#125;, &#123; &quot;key_as_string&quot; : &quot;2017-09-30 17&quot;, &quot;key&quot; : 1506790800000, &quot;doc_count&quot; : 210, &quot;the_sum&quot; : &#123; &quot;value&quot; : 210.0 &#125;, &quot;the_movavg&quot; : &#123; &quot;value&quot; : 113.5 &#125; &#125;, &#123; &quot;key_as_string&quot; : &quot;2017-09-30 18&quot;, &quot;key&quot; : 1506794400000, &quot;doc_count&quot; : 365, &quot;the_sum&quot; : &#123; &quot;value&quot; : 365.0 &#125;, &quot;the_movavg&quot; : &#123; &quot;value&quot; : 210.0 &#125; &#125;, ... &#125;&#125; 对应得python代码（查询数据到画图）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283# coding: utf-8from elasticsearch import Elasticsearchimport matplotlib.pyplot as pltfrom matplotlib.font_manager import FontManager, FontPropertiesclass Smooth: def __init__(self,index): self.es = Elasticsearch([&apos;localhost:9200&apos;]) self.index = index # 处理mac中文编码错误 def getChineseFont(self): return FontProperties(fname=&apos;/System/Library/Fonts/PingFang.ttc&apos;) # 对index进行聚合 def agg(self): # &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:SS&quot; dsl = &apos;&apos;&apos; &#123; &quot;size&quot;: 0, &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;ssh_type&quot;: &quot;ssh_successful_login&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;hour_sum&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;@timestamp&quot;, &quot;interval&quot;: &quot;day&quot;, &quot;format&quot;: &quot;dd&quot; &#125;, &quot;aggs&quot;: &#123; &quot;the_sum&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;number&quot; &#125; &#125;, &quot;the_movavg&quot;: &#123; &quot;moving_avg&quot;: &#123; &quot;buckets_path&quot;: &quot;the_sum&quot;, &quot;window&quot;: 30, &quot;model&quot;: &quot;holt_winters&quot;, &quot;settings&quot;: &#123; &quot;alpha&quot;: 0.5, &quot;beta&quot;: 0.7 &#125; &#125; &#125; &#125; &#125; &#125; &#125; &apos;&apos;&apos; res = self.es.search(index=self.index, body=dsl) return res[&apos;aggregations&apos;][&apos;hour_sum&apos;][&apos;buckets&apos;] # 画图 def draw(self): x,y_true,y_pred = [],[],[] for one in self.agg(): x.append(one[&apos;key_as_string&apos;]) y_true.append(one[&apos;the_sum&apos;][&apos;value&apos;]) if &apos;the_movavg&apos; in one.keys(): # 前几条数据没有 the_movavg 字段，故将真实值赋值给pred值 y_pred.append(one[&apos;the_movavg&apos;][&apos;value&apos;]) else: y_pred.append(one[&apos;the_sum&apos;][&apos;value&apos;]) x_line = range(len(x)) plt.figure(figsize=(10,5)) plt.plot(x_line,y_true,color=&quot;r&quot;) plt.plot(x_line,y_pred,color=&quot;g&quot;) plt.xlabel(u&quot;每单位时间&quot;,fontproperties=self.getChineseFont()) #X轴标签 plt.xticks(range(len(x)), x) plt.ylabel(u&quot;聚合结果&quot;,fontproperties=self.getChineseFont()) #Y轴标签 plt.title(u&quot;10月份 SSH 主机登录成功聚合图&quot;,fontproperties=self.getChineseFont()) # 标题 plt.legend([u&quot;True value&quot;,u&quot;Predict value&quot;]) plt.show()smooth = Smooth(&quot;logstash-sshlogin-others-success-2017-10&quot;)print smooth.draw() 结果图示为： 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://yoursite.com/tags/ELK/"},{"name":"ES","slug":"ES","permalink":"http://yoursite.com/tags/ES/"},{"name":"异常检测","slug":"异常检测","permalink":"http://yoursite.com/tags/异常检测/"}]},{"title":"Elasticsearch-DSL部分集合","slug":"ELK/Elasticsearch-DSL部分集合","date":"2017-11-14T09:26:48.000Z","updated":"2018-03-28T16:03:34.089Z","comments":true,"path":"2017/11/14/ELK/Elasticsearch-DSL部分集合/","link":"","permalink":"http://yoursite.com/2017/11/14/ELK/Elasticsearch-DSL部分集合/","excerpt":"ELK是日志收集分析神器，在这篇文章中将会介绍一些ES的常用命令。 点击阅读：ELK Stack 从入门到放弃","text":"ELK是日志收集分析神器，在这篇文章中将会介绍一些ES的常用命令。 点击阅读：ELK Stack 从入门到放弃 DSL中遇到的错误及解决办法分片限制错误1Trying to query 2632 shards, which is over the limit of 1000. This limit exists because querying many shards at the same time can make the job of the coordinating node very CPU and/or memory intensive. It is usually a better idea to have a smaller number of larger shards. Update [action.search.shard_count.limit] to a greater value if you really want to query that many shards at the same time. 解决办法：1234567891011修改该限制数目curl -k -u admin:admin -XPUT &apos;http://localhost:9200/_cluster/settings&apos; -H &apos;Content-Type: application/json&apos; -d&apos; &#123; &quot;persistent&quot; : &#123; &quot;action.search.shard_count.limit&quot; : &quot;5000&quot; &#125;&#125;&apos;-k -u admin:admin 表述如果有权限保护的话可以加上 Fileddate 错误1Fielddata is disabled on text fields by default. Set fielddata=true on [make] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. 解决办法：1234567891011121314cars: 索引名transactions：索引对应的类型color：字段curl -XPUT -k -u admin:admin &apos;localhost:9200/cars/_mapping/transactions?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;properties&quot;: &#123; &quot;color&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fielddata&quot;: true &#125; &#125;&#125;&apos; 指定关键词查询，排序和函数统计指定关键词from 为首个偏移量，size为返回数据的条数12345678910http://10.10.11.139:9200/logstash-nginx-access-*/nginx-access/_search?pretty&#123; &quot;from&quot;:0,size&quot;:1000, &quot;query&quot; : &#123; &quot;term&quot; : &#123; &quot;major&quot; : &quot;55&quot; &#125; &#125;&#125; 添加排序(需要进行mapping设置，asc 为升序 desc为降序)1234567891011&#123; &quot;from&quot;:0,&quot;size&quot;:1000, &quot;sort&quot;:[ &#123;&quot;offset&quot;:&quot;desc&quot;&#125; ], &quot;query&quot; : &#123; &quot;term&quot; : &#123; &quot;major&quot; : &quot;55&quot; &#125; &#125;&#125; mode 方法mode方法包括 min／max／avg／sum／median 假如现在要对price字段进行排序，但是price字段有多个值，这个时候就可以使用mode 方法了。 12345678&#123; &quot;query&quot; : &#123; &quot;term&quot; : &#123; &quot;product&quot; : &quot;chocolate&quot; &#125; &#125;, &quot;sort&quot; : [ &#123;&quot;price&quot; : &#123;&quot;order&quot; : &quot;asc&quot;, &quot;mode&quot; : &quot;avg&quot;&#125;&#125; ]&#125; IP范围和网段查询IP range 搜索错误：1Fielddata is disabled on text fields by default. Set fielddata=true on [clientip] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. 解决办法：1234567891011curl -k -u admin:admin -XPUT &apos;10.10.11.139:9200/logstash-nginx-access-*/_mapping/nginx-access?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;properties&quot;: &#123; &quot;clientip&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fielddata&quot;: true, &quot;norms&quot;: false &#125; &#125;&#125;&apos; 查看某个索引的mapping1curl -k -u admin:admin -XGET http://10.10.11.139:9200/logstash-nginx-access-*/_mapping?pretty (当IP为不可解析使就会出现错误)123456789101112131415http://10.10.11.139:9200/logstash-sshlogin-others-success-*/zhongcai/_search?pretty&#123; &quot;size&quot;:100, &quot;aggs&quot; : &#123; &quot;ip_ranges&quot; : &#123; &quot;ip_range&quot; : &#123; &quot;field&quot; : &quot;clientip&quot;, &quot;ranges&quot; : [ &#123; &quot;to&quot; : &quot;40.77.167.73&quot; &#125;, &#123; &quot;from&quot; : &quot;40.77.167.75&quot; &#125; ] &#125; &#125; &#125;&#125; 网段查询1234567891011121314http://10.10.11.139:9200/logstash-sshlogin-others-success-*/zhongcai/_search?pretty&#123; &quot;aggs&quot; : &#123; &quot;ip_ranges&quot; : &#123; &quot;ip_range&quot; : &#123; &quot;field&quot; : &quot;ip&quot;, &quot;ranges&quot; : [ &#123; &quot;mask&quot; : &quot;172.21.202.0/24&quot; &#125;, &#123; &quot;mask&quot; : &quot;172.21.202.0/24&quot; &#125; ] &#125; &#125; &#125;&#125; 关于索引的操作删除某个索引-k -u admin:admin 为用户名：密码1curl -XDELETE -k -u admin:admin &apos;http://localhost:9200/my_index&apos; 查看某个索引的Mapping1curl -XGET &quot;http://127.0.0.1:9200/my_index/_mapping?pretty&quot; 索引数据迁移Es索引reindex(从ip_remote上迁移到本地)123456789101112131415curl -XPOST &apos;localhost:9200/_reindex?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;source&quot;: &#123; &quot;remote&quot;: &#123; &quot;host&quot;: &quot;http://ip_remote:9200&quot;, &quot;username&quot;: &quot;username&quot;, &quot;password&quot;: &quot;passwd&quot; &#125;, &quot;index&quot;: &quot;old_index&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_index&quot; &#125;&#125;&apos; 为某个索引添加字段添加number字段： 唯一ID1234567curl -POST &apos;http://127.0.0.1:9200/my_idnex/my_index_type/id/_update?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;doc&quot; : &#123; &quot;number&quot; : 1 &#125;&#125;&apos; 批量操作123456789101112curl -XPOST &apos;localhost:9200/logstash-sshlogin-others-success-2017-*/_update_by_query?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;script&quot;: &#123; &quot;inline&quot;: &quot;ctx._source.number=1&quot;, &quot;lang&quot;: &quot;painless&quot; &#125;, &quot;query&quot;: &#123; &quot;match_all&quot;: &#123; &#125; &#125;&#125;&apos; 根据指定条件进行聚合每小时成功登录的次数进行聚合123456789101112131415161718curl -POST &apos;http://127.0.0.1:9200/logstash-sshlogin-others-success-2017-*/zhongcai-sshlogin/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;ssh_type&quot;: &quot;ssh_successful_login&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;sums&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;@timestamp&quot;, &quot;interval&quot;: &quot;hour&quot;, &quot;format&quot;: &quot;yyyy-MM-dd HH&quot; &#125; &#125; &#125;&#125;&apos; 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://yoursite.com/tags/ELK/"},{"name":"ES","slug":"ES","permalink":"http://yoursite.com/tags/ES/"}]},{"title":"数据结构算法之链表","slug":"数据结构/数据结构算法之链表","date":"2017-11-12T16:58:37.000Z","updated":"2018-04-14T19:36:47.031Z","comments":true,"path":"2017/11/13/数据结构/数据结构算法之链表/","link":"","permalink":"http://yoursite.com/2017/11/13/数据结构/数据结构算法之链表/","excerpt":"链表面试总结，使用python实现，参考：https://www.cnblogs.com/lixiaohui-ambition/archive/2012/09/25/2703195.html","text":"链表面试总结，使用python实现，参考：https://www.cnblogs.com/lixiaohui-ambition/archive/2012/09/25/2703195.html 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394#coding:utf-8# 定义链表class ListNode: def __init__(self): self.data = None self.pnext = None# 链表操作类class ListNode_handle: def __init__(self): self.cur_node = None # 链表添加元素 def add(self,data): ln = ListNode() ln.data = data ln.pnext = self.cur_node self.cur_node = ln return ln # 打印链表 def prt(self,ln): while ln: print(ln.data,end=&quot; &quot;) ln = ln.pnext # 逆序输出 def _reverse(self,ln): _list = [] while ln: _list.append(ln.data) ln = ln.pnext ln_2 = ListNode() ln_h = ListNode_handle() for i in _list: ln_2 = ln_h.add(i) return ln_2 # 求链表的长度 def _length(self,ln): _len = 0 while ln: _len += 1 ln = ln.pnext return _len # 查找指定位置的节点 def _find_loc(self,ln,loc): _sum = 0 while ln and _sum != loc: _sum += 1 ln = ln.pnext return ln.data # 判断某个节点是否在链表中 def _exist(self,ln,data): flag = False while ln and data != ln.data: ln = ln.pnext return flag# 创建链表 ln = ListNode()ln_h = ListNode_handle()a = [1,4,2,5,8,5,7,9]for i in a: ln = ln_h.add(i)print(&quot;正序输出...&quot;)ln_h.prt(ln)print(&quot;\\n\\n逆序输出...&quot;)ln_2 = ln_h._reverse(ln)ln_h.prt(ln_2)# 求链表ln的长度length = ln_h._length(ln)print(&quot;\\n\\nln的长度为:&quot;,length)# 查找链表ln中的倒数第３个节点data = ln_h._find_loc(ln,ln_h._length(ln)-3)print(&quot;\\n\\n倒数第三个节点为:&quot;,data)# 返回某个节点在链表中的位置loc = ln_h._loc(ln,5)# 判断某个节点是否在链表中flag = ln_h._exist(ln,5)print(&quot;\\n\\n５是否存在与链表ln中:&quot;,end=&quot; &quot;)if flag: print(&quot;Yes&quot;)else: print(&quot;No&quot;) 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"编程珠玑","slug":"编程珠玑","permalink":"http://yoursite.com/categories/编程珠玑/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/tags/数据结构/"}]},{"title":"数据结构算法之合并两个有序序列","slug":"数据结构/数据结构算法之合并两个有序序列","date":"2017-11-12T16:55:29.000Z","updated":"2018-04-14T19:36:49.511Z","comments":true,"path":"2017/11/13/数据结构/数据结构算法之合并两个有序序列/","link":"","permalink":"http://yoursite.com/2017/11/13/数据结构/数据结构算法之合并两个有序序列/","excerpt":"有序序列的合并，python实现。","text":"有序序列的合并，python实现。 12345678910111213141516171819202122232425#coding:utf-8a = [2,4,6,8,10]b = [3,5,7,9,11,13,15]c = []def merge(a,b): i,j = 0,0 while i&lt;=len(a)-1 and j&lt;=len(b)-1: if a[i]&lt;b[j]: c.append(a[i]) i+=1 else: c.append(b[j]) j+=1 if i&lt;=len(a)-1: for m in a[i:]: c.append(m) if j&lt;=len(b)-1: for n in b[j:]: c.append(n) print(c)merge(a,b) 运行结果为：1[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15] 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"编程珠玑","slug":"编程珠玑","permalink":"http://yoursite.com/categories/编程珠玑/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/tags/数据结构/"}]},{"title":"数据结构算法之排序","slug":"数据结构/数据结构算法之排序","date":"2017-11-12T16:51:28.000Z","updated":"2018-04-14T19:36:48.675Z","comments":true,"path":"2017/11/13/数据结构/数据结构算法之排序/","link":"","permalink":"http://yoursite.com/2017/11/13/数据结构/数据结构算法之排序/","excerpt":"数据结构面试中经常会被问到篇排序相关的问题，那么这篇文章会研究下怎么用python来实现排序。","text":"数据结构面试中经常会被问到篇排序相关的问题，那么这篇文章会研究下怎么用python来实现排序。 冒泡排序12345678910111213#coding：utf-8# 冒泡排序def maopao(): a = [2,1,4,3,9,5,6,8,7] for i in range(len(a)-1): for j in range(len(a)-1-i): if a[j]&gt;a[j+1]: temp = a[j] a[j] = a[j+1] a[j+1] = temp print(a)maopao() 结果为：1[1, 2, 3, 4, 5, 6, 7, 8, 9] 归并排序1234567891011121314151617181920212223242526272829# 归并排序def merge(a,b): i,j = 0,0 c = [] while i&lt;=len(a)-1 and j&lt;=len(b)-1: if a[i]&lt;b[j]: c.append(a[i]) i+=1 else: c.append(b[j]) j+=1 if i&lt;=len(a)-1: for m in a[i:]: c.append(m) if j&lt;=len(b)-1: for n in b[j:]: c.append(n) return cdef guibing(a): if len(a)&lt;=1: return a center = int(len(a)/2) left = guibing(a[:center]) right = guibing(a[center:]) return merge(left,right)print(guibing([2,1,4,3,9,5,6,8,7])) 结果为：1[1, 2, 3, 4, 5, 6, 7, 8, 9] 快速排序123456789101112131415161718192021222324252627# 快速排序 def kpsort(left,right,a): based = a[left] i = left j = right while i &lt; j: # 从数组右边开始遍历 while a[j]&gt;=based and i&lt;j: j -= 1 a[i] = a[j] while a[i]&lt;=based and i&lt;j: i += 1 a[j]= a[i] a[i] = based return i def kuaipai(left,right,a): if left&lt;right: p = kpsort(left,right,a) kuaipai(left,p-1,a) kuaipai(p+1,right,a) return a print(kuaipai(0,8,a =[2,1,4,3,9,5,6,8,7])) 结果为1[1, 2, 3, 4, 5, 6, 7, 8, 9] 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"编程珠玑","slug":"编程珠玑","permalink":"http://yoursite.com/categories/编程珠玑/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/tags/数据结构/"}]},{"title":"数据结构算法之二叉树","slug":"数据结构/数据结构算法之二叉树","date":"2017-11-12T16:44:41.000Z","updated":"2018-04-14T19:36:50.351Z","comments":true,"path":"2017/11/13/数据结构/数据结构算法之二叉树/","link":"","permalink":"http://yoursite.com/2017/11/13/数据结构/数据结构算法之二叉树/","excerpt":"数据结构面试中经常会被问到篇二叉树相关的问题，那么这篇文章会研究下怎么用python来进行二叉树的构建和遍历。","text":"数据结构面试中经常会被问到篇二叉树相关的问题，那么这篇文章会研究下怎么用python来进行二叉树的构建和遍历。 注意：py2中1print root.elem, 在py3中要换成1print (root.elem,end=&quot; &quot;) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140# coding:utf-8# 定义节点类class Node: def __init__(self,elem = -1,): self.elem = elem self.left = None self.right = None # 定义二叉树class Tree: def __init__(self): self.root = Node() self.myqu = [] # 添加节点 def add(self,elem): node = Node(elem) if self.root.elem == -1: # 判断如果是根节点 self.root = node self.myqu.append(self.root) else: treenode = self.myqu[0] if treenode.left == None: treenode.left = node self.myqu.append(treenode.left) else: treenode.right = node self.myqu.append(treenode.right) self.myqu.pop(0) # 利用递归实现树的先序遍历 def xianxu(self,root): if root == None: return print root.elem, self.xianxu(root.left) self.xianxu(root.right) # 利用递归实现树的中序遍历 def zhongxu(self,root): if root == None: return self.zhongxu(root.left) print root.elem, self.zhongxu(root.right) # 利用递归实现树的后序遍历 def houxu(self,root): if root == None: return self.houxu(root.left) self.houxu(root.right) print root.elem, # 利用队列实现层次遍历 def cengci(self,root): if root == None: return myq = [] node = root myq.append(node) while myq: node = myq.pop(0) print node.elem, if node.left != None: myq.append(node.left) if node.right != None: myq.append(node.right) # 求树的叶子节点 def getYeJiedian(self,root): if root == None: return 0 if root.left == None and root.right == None: return 1 return self.getYeJiedian(root.left) + self.getYeJiedian(root.right) # 由先序和中序,还原二叉树 def preMidToHou(self,pre,mid): if len(pre)==0: return None if len(pre)==1: Node(mid[0]) root = Node(pre[0]) root_index = mid.index(pre[0]) root.left = self.preMidToHou(pre[1:root_index + 1],mid[:root_index]) root.right = self.preMidToHou(pre[root_index + 1:],mid[root_index + 1:]) return root # 由后序和中序,还原二叉树 def preMidToHou(self,mid,hou): if len(hou)==0: return None if len(hou)==1: Node(mid[0]) root = Node(hou[-1]) root_index = mid.index(hou[-1]) root.left = self.preMidToHou(mid[:root_index],hou[:root_index]) root.right = self.preMidToHou(mid[root_index + 1:],mid[root_index + 1:]) return root# 创建一个树，添加节点tree = Tree()for i in range(10): tree.add(i) print(&quot;二叉树的先序遍历:&quot;)print(tree.xianxu(tree.root))print(&quot;二叉树的中序遍历:&quot;)print(tree.zhongxu(tree.root))print(&quot;二叉树的后序遍历:&quot;)print(tree.houxu(tree.root))print(&quot;二叉树的层次遍历&quot;)print(tree.cengci(tree.root))print(&quot;\\n二叉树的叶子节点为:&quot;)print(tree.getYeJiedian(tree.root))print(&quot;\\n已知二叉树先序遍历和中序遍历，求后序:&quot;)print(&quot;先序:&quot;)print(tree.xianxu(tree.root))print(&quot;中序:&quot;)print(tree.zhongxu(tree.root))print(&quot;后序:&quot;)root = tree.preMidToHou([0,1,3,7,8,4,9,2,5,6],[7,3,8,1,9,4,0,5,2,6])print(tree.houxu(root))print(&quot;\\n已知二叉树后序遍历和中序遍历，求前序:&quot;)print(&quot;后序:&quot;)print(tree.houxu(tree.root))print(&quot;中序:&quot;)print(tree.zhongxu(tree.root))print(&quot;前序:&quot;)root = tree.preMidToHou([7,3,8,1,9,4,0,5,2,6],[7,8,3,9,4,1,5,6,2,0])print(tree.xianxu(root)) 运行结果为：123456789101112131415161718192021222324252627282930二叉树的先序遍历:0 1 3 7 8 4 9 2 5 6 None二叉树的中序遍历:7 3 8 1 9 4 0 5 2 6 None二叉树的后序遍历:7 8 3 9 4 1 5 6 2 0 None二叉树的层次遍历0 1 2 3 4 5 6 7 8 9 None二叉树的叶子节点为:5已知二叉树先序遍历和中序遍历，求后序:先序:0 1 3 7 8 4 9 2 5 6 None中序:7 3 8 1 9 4 0 5 2 6 None后序:1 3 7 8 4 9 0 5 2 6 None已知二叉树后序遍历和中序遍历，求前序:后序:7 8 3 9 4 1 5 6 2 0 None中序:7 3 8 1 9 4 0 5 2 6 None前序:0 1 3 7 8 4 9 6 2 5 None 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"编程珠玑","slug":"编程珠玑","permalink":"http://yoursite.com/categories/编程珠玑/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/tags/数据结构/"}]},{"title":"回归分析之Sklearn实现电力预测","slug":"机器学习/回归分析之Sklearn实现电力预测","date":"2017-11-07T05:39:15.000Z","updated":"2018-03-28T15:58:38.473Z","comments":true,"path":"2017/11/07/机器学习/回归分析之Sklearn实现电力预测/","link":"","permalink":"http://yoursite.com/2017/11/07/机器学习/回归分析之Sklearn实现电力预测/","excerpt":"参考原文：http://www.cnblogs.com/pinard/p/6016029.html这里进行了手动实现，增强记忆。","text":"参考原文：http://www.cnblogs.com/pinard/p/6016029.html这里进行了手动实现，增强记忆。 1：数据集介绍使用的数据是UCI大学公开的机器学习数据 数据的介绍在这： http://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant 数据的下载地址在这：http://archive.ics.uci.edu/ml/machine-learning-databases/00294/ 里面是一个循环发电场的数据，共有9568个样本数据，每个数据有5列，分别是:AT（温度）, V（压力）, AP（湿度）, RH（压强）, PE（输出电力)。我们不用纠结于每项具体的意思。 我们的问题是得到一个线性的关系，对应PE是样本输出，而AT/V/AP/RH这4个是样本特征， 机器学习的目的就是得到一个线性回归模型，即: PE = \\theta _{0} + \\theta _{0} * AT + \\theta _{0} * V +\\theta _{0} * AP +\\theta _{0}*RH而需要学习的，就是θ0,θ1,θ2,θ3,θ4这5个参数。 2：准备数据下载源数据之后，解压会得到一个xlsx的文件，打开另存为csv文件，数据已经整理好，没有非法数据，但是数据并没有进行归一化，不过这里我们可以使用sklearn来帮我处理 sklearn的归一化处理参考：http://blog.csdn.net/gamer_gyt/article/details/77761884 3：使用pandas来进行数据的读取1234import pandas as pd# pandas 读取数据data = pd.read_csv(&quot;Folds5x2_pp.csv&quot;)data.head() 然后会看到如下结果，说明数据读取成功： 123456 AT V AP RH PE0 8.34 40.77 1010.84 90.01 480.481 23.64 58.49 1011.40 74.20 445.752 29.74 56.90 1007.15 41.91 438.763 19.07 49.69 1007.22 76.79 453.094 11.80 40.66 1017.13 97.20 464.43 4：准备运行算法的数据1234X = data[[&quot;AT&quot;,&quot;V&quot;,&quot;AP&quot;,&quot;RH&quot;]]print X.shapey = data[[&quot;PE&quot;]]print y.shape 12(9568, 4)(9568, 1) 说明有9658条数据，其中”AT”,”V”,”AP”,”RH” 四列作为样本特征，”PE”列作为样本输出。 5：划分训练集和测试集12345678from sklearn.cross_validation import train_test_split# 划分训练集和测试集X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=1)print X_train.shapeprint y_train.shapeprint X_test.shapeprint y_test.shape 1234(7176, 4)(7176, 1)(2392, 4)(2392, 1) 75%的数据被划分为训练集，25的数据划分为测试集。 6：运行sklearn 线性模型12345678from sklearn.linear_model import LinearRegressionlinreg = LinearRegression()linreg.fit(X_train,y_train)# 训练模型完毕，查看结果print linreg.intercept_print linreg.coef_ 12[ 447.06297099][[-1.97376045 -0.23229086 0.0693515 -0.15806957]] 即我们得到的模型结果为： PE = 447.06297099 - 1.97376045*AT - 0.23229086*V + 0.0693515*AP -0.15806957*RH 7：模型评价我们需要评价模型的好坏，通常对于线性回归来讲，我么一般使用均方差（MSE，Mean Squared Error）或者均方根差（RMSE，Root Mean Squared Error）来评价模型的好坏 123456y_pred = linreg.predict(X_test)from sklearn import metrics# 使用sklearn来计算mse和Rmseprint &quot;MSE:&quot;,metrics.mean_squared_error(y_test, y_pred)print &quot;RMSE:&quot;,np.sqrt(metrics.mean_squared_error(y_test, y_pred)) 12MSE: 20.0804012021RMSE: 4.48111606657 得到了MSE或者RMSE，如果我们用其他方法得到了不同的系数，需要选择模型时，就用MSE小的时候对应的参数。 8：交叉验证我们可以通过交叉验证来持续优化模型，代码如下，我们采用10折交叉验证，即cross_val_predict中的cv参数为10： 12345# 交叉验证from sklearn.model_selection import cross_val_predictpredicted = cross_val_predict(linreg,X,y,cv=10)print &quot;MSE:&quot;,metrics.mean_squared_error(y, predicted)print &quot;RMSE:&quot;,np.sqrt(metrics.mean_squared_error(y, predicted)) 12MSE: 20.7955974619RMSE: 4.56021901469 可以看出，采用交叉验证模型的MSE比第6节的大，主要原因是我们这里是对所有折的样本做测试集对应的预测值的MSE，而第6节仅仅对25%的测试集做了MSE。两者的先决条件并不同。 9：画图查看结果12345678# 画图查看结果import matplotlib.pyplot as pltfig, ax = plt.subplots()ax.scatter(y, predicted)ax.plot([y.min(), y.max()], [y.min(), y.max()], &apos;k--&apos;, lw=4)ax.set_xlabel(&apos;Measured&apos;)ax.set_ylabel(&apos;Predicted&apos;)plt.show() 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"回归分析","slug":"回归分析","permalink":"http://yoursite.com/tags/回归分析/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"}]},{"title":"回归分析之线性回归（N元线性回归）","slug":"机器学习/回归分析之线性回归（N元线性回归）","date":"2017-09-29T08:45:14.000Z","updated":"2018-03-28T15:58:33.217Z","comments":true,"path":"2017/09/29/机器学习/回归分析之线性回归（N元线性回归）/","link":"","permalink":"http://yoursite.com/2017/09/29/机器学习/回归分析之线性回归（N元线性回归）/","excerpt":"在上一篇文章中我们介绍了 回归分析之理论篇，在其中我们有聊到线性回归和非线性回归，包括广义线性回归，这一篇文章我们来聊下回归分析中的线性回归。","text":"在上一篇文章中我们介绍了 回归分析之理论篇，在其中我们有聊到线性回归和非线性回归，包括广义线性回归，这一篇文章我们来聊下回归分析中的线性回归。 一元线性回归预测房价：输入编号 | 平方米 | 价格-|-|-1 | 150 | 64502 | 200 | 74503| 250 |84504| 300 |94505| 350 |114506| 400 |154507| 600| 18450 针对上边这种一元数据来讲，我们可以构建的一元线性回归函数为 H(x) = k*x + b其中H(x)为平方米价格表，k是一元回归系数，b为常数。最小二乘法的公式： k =\\frac{ \\sum_{1}^{n} (x_{i} - \\bar{x} )(y_{i} - \\bar{y}) } { \\sum_{1}^{n}(x_{i}-\\bar{x})^{2} }自己使用python代码实现为：123456789101112131415161718def leastsq(x,y): &quot;&quot;&quot; x,y分别是要拟合的数据的自变量列表和因变量列表 &quot;&quot;&quot; meanX = sum(x) * 1.0 / len(x) # 求x的平均值 meanY = sum(y) * 1.0 / len(y) # 求y的平均值 xSum = 0.0 ySum = 0.0 for i in range(len(x)): xSum += (x[i] - meanX) * (y[i] - meanY) ySum += (x[i] - meanX) ** 2 k = ySum/xSum b = ySum - k * meanX return k,b 使用python的scipy包进行计算:12345678910111213141516171819202122232425262728leastsq(func, x0, args=(), Dfun=None, full_output=0, col_deriv=0, ftol=1.49012e-08, xtol=1.49012e-08, gtol=0.0, maxfev=0, epsfcn=None, factor=100, diag=None)from scipy.optimize import leastsqimport numpy as npdef fun(p, x): &quot;&quot;&quot; 定义想要拟合的函数 &quot;&quot;&quot; k,b = p # 从参数p获得拟合的参数 return k*x + bdef err(p, x, y): return fun(p,x) - y#定义起始的参数 即从 y = 1*x+1 开始，其实这个值可以随便设，只不过会影响到找到最优解的时间p0 = [1,1]#将list类型转换为 numpy.ndarray 类型，最初我直接使用#list 类型,结果 leastsq函数报错，后来在别的blog上看到了，原来要将类型转#换为numpy的类型x1 = np.array([150,200,250,300,350,400,600])y1 = np.array([6450,7450,8450,9450,11450,15450,18450])xishu = leastsq(err, p0, args=(x1,y1))print xishu[0] 当然python的leastsq函数不仅仅局限于一元一次的应用，也可以应用到一元二次，二元二次，多元多次等，具体可以看下这篇博客：http://www.cnblogs.com/NanShan2016/p/5493429.html 多元线性回归总之：我们可以用python leastsq函数解决几乎所有的线性回归的问题了，比如说 y = a * x^2 + b * x + cy = a * x_1^2 + b * x_1 + c * x_2 + dy = a * x_1^3 + b * x_1^2 + c * x_1 + d在使用时只需把参数列表和 fun 函数中的return 换一下，拿以下函数举例 y = a * x_1^2 + b * x_1 + c * x_2 + d对应的python 代码是：12345678910111213141516171819202122232425262728from scipy.optimize import leastsqimport numpy as npdef fun(p, x1, x2): &quot;&quot;&quot; 定义想要拟合的函数 &quot;&quot;&quot; a,b,c,d = p # 从参数p获得拟合的参数 return a * (x1**2) + b * x1 + c * x2 + ddef err(p, x1, x2, y): return fun(p,x1,x2) - y#定义起始的参数 即从 y = 1*x+1 开始，其实这个值可以随便设，只不过会影响到找到最优解的时间p0 = [1,1,1,1]#将list类型转换为 numpy.ndarray 类型，最初我直接使用#list 类型,结果 leastsq函数报错，后来在别的blog上看到了，原来要将类型转#换为numpy的类型x1 = np.array([150,200,250,300,350,400,600]) # 面积x2 = np.array([4,2,7,9,12,14,15]) # 楼层y1 = np.array([6450,7450,8450,9450,11450,15450,18450]) # 价格/平方米xishu = leastsq(err, p0, args=(x1,x2,y1))print xishu[0] sklearn中的线性回归应用普通最小二乘回归这里我们使用的是sklearn中的linear_model来模拟y=a * x_1 + b * x_2 + c 1234567891011121314151617In [1]: from sklearn.linear_model import LinearRegressionIn [2]: linreg = LinearRegression()In [3]: linreg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])In [4]: linreg.coef_Out[4]: array([ 0.5, 0.5])In [5]: linreg.intercept_Out[5]: 1.1102230246251565e-16In [6]: linreg.predict([4,4])Out[6]: array([ 4.])In [7]: zip([&quot;x1&quot;,&quot;x2&quot;], linreg.coef_)Out[7]: [(&apos;x1&apos;, 0.5), (&apos;x2&apos;, 0.49999999999999989)] 所以可得y = 0.5 * x_1 + 0.5 * x_2 + 1.11e-16 linreg.coef_ 为系数 a,b linreg.intercept_ 为截距 c 缺点：因为系数矩阵x与它的转置矩阵相乘得到的矩阵不能求逆，导致最小二乘法得到的回归系数不稳定，方差很大。 多项式回归：基函数扩展线性模型机器学习中一种常见的模式是使用线性模型训练数据的非线性函数。这种方法保持了一般快速的线性方法的性能，同时允许它们适应更广泛的数据范围。 例如，可以通过构造系数的多项式特征来扩展一个简单的线性回归。在标准线性回归的情况下，你可能有一个类似于二维数据的模型： y(w,x) = w_{0} + w_{1} x_{1} + w_{2} x_{2}如果我们想把抛物面拟合成数据而不是平面，我们可以结合二阶多项式的特征，使模型看起来像这样: y(w,x) = w_{0} + w_{1} x_{1} + w_{2} x_{2} + w_{3} x_{1}x_{2} + w_{4} x_{1}^2 + w_{5} x_{2}^2我们发现，这仍然是一个线性模型，想象着创建一个新变量： z = [x_{1},x_{2},x_{1} x_{2},x_{1}^2,x_{2}^2]可以把线性回归模型写成下边这种形式： y(w,x) = w_{0} + w_{1} z_{1} + w_{2} z_{2} + w_{3} z_{3} + w_{4} z_{4} + w_{5} z_{5}我们看到，所得的多项式回归与我们上面所考虑的线性模型相同（即模型在W中是线性的），可以用同样的方法来求解。通过考虑在用这些基函数建立的高维空间中的线性拟合，该模型具有灵活性，可以适应更广泛的数据范围。 使用如下代码，将二维数据进行二元转换,转换规则为： [x_1, x_2] => [1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]12345678910111213141516171819In [15]: from sklearn.preprocessing import PolynomialFeaturesIn [16]: import numpy as npIn [17]: X = np.arange(6).reshape(3,2)In [18]: XOut[18]: array([[0, 1], [2, 3], [4, 5]])In [19]: poly = PolynomialFeatures(degree=2)In [20]: poly.fit_transform(X)Out[20]: array([[ 1., 0., 1., 0., 0., 1.], [ 1., 2., 3., 4., 6., 9.], [ 1., 4., 5., 16., 20., 25.]]) 验证：1234567891011121314151617181920212223242526In [38]: from sklearn.preprocessing import PolynomialFeaturesIn [39]: from sklearn.linear_model import LinearRegressionIn [40]: from sklearn.pipeline import PipelineIn [41]: import numpy as npIn [42]: In [42]: model = Pipeline( [ (&quot;poly&quot;,PolynomialFeatures(degree=3)),(&quot;linear&quot;,LinearRegression(fit_intercept=False)) ] )In [43]: modelOut[43]: Pipeline(steps=[(&apos;poly&apos;, PolynomialFeatures(degree=3, include_bias=True, interaction_only=False)), (&apos;linear&apos;, LinearRegression(copy_X=True, fit_intercept=False, n_jobs=1, normalize=False))])In [44]: x = np.arange(5)In [45]: y = 3 - 2 * x + x ** 2 - x ** 3In [46]: yOut[46]: array([ 3, 1, -5, -21, -53])In [47]: model = model.fit(x[:,np.newaxis],y)In [48]: model.named_steps[&apos;linear&apos;].coef_Out[48]: array([ 3., -2., 1., -1.]) 我们可以看出最后求出的参数和一元三次方程是一致的。 这里如果把degree改为2，y的方程也换一下，结果也是一致的123456789101112131415161718In [51]: from sklearn.linear_model import LinearRegressionIn [52]: from sklearn.preprocessing import PolynomialFeaturesIn [53]: from sklearn.pipeline import PipelineIn [54]: import numpy as npIn [55]: model = Pipeline( [ (&quot;poly&quot;,PolynomialFeatures(degree=2)),(&quot;linear&quot;,LinearRegression(fit_intercept=False)) ] )In [56]: x = np.arange(5)In [57]: y = 3 + 2 * x + x ** 2In [58]: model = model.fit(x[:, np.newaxis], y)In [59]: model.named_steps[&apos;linear&apos;].coef_Out[59]: array([ 3., 2., 1.]) 线性回归的评测在上一篇文章中我们聊到了回归模型的评测方法，解下来我们详细聊聊如何来评价一个回归模型的好坏。 这里我们定义预测值和真实值分别为：12true = [10, 5, 3, 2]pred = [9, 5, 5, 3] 1: 平均绝对误差（Mean Absolute Error, MAE） \\frac{1}{N}(\\sum_{1}^{n} |y_i - \\bar{y}|)2: 均方误差（Mean Squared Error, MSE） \\frac{1}{N}\\sum_{1}^{n}(y_i - \\bar{y})^23: 均方根误差（Root Mean Squared Error, RMSE） \\frac{1}{N} \\sqrt{ \\sum_{1}^{n}(y_i - \\bar{y})^2 }12345678910111213141516171819202122232425In [80]: from sklearn import metricsIn [81]: import numpy as npIn [82]: true = [10, 5, 3, 2]In [83]: pred = [9, 5, 5, 3]In [84]: print(&quot;MAE: &quot;, metrics.mean_absolute_error(true,pred))(&apos;MAE: &apos;, 1.0)In [85]: print(&quot;MAE By Hand: &quot;, (1+0+2+1)/4.)(&apos;MAE By Hand: &apos;, 1.0)In [86]: print(&quot;MSE: &quot;, metrics.mean_squared_error(true,pred))(&apos;MSE: &apos;, 1.5)In [87]: print(&quot;MSE By Hand: &quot;, (1 ** 2 + 0 ** 2 + 2 ** 2 + 1 ** 2 ) / 4.)(&apos;MSE By Hand: &apos;, 1.5)In [88]: print(&quot;RMSE: &quot;, np.sqrt(metrics.mean_squared_error(true,pred)))(&apos;RMSE: &apos;, 1.2247448713915889)In [89]: print(&quot;RMSE By Hand: &quot;, np.sqrt((1 ** 2 + 0 ** 2 + 2 ** 2 + 1 ** 2 ) / 4.))(&apos;RMSE By Hand: &apos;, 1.2247448713915889) 总结线性回归在现实中还是可以解决很多问题的，但是并不是万能的，后续我会继续整理逻辑回归，岭回归等相关回归的知识，如果你感觉有用，欢迎分享！ 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"回归分析","slug":"回归分析","permalink":"http://yoursite.com/tags/回归分析/"}]},{"title":"几种距离计算公式在数据挖掘中的应用场景分析","slug":"机器学习/几种距离计算公式在数据挖掘中的应用场景分析","date":"2017-09-20T02:23:39.000Z","updated":"2018-03-28T15:58:18.257Z","comments":true,"path":"2017/09/20/机器学习/几种距离计算公式在数据挖掘中的应用场景分析/","link":"","permalink":"http://yoursite.com/2017/09/20/机器学习/几种距离计算公式在数据挖掘中的应用场景分析/","excerpt":"本文涉及以下几种距离计算公式的分析，参考资料为《面向程序员的数据挖掘指南》 曼哈顿距离 欧几里得距离 闵可夫斯基距离 皮尔逊相关系数 余弦相似度","text":"本文涉及以下几种距离计算公式的分析，参考资料为《面向程序员的数据挖掘指南》 曼哈顿距离 欧几里得距离 闵可夫斯基距离 皮尔逊相关系数 余弦相似度 之前整理过一篇关于距离相关的文章：机器学习算法中的距离和相似性计算公式，分析以及python实现 闵可夫斯基距离两个n维变量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的闵可夫斯基距离定义为： \\sqrt[p]{ \\sum_{k=1}^{n} \\left | x_{1k}-x_{2k} \\right |^p}其中p是一个变参数。 当p=1时，就是曼哈顿距离 当p=2时，就是欧氏距离 当p→∞时，就是切比雪夫距离 根据变参数的不同，闵氏距离可以表示一类的距离。 p值越大，单个维度的差值大小会对整体距离有更大的影响 曼哈顿距离／欧几里得距离的瑕疵在《面向程序员的数据挖掘指南》中给出了这样一组样例数据, 下图为一个在线音乐网站的的用户评分情况，用户可以用1-5星来评价一个乐队，下边是8位用户对8个乐队的评价： 表中的横线表示用户没有对乐队进行评价，我们在计算两个用户的距离时，只采用他们都评价过的乐队。 现在来求Angelica和Bill的距离，因为他们共同评分过的乐队有5个，所以使用其对该5个乐队的评分进行曼哈顿距离的计算为： 1Dis_1 = |3.5-2| + |2-3.5| + |5-2| + |1.5-3.5| + |2-3| = 9 同样使用欧式距离计算为：1Dis_2 = sqrt( (3.5-2)^2 + (2-3.5)^2 + (5-2)^2 + (1.5-3.5)^2 + (2-3)^2 ) = 4.3 当对Angelica和Bill，Bill和Chan进行距离对比时，由于两者的共同评分过的乐队均为5，数据都在一个5维空间里，是公平的，如果现在要计算Angelica和Hailey与Bill的距离时，会发现，Angelica与Bill共同评分的有5个乐队，Hailey与Bill共同评分的有3个乐队，也就是说两者数据一个在5维空间里，一个在三维空间里，这样明显是不公平的。这将会对我们进行计算时产生不好的影响，所以曼哈顿距离和欧几里得距离在数据完整的情况下效果最好。 用户问题／皮尔逊相关系数／分数膨胀现象——用户问题仔细观察用户对乐队的评分数据，可以发现每个用户的评分标准不同： Bill没有打出极端的分数，都在2-4分之间 Jordyn似乎喜欢所有的乐队，打分都在4-5之间 Hailey是一个有趣的人，他的评分不是1就是4 那么如何比较这些用户呢？比如说Hailey的4分是相当于Jordyn的4分还是5分呢？我觉得更接近5分，这样一来，就影响推荐系统的准确性了！ 解决该现象解决该现象的办法之一就是 使用皮尔逊相关系数，例如下边这样的数据样例（Clara和Robert对五个乐队的评分）： 这种现象在数据挖掘领域被称为“分数膨胀“。我们将其评分画成图，如下： 一条直线-完全吻合，代表着Clara和Robert的喜好完全一致。 皮尔逊相关系数用于衡量两个变量之间的相关性，他的值在-1～1，1代表完全一致，-1代表完全相悖。所以我们可以利用皮尔逊相关系数来找到相似的用户。 皮尔逊相关系数的计算公式为：该公式除了看起来比较复杂，另外需要对数据进行两次遍历，第一次遍历求出 x平均值和y平均值，第二次遍历才能出现结果，这里提供另外一个计算公式，能够计算皮尔逊相关系数的近似值： 余弦相似度／稀疏数据假设这样一个数据集，一个在线音乐网站，有10000w首音乐（这里不考虑音乐类型，年代等因素），每个用户常听的也就其中的几十首，这种情况下使用曼哈顿或者欧几里得或者皮尔逊相关系数进行计算用户之间相似性，计算相似值会非常小，因为用户之间的交集本来就很少，这样对于计算结果来讲是很不准确的，这个时候就需要余弦相似度了，余弦相似度进行计算时会自动略过这些非零值。 总结这里只是简答的介绍了这几种相似性距离度量的方法和场景，但是在实际环境中远比这个复杂许多。这里总结下： 如果数据存在“分数膨胀“问题，就使用皮尔逊相关系数 如果数据比较密集，变量之间基本都存在共有值，且这些距离数据都是非常重要的，那就使用欧几里得或者曼哈顿距离 如果数据是稀疏的，就使用余弦相似度 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"相似度计算","slug":"相似度计算","permalink":"http://yoursite.com/tags/相似度计算/"}]},{"title":"回归分析之理论篇","slug":"机器学习/回归分析之理论篇","date":"2017-09-17T00:10:27.000Z","updated":"2018-03-28T15:58:24.349Z","comments":true,"path":"2017/09/17/机器学习/回归分析之理论篇/","link":"","permalink":"http://yoursite.com/2017/09/17/机器学习/回归分析之理论篇/","excerpt":"2015年的机器学习博客其实都是看《机器学习实战》这本书时学到的，说实话当时也是知其然，不知其所以然，以至于对其理解不深刻，好多细节和理论知识都搞的是乱七八糟，自从工作之后再去看一个算法，思考的比之前多了点，查看资料也比之前多了点，生怕理解错误，影响其他人，当然在理解的程度上还是不够深刻，这也是一个学习的过程吧，记录一下，欢迎指正。 CSDN链接：点击阅读","text":"2015年的机器学习博客其实都是看《机器学习实战》这本书时学到的，说实话当时也是知其然，不知其所以然，以至于对其理解不深刻，好多细节和理论知识都搞的是乱七八糟，自从工作之后再去看一个算法，思考的比之前多了点，查看资料也比之前多了点，生怕理解错误，影响其他人，当然在理解的程度上还是不够深刻，这也是一个学习的过程吧，记录一下，欢迎指正。 CSDN链接：点击阅读 一：一些名词定义1）指数分布族指数分布族是指可以表示为指数形式的概率分布。 f_X(x\\mid\\theta) = h(x) \\exp \\left (\\eta(\\theta) \\cdot T(x) -A(\\theta)\\right )其中，η为自然参数(nature parameter)，T(x)是充分统计量（sufficient statistic）。当参数A，h，T都固定以后，就定义了一个以η为参数的函数族。 伯努利分布与高斯分布是两个典型的指数分布族 伯努利分布又名两点分布或者0-1分布，是一个离散型概率分布。假设1的概率为p，0的概率为q，则其概率质量函数为：1&#123;\\displaystyle f_&#123;X&#125;(x)=p^&#123;x&#125;(1-p)^&#123;1-x&#125;=\\left\\&#123;&#123;\\begin&#123;matrix&#125;p&amp;&#123;\\mbox&#123;if &#125;&#125;x=1,\\\\q\\ &amp;&#123;\\mbox&#123;if &#125;&#125;x=0.\\\\\\end&#123;matrix&#125;&#125;\\right.&#125; 其期望值为： {\\displaystyle \\operatorname {E} [X]=\\sum _{i=0}^{1}x_{i}f_{X}(x)=0+p=p}其方差为： {\\displaystyle \\operatorname {var} [X]=\\sum _{i=0}^{1}(x_{i}-E[X])^{2}f_{X}(x)=(0-p)^{2}(1-p)+(1-p)^{2}p=p(1-p)=pq}正态分布(高斯分布)若随机变量X服从一个位置参数为 ${\\displaystyle \\mu }$ 、尺度参数为 ${\\displaystyle \\sigma } $ 的概率分布，记为： X \\sim N(\\mu,\\sigma^2),其概率密度函数为:1f(x) = &#123;1 \\over \\sigma\\sqrt&#123;2\\pi&#125; &#125;\\,e^&#123;- &#123;&#123;(x-\\mu )^2 \\over 2\\sigma^2&#125;&#125;&#125; 正态分布的数学期望值或期望值$ {\\displaystyle \\mu } $ 等于位置参数，决定了分布的位置；其方差 $ {\\displaystyle \\sigma ^{2}} $ 的开平方或标准差$ {\\displaystyle \\sigma }$ 等于尺度参数，决定了分布的幅度。 标准正态分布：如果$ {\\displaystyle \\mu =0} $ 并且 $ {\\displaystyle \\sigma =1} $ 则这个正态分布称为标准正态分布。简化为：1f(x) = \\frac&#123;1&#125;&#123;\\sqrt&#123;2\\pi&#125;&#125; \\, \\exp\\left(-\\frac&#123;x^2&#125;&#123;2&#125; \\right) 如下图所示： 正态分布中一些值得注意的量： 密度函数关于平均值对称 平均值与它的众数（statistical mode）以及中位数（median）同一数值。 函数曲线下68.268949%的面积在平均数左右的一个标准差范围内。 95.449974%的面积在平均数左右两个标准差 $ {\\displaystyle 2\\sigma } $ 的范围内。 99.730020%的面积在平均数左右三个标准差$ {\\displaystyle 3\\sigma } $ 的范围内。 99.993666%的面积在平均数左右四个标准差$ {\\displaystyle 4\\sigma } $ 的范围内。 函数曲线的反曲点（inflection point）为离平均数一个标准差距离的位置。 2）多重共线性和完全共线性多重共线性：指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。一般来说，由于经济数据的限制使得模型设计不当，导致设计矩阵中解释变量间存在普遍的相关关系。通俗点理解就是自变量里边有一些是打酱油的，可以由另外一些变量推导出来，当变量中存在大量的多重共线性变量就会导致模型误差很大，这个时候就需要从自变量中将“打酱油”的变量给剔除掉。 完全共线性：在多元回归中，一个自变量是一个或多个其他自变量的线性函数。 两者在某种特殊情况下是有交集的。 3）T检验T检验又叫student T 检验，主要用于样本含量小，总标准差 $\\sigma$ 未知的正太分布数据。T检验是用于小样本的两个平均值差异程度的检查方法，他是用T分布理论值来推断事件发生的概率，从而判断两个平均数的差异是否显著。参考: http://blog.csdn.net/shulixu/article/details/53354206 4）关系 函数关系 确定性关系，y=3+2x 相关关系 非确定性关系，比如说高中时数学成绩好的人，一般物理成绩也好，这是因为它们背后使用的都是数学逻辑，这种酒叫做非确定性关系。 5）虚拟变量定义： 又称虚设变量、名义变量或哑变量，用以反映质的属性的一个人工变量，是量化了的自变量，通常取值为0或1。（通常为离散变量，因子变量） 作用： 引入哑变量可使线形回归模型变得更复杂，但对问题描述更简明，一个方程能达到两个方程的作用，而且接近现实。 设置： 例如：体重（w）和身高（h），性别（s）的关系，但这里性别并非连续的或者数字可以表示的变量，你并不能拿 1表示男，2表示女，这里的性别是离散变量，只能为男或者女，所以这里就需要引入哑变量来处理。性别（s） =》 isman（男1，非男0），iswoman （因为只有两种可能，所以这里只需要引入一个哑变量即可），同理假设这里有另外一个变量肤色（有黑，白，黄三种可能），那么这里只需引入两个哑变量即可（isblack，iswhite），因为不是这两种的话那肯定是黄色皮肤了。 例子：针对上边所说的体重和身高，性别的关系。 构建模型： 1）加法模型1w = a + b * h + c * isman 针对数据样本而言，性别是确定的，所以 c * isman 的结果不是c就是0，所以在加法模型下，影响的是模型在y轴上的截距。这说明的是针对不同的性别而言，回归方程是平衡的，只不过是截距不一样。 2）乘法模型1w = a + b * h + c * isman * h + d * iswoman * h 同样针对数据样本而言，性别也是确定的，假设一个男性，isman 为1，iswoman 为0，则上述模型变成了 w = a + bh + c h =a + (b+c) * h，这个时候就是在y轴上的截距一样，而斜率不一致。 3）混合模型1w = a + b * h + c * isman + d * iswoman + e * isman * h + f * iswoman * h 假设一个针对一个性别为男的样本数据，该模型变可以变成 w = a + bh + c + e h = a +c + (b+e)*h，这个时候斜率和截距都是不一样的。 二：什么是回归（分析）回归就是利用样本（已知数据），产生拟合方程，从而（对未知数据）进行预测。比如说我有一组随机变量X（X1，X2，X3…）和另外一组随机变量Y（Y1，Y2，Y3…）,那么研究变量X与Y之间的统计学方法就叫做回归分析。当然这里X和Y是单一对应的，所以这里是一元线性回归。 回归分为线性回归和非线性回归，其中一些非线性回归可以用线性回归的方法来进行分析的叫做==广义线性回归==，接下来我们来了解下每一种回归： 1）线性回归线性回归可以分为一元线性回归和多元线性回归。当然线性回归中自变量的指数都是1，这里的线性并非真的是指用一条线将数据连起来，也可以是一个二维平面，三维平面等。 一元线性回归：自变量只有一个的回归，比如说北京二环的房子面积（Area）和房子总价（Money）的关系，随着面积（Area）的增大，房屋价格也是不断增长。这里的自变量只有面积，所以这里是一元线性回归。 多元线性回归：自变量大于等于两个，比如说北京二环的房子面积（Area），楼层（floor）和房屋价格（Money）的关系，这里自变量是两个，所以是二元线性回归，三元，多元同理。 2）非线性回归有一类模型，其回归参数不是线性的，也不能通过转换的方法将其变为线性的参数，这类模型称为非线性回归模型。非线性回归可以分为一元回归和多元回归。非线性回归中至少有一个自变量的指数不为1。回归分析中，当研究的因果关系只涉及因变量和一个自变量时，叫做一元回归分析；当研究的因果关系涉及因变量和两个或两个以上自变量时，叫做多元回归分析。 3）广义线性回归一些非线性回归可以用线性回归的方法来进行分析叫做广义线性回归。典型的代表是Logistic回归。 4）如何衡量相关关系既判断适不适合使用线性回归模型？使用相关系数（-1，1），绝对值越接近于1，相关系数越高，越适合使用线性回归模型（Rxy&gt;0,代表正相关，Rxy&lt;0,代表负相关） r_{XY} = \\frac{ \\sum (X_{i}-\\bar{X})(Y_{i}-\\bar{Y}) }{ \\sqrt{ \\sum (X_{i}-\\bar{X})^2) \\sum (Y_{i}-\\bar{Y})^2) } }三：回归中困难点1）选定变量 假设自变量特别多，有一些是和因变量相关的，有一些是和因变量不相关的，这里我们就需要筛选出有用的变量，如果筛选后变量还特别多的话，可以采用降维的方式进行变量缩减（可以参考之前的PCA降维的文章：http://blog.csdn.net/gamer_gyt/article/details/51418069 ，基本是整理《机器学习实战》这本书的笔记） 2）发现多重共线性(1).方差扩大因子法( VIF) 一般认为如果最大的VIF超过10，常常表示存在多重共线性。 (2).容差容忍定法 如果容差（tolerance）&lt;=0.1，常常表示存在多重共线性。 (3). 条件索引 条件索引(condition index)&gt;10，可以说明存在比较严重的共线性 3）过拟合与欠拟合问题过拟合和欠拟合其实对每一个模型来讲都是存在的，过拟合就是模型过于符合训练数据的趋势，欠拟合就是模型对于训练数据和测试数据都表现出不好的情况。针对于欠拟合来讲，是很容易发现的，通常不被讨论。 在进行模型训练的时候，算法要进行不断的学习，模型在训练数据和测试数据上的错误都在不断下降，但是，如果学习的时间过长的话，模型在训练数据集上的表现将会继续下降，这是因为模型已经过拟合，并且学习到了训练数据集中不恰当的细节和噪音，同时，测试集上的错误率开始上升，也是模型泛化能力在下降。 这个完美的临界点就在于测试集中的错误率在上升时，此时训练集和测试集上都有良好的表现。通常有两种手段可以帮助你找到这个完美的临界点：重采样方法和验证集方法。 如何限制过拟合？ 过拟合和欠拟合可以导致很差的模型表现。但是到目前为止大部分机器学习实际应用时的问题都是过拟合。过拟合是个问题因为训练数据上的机器学习算法的评价方法与我们最关心的实际上的评价方法，也就是算法在位置数据上的表现是不一样的。当评价机器学习算法时我们有两者重要的技巧来限制过拟合使用重采样来评价模型效能保留一个验证数据集最流行的重采样技术是k折交叉验证。指的是在训练数据的子集上训练和测试模型k次，同时建立对于机器学习模型在未知数据上表现的评估。验证集只是训练数据的子集，你把它保留到你进行机器学习算法的最后才使用。在训练数据上选择和调谐机器学习算法之后，我们在验证集上在对于模型进行评估，以便得到一些关于模型在未知数据上的表现的认知。 4）检验模型是否合理验证目前主要采用如下三类办法：1、拟合优度检验主要有R^2，t检验，f检验等等这三种检验为常规验证，只要在95%的置信度内满足即可说明拟合效果良好。2、预测值和真实值比较主要是差值和比值，一般差值和比值都不超过5%。3、另外的办法GEH方法最为常用。GEH是Geoffrey E. Havers于1970年左右提出的一种模型验证方法，其巧妙的运用一个拟定的公式和标准界定模型的拟合优劣。GEH=(2(M-C)^2/(M+C))^(1/2)其中M是预测值，C是实际观测值如果GEH小于5，认为模型拟合效果良好，如果GEH在5-10之间，必须对数据不可靠需要进行检查，如果GEH大于10，说明数据存在问题的几率很高。http://blog.sina.com.cn/s/blog_66188c300100hl45.html 5）线性回归的模型评判 误差平方和（残差平方和） 例如二维平面上的一点（x1，y1），经过线性回归模型预测其值为 y_1，那么预测模型的好与坏就是计算预测结果到直线的距离的大小，由于是一组数据，那么便是这一组数据的和。 点到直线的距离公式为： \\frac{\\left | A_{x_{0}}+B_{y_{0}} +C \\right |}{\\sqrt{A^2 + B^2 }}由于涉及到开方，在计算过程中十分不方便，所以这里转换为纵轴上的差值，即利用预测值与真实值的差进行累加求和，最小时即为最佳的线性回归模型，但是这里涉及到预测值与真实值的差可能为负数，所以这里用平方，所以最终的误差平方和为： RSS = \\sum_{i=1}^{n}(y_{i}- \\hat{y_{i}} )^2 = \\sum_{i=1}^{n}[y_{i} - (\\alpha +\\beta x_{i})]^2 AIC准则（赤池信息准则） AIC=n ln (RSSp/n)+2pn为变量总个数，p为选出的变量个数，AIC越小越好 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"回归分析","slug":"回归分析","permalink":"http://yoursite.com/tags/回归分析/"},{"name":"正态分布","slug":"正态分布","permalink":"http://yoursite.com/tags/正态分布/"}]},{"title":"数据归一化和其在sklearn中的处理","slug":"机器学习/数据归一化和其在sklearn中的处理","date":"2017-09-01T03:33:50.000Z","updated":"2018-03-28T15:58:46.421Z","comments":true,"path":"2017/09/01/机器学习/数据归一化和其在sklearn中的处理/","link":"","permalink":"http://yoursite.com/2017/09/01/机器学习/数据归一化和其在sklearn中的处理/","excerpt":"一：数据归一化数据归一化（标准化）处理是数据挖掘的一项基础工作，不同评价指标往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果，为了消除指标之间的量纲影响，需要进行数据标准化处理，以解决数据指标之间的可比性。原始数据经过数据标准化处理后，各指标处于同一数量级，适合进行综合对比评价。","text":"一：数据归一化数据归一化（标准化）处理是数据挖掘的一项基础工作，不同评价指标往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果，为了消除指标之间的量纲影响，需要进行数据标准化处理，以解决数据指标之间的可比性。原始数据经过数据标准化处理后，各指标处于同一数量级，适合进行综合对比评价。归一化方法有两种形式，一种是把数变为（0，1）之间的小数，一种是把有量纲表达式变为无量纲表达式。在机器学习中我们更关注的把数据变到0～1之间，接下来我们讨论的也是第一种形式。 1）min-max标准化min-max标准化也叫做离差标准化，是对原始数据的线性变换，使结果落到[0,1]区间，其对应的数学公式如下： X_{scale} = \\frac{x-min}{max-min}对应的python实现为123# x为数据 比如说 [1,2,1,3,2,4,1]def Normalization(x): return [(float(i)-min(x))/float(max(x)-min(x)) for i in x] 如果要将数据转换到[-1,1]之间，可以修改其数学公式为： X_{scale} = \\frac{x-x_{mean}}{max-min}x_mean 表示平均值。 对应的python实现为12345import numpy as np# x为数据 比如说 [1,2,1,3,2,4,1]def Normalization(x): return [(float(i)-np.mean(x))/float(max(x)-min(x)) for i in x] 其中max为样本数据的最大值，min为样本数据的最小值。这种方法有个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义。 该标准化方法有一个缺点就是，如果数据中有一些偏离正常数据的异常点，就会导致标准化结果的不准确性。比如说一个公司员工（A，B，C，D）的薪水为6k,8k,7k,10w,这种情况下进行归一化对每个员工来讲都是不合理的。 当然还有一些其他的办法也能实现数据的标准化。 2）z-score标准化z-score标准化也叫标准差标准化，代表的是分值偏离均值的程度，经过处理的数据符合标准正态分布，即均值为0，标准差为1。其转化函数为 X_{scale} = \\frac{x-\\mu }{\\sigma }其中μ为所有样本数据的均值，σ为所有样本数据的标准差。 其对应的python实现为：12345import numpy as np#x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]def z_score(x): return (x - np.mean(x) )/np.std(x, ddof = 1) z-score标准化方法同样对于离群异常值的影响。接下来看一种改进的z-score标准化方法。 3）改进的z-score标准化将标准分公式中的均值改为中位数，将标准差改为绝对偏差。 X_{scale} = \\frac{x-x_{center} }{\\sigma_{1} }中位数是指将所有数据进行排序，取中间的那个值，如数据量是偶数，则取中间两个数据的平均值。 σ1为所有样本数据的绝对偏差,其计算公式为： \\frac{1}{N} \\sum_{1}^{n}|x_{i} - x_{center}| 二：sklearn中的归一化sklearn.preprocessing 提供了一些实用的函数 用来处理数据的维度，以供算法使用。 1）均值-标准差缩放即我们上边对应的z-score标准化。在sklearn的学习中，数据集的标准化是很多机器学习模型算法的常见要求。如果个别特征看起来不是很符合正态分布，那么他们可能为表现不好。 实际上，我们经常忽略分布的形状，只是通过减去整组数据的平均值，使之更靠近数据中心分布，然后通过将非连续数特征除以其标准偏差进行分类。 例如，用于学习算法（例如支持向量机的RBF内核或线性模型的l1和l2正则化器）的目标函数中使用的许多元素假设所有特征都以零为中心并且具有相同顺序的方差。如果特征的方差大于其他数量级，则可能主导目标函数，使估计器无法按预期正确地学习其他特征。 例子：12345678910&gt;&gt;&gt; from sklearn import preprocessing&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; X_train = np.array([[ 1., -1., 2.],... [ 2., 0., 0.],... [ 0., 1., -1.]])&gt;&gt;&gt; X_scaled = preprocessing.scale(X_train)&gt;&gt;&gt; X_scaledarray([[ 0. , -1.22474487, 1.33630621], [ 1.22474487, 0. , -0.26726124], [-1.22474487, 1.22474487, -1.06904497]]) 标准化后的数据符合标准正太分布1234&gt;&gt;&gt; X_scaled.mean(axis=0)array([ 0., 0., 0.])&gt;&gt;&gt; X_scaled.std(axis=0)array([ 1., 1., 1.]) 预处理模块还提供了一个实用程序级StandardScaler，它实现了Transformer API来计算训练集上的平均值和标准偏差，以便能够稍后在测试集上重新应用相同的变换。1234567891011&gt;&gt;&gt; scaler = preprocessing.StandardScaler().fit(X_train)&gt;&gt;&gt; scalerStandardScaler(copy=True, with_mean=True, with_std=True)&gt;&gt;&gt; scaler.mean_array([ 1. , 0. , 0.33333333])&gt;&gt;&gt; scaler.scale_array([ 0.81649658, 0.81649658, 1.24721913])&gt;&gt;&gt; scaler.transform(X_train)array([[ 0. , -1.22474487, 1.33630621], [ 1.22474487, 0. , -0.26726124], [-1.22474487, 1.22474487, -1.06904497]]) 使用转换器可以对新数据进行转换123&gt;&gt;&gt; X_test = [[-1., 1., 0.]]&gt;&gt;&gt; scaler.transform(X_test)array([[-2.44948974, 1.22474487, -0.26726124]]) 2）min-max标准化X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0)) 12345678910&gt;&gt;&gt; X_train = np.array([[ 1., -1., 2.],... [ 2., 0., 0.],... [ 0., 1., -1.]])&gt;&gt;&gt; min_max_scaler = preprocessing.MinMaxScaler()&gt;&gt;&gt; X_train_minmax = min_max_scaler.fit_transform(X_train)&gt;&gt;&gt; X_train_minmaxarray([[ 0.5 , 0. , 1. ], [ 1. , 0.5 , 0.33333333], [ 0. , 1. , 0. ]]) 上边我们创建的min_max_scaler 同样适用于新的测试数据1234&gt;&gt;&gt; X_test = np.array([[ -3., -1., 4.]])&gt;&gt;&gt; X_test_minmax = min_max_scaler.transform(X_test)&gt;&gt;&gt; X_test_minmaxarray([[-1.5 , 0. , 1.66666667]]) 可以通过scale_和min方法查看标准差和最小值1234&gt;&gt;&gt; min_max_scaler.scale_ array([ 0.5 , 0.5 , 0.33333333])&gt;&gt;&gt; min_max_scaler.min_array([ 0. , 0.5 , 0.33333333]) 3）最大值标准化对于每个数值／每个维度的最大值 12345678910111213141516&gt;&gt;&gt; X_trainarray([[ 1., -1., 2.], [ 2., 0., 0.], [ 0., 1., -1.]])&gt;&gt;&gt; max_abs_scaler = preprocessing.MaxAbsScaler()&gt;&gt;&gt; X_train_maxabs = max_abs_scaler.fit_transform(X_train)&gt;&gt;&gt; X_train_maxabsarray([[ 0.5, -1. , 1. ], [ 1. , 0. , 0. ], [ 0. , 1. , -0.5]])&gt;&gt;&gt; X_test = np.array([[ -3., -1., 4.]])&gt;&gt;&gt; X_test_maxabs = max_abs_scaler.transform(X_test)&gt;&gt;&gt; X_test_maxabs array([[-1.5, -1. , 2. ]])&gt;&gt;&gt; max_abs_scaler.scale_ array([ 2., 1., 2.]) 4）规范化规范化是文本分类和聚类中向量空间模型的基础 12345678&gt;&gt;&gt; X = [[ 1., -1., 2.],... [ 2., 0., 0.],... [ 0., 1., -1.]]&gt;&gt;&gt; X_normalized = preprocessing.normalize(X, norm=&apos;l2&apos;)&gt;&gt;&gt; X_normalizedarray([[ 0.40824829, -0.40824829, 0.81649658], [ 1. , 0. , 0. ], [ 0. , 0.70710678, -0.70710678]]) 解释：norm 该参数是可选的，默认值是l2（向量各元素的平方和然后求平方根），用来规范化每个非零向量，如果axis参数设置为0，则表示的是规范化每个非零的特征维度。 机器学习中的范数规则：点击阅读其他对应参数：点击查看 preprocessing模块提供了训练种子的功能，我们可通过以下方式得到一个新的种子，并对新数据进行规范化处理。123456789&gt;&gt;&gt; normalizer = preprocessing.Normalizer().fit(X)&gt;&gt;&gt; normalizerNormalizer(copy=True, norm=&apos;l2&apos;)&gt;&gt;&gt; normalizer.transform(X)array([[ 0.40824829, -0.40824829, 0.81649658], [ 1. , 0. , 0. ], [ 0. , 0.70710678, -0.70710678]])&gt;&gt;&gt; normalizer.transform([[-1,1,0]])array([[-0.70710678, 0.70710678, 0. ]]) 5）二值化将数据转换到0-1 之间123456789&gt;&gt;&gt; X[[1.0, -1.0, 2.0], [2.0, 0.0, 0.0], [0.0, 1.0, -1.0]]&gt;&gt;&gt; binarizer = preprocessing.Binarizer().fit(X)&gt;&gt;&gt; binarizerBinarizer(copy=True, threshold=0.0)&gt;&gt;&gt; binarizer.transform(X)array([[ 1., 0., 1.], [ 1., 0., 0.], [ 0., 1., 0.]]) 可以调整二值化的门阀12345&gt;&gt;&gt; binarizer = preprocessing.Binarizer(threshold=1.1)&gt;&gt;&gt; binarizer.transform(X)array([[ 0., 0., 1.], [ 1., 0., 0.], [ 0., 0., 0.]]) 6）编码的分类特征通常情况下，特征不是作为连续值给定的。例如一个人可以有1[&quot;male&quot;, &quot;female&quot;], [&quot;from Europe&quot;, &quot;from US&quot;, &quot;from Asia&quot;], [&quot;uses Firefox&quot;, &quot;uses Chrome&quot;, &quot;uses Safari&quot;, &quot;uses Internet Explorer&quot;] 这些特征可以被有效的编码为整数，例如12[&quot;male&quot;, &quot;from US&quot;, &quot;uses Internet Explorer&quot;] =&gt; [0, 1, 3][&quot;female&quot;, &quot;from Asia&quot;, &quot;uses Chrome&quot;] would be [1, 2, 1]. 这样的整数不应该直接应用到scikit的算法中，可以通过one-of-k或者独热编码（OneHotEncorder），该种处理方式会把每个分类特征的m中可能值转换成m个二进制值。 123456&gt;&gt;&gt; enc = preprocessing.OneHotEncoder()&gt;&gt;&gt; enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])OneHotEncoder(categorical_features=&apos;all&apos;, dtype=&lt;class &apos;numpy.float64&apos;&gt;, handle_unknown=&apos;error&apos;, n_values=&apos;auto&apos;, sparse=True)&gt;&gt;&gt; enc.transform([[0,1,3]]).toarray()array([[ 1., 0., 0., 1., 0., 0., 0., 0., 1.]]) 默认情况下，从数据集中自动推断出每个特征可以带多少个值。可以明确指定使用的参数n_values。在我们的数据集中有两种性别，三种可能的大陆和四种Web浏览器。然后，我们拟合估计量，并转换一个数据点。在结果中，前两个数字编码性别，下一组三个数字的大陆和最后四个Web浏览器。123456&gt;&gt;&gt; enc = preprocessing.OneHotEncoder(n_values=[2,3,4])&gt;&gt;&gt; enc.fit([[1,2,3],[0,2,0]])OneHotEncoder(categorical_features=&apos;all&apos;, dtype=&lt;class &apos;numpy.float64&apos;&gt;, handle_unknown=&apos;error&apos;, n_values=[2, 3, 4], sparse=True)&gt;&gt;&gt; enc.transform([[1,0,0]]).toarray()array([[ 0., 1., 1., 0., 0., 1., 0., 0., 0.]]) 7）填补缺失值由于各种原因，真实数据中存在大量的空白值，这样的数据集，显然是不符合scikit的要求的，那么preprocessing模块提供这样一个功能，利用已知的数据来填补这些空白。12345678910&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; from sklearn.preprocessing import Imputer&gt;&gt;&gt; imp = Imputer(missing_values=&apos;NaN&apos;,strategy=&apos;mean&apos;,verbose=0)&gt;&gt;&gt; imp.fit([[1, 2], [np.nan, 3], [7, 6]])Imputer(axis=0, copy=True, missing_values=&apos;NaN&apos;, strategy=&apos;mean&apos;, verbose=0)&gt;&gt;&gt; X = [[np.nan, 2], [6, np.nan], [7, 6]]&gt;&gt;&gt; print(imp.transform(X)) [[ 4. 2. ] [ 6. 3.66666667] [ 7. 6. ]] Imputer同样支持稀疏矩阵123456789101112&gt;&gt;&gt; import scipy.sparse as sp&gt;&gt;&gt; X = sp.csc_matrix([[1,2],[0,3],[7,6]])&gt;&gt;&gt; imp = Imputer(missing_values=0,strategy=&apos;mean&apos;,axis=0)&gt;&gt;&gt; imp.fit(X)Imputer(axis=0, copy=True, missing_values=0, strategy=&apos;mean&apos;, verbose=0)&gt;&gt;&gt; X_test = sp.cscsp.csc sp.csc_matrix( &gt;&gt;&gt; X_test = sp.csc_matrix([[0,2],[6,0],[7,6]])&gt;&gt;&gt; print(imp.transform(X_test))[[ 4. 2. ] [ 6. 3.66666667] [ 7. 6. ]] 8）生成多项式特征通常，通过考虑输入数据的非线性特征来增加模型的复杂度是很有用的。一个简单而常用的方法是多项式特征，它可以得到特征的高阶和相互作用项。 其遵循的原则是 (X_1, X_2) -> (1, X_1, X_2, X_1^2, X_1X_2, X_2^2)123456789101112&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; from sklearn.preprocessing import PolynomialFeatures&gt;&gt;&gt; X = np.arange(6).reshape(3, 2)&gt;&gt;&gt; X array([[0, 1], [2, 3], [4, 5]])&gt;&gt;&gt; poly = PolynomialFeatures(2)&gt;&gt;&gt; poly.fit_transform(X) array([[ 1., 0., 1., 0., 0., 1.], [ 1., 2., 3., 4., 6., 9.], [ 1., 4., 5., 16., 20., 25.]]) 有些情况下，有相互关系的标签才是必须的，这个时候可以通过设置 interaction_only=True 来进行多项式特征的生成12345678910&gt;&gt;&gt; X = np.arange(9).reshape(3, 3)&gt;&gt;&gt; X array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])&gt;&gt;&gt; poly = PolynomialFeatures(degree=3, interaction_only=True)&gt;&gt;&gt; poly.fit_transform(X) array([[ 1., 0., 1., 2., 0., 0., 2., 0.], [ 1., 3., 4., 5., 12., 15., 20., 60.], [ 1., 6., 7., 8., 42., 48., 56., 336.]]) 其遵循的规则是： (X_1, X_2, X_3) -> (1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3) 对应的scikit-learn资料为： http://scikit-learn.org/stable/modules/preprocessing.html 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"数据归一化","slug":"数据归一化","permalink":"http://yoursite.com/tags/数据归一化/"}]},{"title":"MachingLearning中的距离和相似性计算以及python实现","slug":"机器学习/MachingLearning中的距离和相似性计算以及python实现","date":"2017-07-16T04:14:43.000Z","updated":"2018-03-28T15:59:03.869Z","comments":true,"path":"2017/07/16/机器学习/MachingLearning中的距离和相似性计算以及python实现/","link":"","permalink":"http://yoursite.com/2017/07/16/机器学习/MachingLearning中的距离和相似性计算以及python实现/","excerpt":"前言写这篇文章的目的不是说摘抄网上其他人的总结，刚才最近在看这方面的东西，为了让自己能够实际的去感受下每种求距离公式的差别，然后用python进行具体实现。","text":"前言写这篇文章的目的不是说摘抄网上其他人的总结，刚才最近在看这方面的东西，为了让自己能够实际的去感受下每种求距离公式的差别，然后用python进行具体实现。在机器学习中，经常要用到距离和相似性的计算公式，我么要常计算个体之间的差异大小，继而评价个人之间的差异性和相似性，最常见的就是数据分析中的相关分析，数据挖掘中的分类和聚类算法。如利用k-means进行聚类时，判断个体所属的类别，要利用距离计算公式计算个体到簇心的距离，如利用KNN进行分类时，计算个体与已知类别之间的相似性，从而判断个体所属的类别等。 文章编辑的过程中或许存在一个错误或者不合理的地方，欢迎指正。 参考：http://www.cnblogs.com/heaad/archive/2011/03/08/1977733.html 推荐：https://my.oschina.net/hunglish/blog/787596 欧氏距离也称欧几里得距离，是指在m维空间中两个点之间的真实距离。欧式距离在ML中使用的范围比较广，也比较通用，就比如说利用k-Means对二维平面内的数据点进行聚类，对魔都房价的聚类分析（price/m^2 与平均房价）等。 二维空间的欧氏距离二维平面上两点a(x1,y1)与b(x2,y2)间的欧氏距离 d12 =\\sqrt{(x_{1}-x_{2})^2+(y_{1}-y_{2})^2}python 实现为： 123456789# coding: utf-8from numpy import *def twoPointDistance(a,b): d = sqrt( (a[0]-b[0])**2 + (a[1]-b[1])**2 ) return dprint &apos;a,b 二维距离为：&apos;,twoPointDistance((1,1),(2,2)) 三维空间的欧氏距离三维空间两点a(x1,y1,z1)与b(x2,y2,z2)间的欧氏距离 d12 =\\sqrt{(x_{1}-x_{2})^2+(y_{1}-y_{2})^2+(z_{1}-z_{2})^2}python 实现为：12345def threePointDistance(a,b): d = sqrt( (a[0]-b[0])**2 + (a[1]-b[1])**2 + (a[2]-b[2])**2 ) return dprint &apos;a,b 三维距离为：&apos;,threePointDistance((1,1,1),(2,2,2)) 多维空间的欧氏距离两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的欧氏距离 \\sqrt{\\sum_{n}^{k=1}(x_{1k}-x_{2k})^2 }python 实现为： 1234567def distance(a,b): sum = 0 for i in range(len(a)): sum += (a[i]-b[i])**2 return sqrt(sum)print &apos;a,b 多维距离为：&apos;,distance((1,1,2,2),(2,2,4,4)) 这里传入的参数可以是任意维的，该公式也适应上边的二维和三维 标准欧氏距离标准化欧氏距离是针对简单欧氏距离的缺点而作的一种改进方案。标准欧氏距离的思路：既然数据各维分量的分布不一样，好吧！那我先将各个分量都“标准化”到均值、方差相等吧。均值和方差标准化到多少呢？这里先复习点统计学知识吧，假设样本集X的均值(mean)为m，标准差(standard deviation)为s，那么X的“标准化变量”表示为： 而且标准化变量的数学期望为0，方差为1。因此样本集的标准化过程(standardization)用公式描述就是： X^* = \\frac{X-m}{s}标准化后的值 = ( 标准化前的值 － 分量的均值 ) /分量的标准差 经过简单的推导就可以得到两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的标准化欧氏距离的公式： d_{12} =\\sqrt {\\sum_{k=1}^{n} (\\frac{x_{1k}-x_{2k}}{s_{k}})^2}如果将方差的倒数看成是一个权重，这个公式可以看成是一种加权欧氏距离(Weighted Euclidean distance)。 python 实现为1234567891011def moreBZOSdis(a,b): sumnum = 0 for i in range(len(a)): # 计算si 分量标准差 avg = (a[i]-b[i])/2 si = sqrt( (a[i] - avg) ** 2 + (b[i] - avg) ** 2 ) sumnum += ((a[i]-b[i])/si ) ** 2 return sqrt(sumnum)print &apos;a,b 标准欧式距离：&apos;,moreBZOSdis((1,2,1,2),(3,3,3,4)) 曼哈顿距离又称为城市街区距离（City Block distance）, 想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。实际驾驶距离就是这个“曼哈顿距离”。而这也是曼哈顿距离名称的来源。同样曼哈顿距离也分为二维，三维和多维。 在计程车几何学中，一个圆是由从圆心向各个固定曼哈顿距离标示出来的点围成的区域，因此这种圆其实就是旋转了45度的正方形。如果有一群圆，且任两圆皆相交，则整群圆必在某点相交；因此曼哈顿距离会形成一个超凸度量空间。 这里有一篇人脸表情分类的论文采用的曼哈顿距离进行计算的，一种人脸表情分类的新方法——Manhattan距离 二维曼哈顿距离二维平面两点a(x1,y1)与b(x2,y2)间的曼哈顿距离 d12 =\\left | x_{1}-x_{2} \\right | + \\left |y_{1}-y_{2} \\right |python实现为1234def twoMHDdis(a,b): return abs(a[0]-b[0])+abs(a[1]-b[1])print &apos;a,b 二维曼哈顿距离为：&apos;, twoMHDdis((1,1),(2,2)) 三维曼哈顿距离三维平面两点a(x1,y1,z1)与b(x2,y2,z2)间的曼哈顿距离 d12 =\\left | x_{1}-x_{2} \\right | + \\left |y_{1}-y_{2} \\right | + \\left |z_{1}-z_{2} \\right |python实现为1234def threeMHDdis(a,b): return abs(a[0]-b[0])+abs(a[1]-b[1]) + abs(a[2]-b[2]) print &apos;a,b 三维曼哈顿距离为：&apos;, threeMHDdis((1,1,1),(2,2,2)) 多维曼哈顿距离多维平面两点a(x1,y1)与b(x2,y2)间的曼哈顿距离 d12 = \\sum_{k=1}^{n} \\left | x_{1k} - x_{2k} \\right |python实现为1234567def moreMHDdis(a,b): sum = 0 for i in range(len(a)): sum += abs(a[i]-b[i]) return sumprint &apos;a,b 多维曼哈顿距离为：&apos;, moreMHDdis((1,1,1,1),(2,2,2,2)) 由于维距离计算是比较灵活的，所以也同样适合二维和三维。 切比雪夫距离切比雪夫距离（Chebyshev Distance）的定义为：max( | x2-x1 | , |y2-y1 | , … ), 切比雪夫距离用的时候数据的维度必须是三个以上，这篇文章中曼哈顿距离，欧式距离，明式距离，切比雪夫距离区别 给了一个很形象的解释如下：1234567891011比如，有同样两个人，在纽约准备到北京参拜天安门，同一个地点出发的话，按照欧式距离来计算，是完全一样的。但是按照切比雪夫距离，这是完全不同的概念了。譬如，其中一个人是土豪，另一个人是中产阶级，第一个人就能当晚直接头等舱走人，而第二个人可能就要等机票什么时候打折再去，或者选择坐船什么的。这样来看的话，距离是不是就不一样了呢？或者还是不清楚，我再说的详细点。同样是这两个人，欧式距离是直接算最短距离的，而切比雪夫距离可能还得加上财力，比如第一个人财富值100，第二个只有30，虽然物理距离一样，但是所包含的内容却是不同的。 二维切比雪夫距离二维平面两点a(x1,y1)与b(x2,y2)间的切比雪夫距离 d_{12} = max( \\left | x_{1} - x_{2} \\right | , \\left | y_{1} - y_{2} \\right |)python 实现为 1234def twoQBXFdis(a,b): return max( abs(a[0]-b[0]), abs(a[1]-b[1]))print &apos;a,b二维切比雪夫距离：&apos; , twoQBXFdis((1,2),(3,4)) 多维切比雪夫距离两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的切比雪夫距离 d12 = max_{i\\epsilon n}( \\left | x_{1i} - x_{2i} \\right | )python 实现为12345678def moreQBXFdis(a,b): maxnum = 0 for i in range(len(a)): if abs(a[i]-b[i]) &gt; maxnum: maxnum = abs(a[i]-b[i]) return maxnumprint &apos;a,b多维切比雪夫距离：&apos; , moreQBXFdis((1,1,1,1),(3,4,3,4)) 马氏距离有M个样本向量X1~Xm，协方差矩阵记为S，均值记为向量μ，则其中样本向量X到u的马氏距离表示为 D(x) = \\sqrt{(X-\\mu )^TS^{-1}(X-\\mu)}而其中向量Xi与Xj之间的马氏距离定义为 D(X_{i},X_{j}) = \\sqrt{(X_{i}-X_{j} )^TS^{-1}(X_{i}-X_{j} )} 若协方差矩阵是单位矩阵（各个样本向量之间独立同分布）,则公式就成了： D(X_{i},X_{j}) = \\sqrt{(X_{i}-X_{j} )^T(X_{i}-X_{j} )}也就是欧氏距离了。 若协方差矩阵是对角矩阵，公式变成了标准化欧氏距离。 马氏距离的优缺点：量纲无关，排除变量之间的相关性的干扰。 夹角余弦几何中夹角余弦可用来衡量两个向量方向的差异，机器学习中借用这一概念来衡量样本向量之间的差异。 二维空间向量的夹角余弦相似度在二维空间中向量A(x1,y1)与向量B(x2,y2)的夹角余弦公式： \\cos \\theta = \\frac{x_{1}x_{2} + y_{1}y_{2}}{ \\sqrt{ x_{1}^2+x_{2}^2 }\\sqrt{ y_{1}^2+y_{2}^2 } }python 实现为12345def twoCos(a,b): cos = (a[0]*b[0]+a[1]*b[1]) / (sqrt(a[0]**2 + b[0]**2) * sqrt(a[1]**2 + b[1]**2) ) return cosprint &apos;a,b 二维夹角余弦距离：&apos;,twoCos((1,1),(2,2)) 多维空间向量的夹角余弦相似度两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)的夹角余弦 类似的，对于两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)，可以使用类似于夹角余弦的概念来衡量它们间的相似程度。 \\cos \\theta = \\frac{a \\cdot b}{\\left | a \\right | \\left | b \\right |}即： \\cos \\theta = \\frac{ \\sum_{k=1}^{n} x_{1k}x_{2k} }{ \\sqrt{ \\sum_{k=1}^{n}x_{1k}^2 }\\sqrt{ \\sum_{k=1}^{n} x_{2k}^2 } }python实现为12345678910def moreCos(a,b): sum_fenzi = 0.0 sum_fenmu_1,sum_fenmu_2 = 0,0 for i in range(len(a)): sum_fenzi += a[i]*b[i] sum_fenmu_1 += a[i]**2 sum_fenmu_2 += b[i]**2 return sum_fenzi/( sqrt(sum_fenmu_1) * sqrt(sum_fenmu_2) )print &apos;a,b 多维夹角余弦距离：&apos;,moreCos((1,1,1,1),(2,2,2,2)) 夹角余弦取值范围为[-1,1]。夹角余弦越大表示两个向量的夹角越小，夹角余弦越小表示两向量的夹角越大。当两个向量的方向重合时夹角余弦取最大值1，当两个向量的方向完全相反夹角余弦取最小值-1。 闵可夫斯基距离闵氏距离不是一种距离，而是一组距离的定义 定义两个n维变量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的闵可夫斯基距离定义为： \\sqrt[p]{ \\sum_{k=1}^{n} \\left | x_{1k}-x_{2k} \\right |^p}其中p是一个变参数。 当p=1时，就是曼哈顿距离 当p=2时，就是欧氏距离 当p→∞时，就是切比雪夫距离 根据变参数的不同，闵氏距离可以表示一类的距离。 闵氏距离的缺点闵氏距离，包括曼哈顿距离、欧氏距离和切比雪夫距离都存在明显的缺点。 举个例子：二维样本(身高,体重)，其中身高范围是150 ~ 190，体重范围是50 ~ 60，有三个样本：a(180,50)，b(190,50)，c(180,60)。那么a与b之间的闵氏距离（无论是曼哈顿距离、欧氏距离或切比雪夫距离）等于a与c之间的闵氏距离，但是身高的10cm真的等价于体重的10kg么？因此用闵氏距离来衡量这些样本间的相似度很有问题。 简单说来，闵氏距离的缺点主要有两个：(1)将各个分量的量纲(scale)，也就是“单位”当作相同的看待了。(2)没有考虑各个分量的分布（期望，方差等)可能是不同的。 汉明距离定义两个等长字符串s1与s2之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。例如字符串“1111”与“1001”之间的汉明距离为2。 应用：信息编码（为了增强容错性，应使得编码间的最小汉明距离尽可能大）。 python 实现12345678def hanmingDis(a,b): sumnum = 0 for i in range(len(a)): if a[i]!=b[i]: sumnum += 1 return sumnumprint &apos;a,b 汉明距离：&apos;,hanmingDis((1,1,2,3),(2,2,1,3)) 杰卡德距离 &amp; 杰卡德相似系数杰卡德距离与杰卡德相似系数相反的概念是杰卡德距离(Jaccard distance)。杰卡德距离可用如下公式表示： J_{\\delta} (A,B) = \\frac{| A \\bigcup B | - | A \\bigcap B |}{| A \\bigcup B |}杰卡德距离用两个集合中不同元素占所有元素的比例来衡量两个集合的区分度。 python 实现1234567def jiekadeDis(a,b): set_a = set(a) set_b = set(b) dis = float(len( (set_a | set_b) - (set_a &amp; set_b) ) )/ len(set_a | set_b) return disprint &apos;a,b 杰卡德距离：&apos;, jiekadeDis((1,2,3),(2,3,4)) 杰卡德相似系数两个集合A和B的交集元素在A，B的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号J(A,B)表示。 J(A,B) = \\frac{| A \\bigcap B |}{| A \\bigcup B |}杰卡德相似系数是衡量两个集合的相似度一种指标。 python 实现1234567def jiekadeXSDis(a,b): set_a = set(a) set_b = set(b) dis = float(len(set_a &amp; set_b) )/ len(set_a | set_b) return disprint &apos;a,b 杰卡德相似系数：&apos;, jiekadeXSDis((1,2,3),(2,3,4)) 杰卡德相似系数与杰卡德距离的应用可将杰卡德相似系数用在衡量样本的相似度上。 样本A与样本B是两个n维向量，而且所有维度的取值都是0或1。例如：A(0111)和B(1011)。我们将样本看成是一个集合，1表示集合包含该元素，0表示集合不包含该元素。 p ：样本A与B都是1的维度的个数 q ：样本A是1，样本B是0的维度的个数 r ：样本A是0，样本B是1的维度的个数 s ：样本A与B都是0的维度的个数 那么样本A与B的杰卡德相似系数可以表示为： 这里p+q+r可理解为A与B的并集的元素个数，而p是A与B的交集的元素个数。 而样本A与B的杰卡德距离表示为： J= \\frac{p}{p+q+r}相关系数 &amp; 相关距离相关系数 \\rho_{XY} = \\frac{Cov(X,Y)}{\\sqrt{D(X)} \\sqrt{D(Y)}}=\\frac{ E( (X-EX) (Y-EY) ) }{ \\sqrt{D(X)} \\sqrt{D(Y)} }相关系数是衡量随机变量X与Y相关程度的一种方法，相关系数的取值范围是[-1,1]。相关系数的绝对值越大，则表明X与Y相关度越高。当X与Y线性相关时，相关系数取值为1（正线性相关）或-1（负线性相关）。 python 实现相关系数可以利用numpy库中的corrcoef函数来计算例如 对于矩阵a,numpy.corrcoef(a)可计算行与行之间的相关系数，numpy.corrcoef(a,rowvar=0)用于计算各列之间的相关系数，输出为相关系数矩阵。123456789101112131415161718from numpy import *a = array([[1, 1, 2, 2, 3], [2, 2, 3, 3, 5], [1, 4, 2, 2, 3]]) print corrcoef(a)&gt;&gt;array([[ 1. , 0.97590007, 0.10482848], [ 0.97590007, 1. , 0.17902872], [ 0.10482848, 0.17902872, 1. ]])print corrcoef(a,rowvar=0)&gt;&gt;array([[ 1. , -0.18898224, 1. , 1. , 1. ], [-0.18898224, 1. , -0.18898224, -0.18898224, -0.18898224], [ 1. , -0.18898224, 1. , 1. , 1. ], [ 1. , -0.18898224, 1. , 1. , 1. ], [ 1. , -0.18898224, 1. , 1. , 1. ]]) 相关距离 D_{xy} = 1 - \\rho _{XY}python 实现（基于相关系数）同样针对矩阵a12345678910111213141516# 行之间的相关距离ones(shape(corrcoef(a)),int) - corrcoef(a)&gt;&gt;array([[ 0. , 0.02409993, 0.89517152], [ 0.02409993, 0. , 0.82097128], [ 0.89517152, 0.82097128, 0. ]]) # 列之间的相关距离ones(shape(corrcoef(a,rowvar = 0)),int) - corrcoef(a,rowvar = 0)&gt;&gt;array([[ 0. , 1.18898224, 0. , 0. , 0. ], [ 1.18898224, 0. , 1.18898224, 1.18898224, 1.18898224], [ 0. , 1.18898224, 0. , 0. , 0. ], [ 0. , 1.18898224, 0. , 0. , 0. ], [ 0. , 1.18898224, 0. , 0. , 0. ]]) 信息熵信息熵并不属于一种相似性度量，是衡量分布的混乱程度或分散程度的一种度量。分布越分散(或者说分布越平均)，信息熵就越大。分布越有序（或者说分布越集中），信息熵就越小。 计算给定的样本集X的信息熵的公式： Entropy(X) = \\sum_{i=1}^{n} -p_{i} log_{2}p_{i}参数的含义： n：样本集X的分类数 pi：X中第i类元素出现的概率 信息熵越大表明样本集S分类越分散，信息熵越小则表明样本集X分类越集中。。当S中n个分类出现的概率一样大时（都是1/n），信息熵取最大值log2(n)。当X只有一个分类时，信息熵取最小值0 python进行计算和实现可参考：http://blog.csdn.net/autoliuweijie/article/details/52244246 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"},{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"相似度计算","slug":"相似度计算","permalink":"http://yoursite.com/tags/相似度计算/"}]},{"title":"一切的闹闹哄哄，只是他在水帘洞躲避风沙那晚做的一个梦","slug":"随手记/一切的闹闹哄哄，只是他在水帘洞躲避风沙那晚做的一个梦","date":"2017-04-15T17:03:56.000Z","updated":"2018-03-28T16:05:59.557Z","comments":true,"path":"2017/04/16/随手记/一切的闹闹哄哄，只是他在水帘洞躲避风沙那晚做的一个梦/","link":"","permalink":"http://yoursite.com/2017/04/16/随手记/一切的闹闹哄哄，只是他在水帘洞躲避风沙那晚做的一个梦/","excerpt":"送同学走之后，我在路边默默的站了有五分钟，突然觉得我无处可去，有一种深入骨髓的悲哀和无奈，然后我就想起了一个命题，“如今的你，何去何从！”我不知道为什么会突然想到这样一个命题，或许是我们每个人都是至尊宝吧。其实每个人对《大话》的理解都是有所不同的，同样的人在不同的时期认识也会有偏差，就好比我第一次看的时候，笑得腹背抽筋，呲牙咧嘴，第二次看的时候，笑得少了，想的多了，过后便什么感觉也没有了，第三次看得时候，忽然觉得不知道是该哭还是该笑，笑得时候太牵强，哭得时候太尴尬。第四次便是这一次，看完之后觉得有一种无可奈何的悲哀。","text":"送同学走之后，我在路边默默的站了有五分钟，突然觉得我无处可去，有一种深入骨髓的悲哀和无奈，然后我就想起了一个命题，“如今的你，何去何从！”我不知道为什么会突然想到这样一个命题，或许是我们每个人都是至尊宝吧。其实每个人对《大话》的理解都是有所不同的，同样的人在不同的时期认识也会有偏差，就好比我第一次看的时候，笑得腹背抽筋，呲牙咧嘴，第二次看的时候，笑得少了，想的多了，过后便什么感觉也没有了，第三次看得时候，忽然觉得不知道是该哭还是该笑，笑得时候太牵强，哭得时候太尴尬。第四次便是这一次，看完之后觉得有一种无可奈何的悲哀。 对《大话》的解读有人说这是一部烂片，也有人说这是一部经典，有人说这是对西游文化的过度消费，有人说这是一代人的反思与成长，有人说阴差阳错的蝴蝶效应，让一个年代的人支撑起了这部超越中国电影水平50年的无厘头式作品，也有人说这是一部笑中带泪，总有一天你走在路上也会像一条狗的悲剧。 一千个人眼中有一千个哈姆雷特，对于一部影视作品，我们无法去评论任何一个人的理解是对还是错，因为经历不同，就像你不能把你的情感强加到别人身上。喜欢一部作品，在不知不觉间你会忽略他的缺点，讨厌一个事物，在不知不觉间就会放大他的短处。我们也不能用我们现代的思维和观念去评判20世纪港台电影的文化和情感。 至尊宝的路线是，有人给他点了三颗痣，拔出来紫青宝剑，开启了与紫霞的交涉，娶亲，戴上紧箍咒，大战牛魔王，西天取经。其实至尊宝这条路线，何尝不是我们的漫漫人生路呢，至尊宝用月光宝盒来寻找500年前的白晶晶，却阴差阳错遇到了紫霞仙子，最后在对白晶晶失而复得之后又得知紫霞在他内心留下的一滴眼泪，最终大彻大悟，明白了舍生取义，参透了生亦何哀，死亦何苦。我们的人生不也一样，总会有各种各样的未知因素影响我们的选择，在努力面对眼前一切之后，最终回头感叹年华，围炉小饮，本来无一物，何处惹尘埃。 对于紫霞和至尊宝的爱情，没有这种体验，是不会懂的，越是深情的人越容易受伤，她放下尊严来爱他，终于他后悔了。在至尊宝的梦里，紫霞仙子说：“飞蛾明明知道前面是火堆，却还义无反顾的扑进去。”她笑一下，接着说：“飞蛾就是这么傻！”而同样的至尊宝，“有一个名字叫紫霞的，你叫七百八十四次“，时光真是个好东西，所谓雕刻时光，是说生活像把钝刀，锉平我们的触觉，而电影则是解毒的重药，它让人突然领悟到，我们的内心要比自己想像的敏感干净得多！你等的那么辛苦，他却陌生到让你心疼。以至于在人生的不同阶段去欣赏《大话》，似乎都能从中找到某些影子，原先是笑，后来是苦，到最后便是沉默了。城墙上的一吻，至尊宝变成了旁观者，他借用夕阳武士的身份和转世的紫霞完成了他告别的深情，于是他的背影，他的离开，寂寞成了“好像一条狗”。 他好像一条狗呀！-“那个人的样子好怪啊”-“我也看到了，他好像条狗啊”是呀，英雄的离开，留下的永远只是背影，只不过在至尊宝这里加上了悲剧色彩，加上了人艰不拆的辛酸泪，以至于多少年后才明白了至尊宝转身离开有多难。至尊宝的结局是一个男人的悲怆与无奈。 至尊宝用了月光宝盒来寻找500年前的白晶晶，同时遇到了在她看来，他是她的命中注定的紫霞仙子，直到后来牛魔王的出现，夺走了紫霞，夺走了白晶晶，夺走了至尊宝往日的快乐，他明白他要夺回这一切，可是面对戏剧般的月光宝盒，至尊宝得到的更多是无力和苍白，面对这些无情的现实，幻想一次又一次地破灭。直到最后的关头，至尊宝终于醒悟，靠月光宝盒不行，至尊宝更是没有那个本事，只有成为孙悟空，只有戴上那个金刚圈，他才有能力同牛魔王一较高下。 这是一个极大的讽刺。你想要得到吗？好，那么你必须先放弃至尊宝的身份，你必须做出选择，必须忍受无尽的痛苦，他想要化解时间无尽的仇恨，就必须放弃自己的感情，不是不爱，而是大彻大悟之后的大爱，他必需化身为孙悟空，帮助唐三藏取得经书，化解这世间的恨。 那么至尊宝的放弃是自觉自愿的醒悟吗？不，他并不愿意，但是他必须拯救紫霞，必须化解人间的恨，他别无选择，他必须戴上紧箍咒。虽然成为了孙悟空，成了大英雄，但他对自己的生存状态极度不满。所以在最后，孙悟空将他心中残存的至尊宝的影子幻化作一位夕阳武士，在对现实世界彻底失望后，只能构造一个虚幻的想象来了却这桩心愿，并借武士的口中表达了对自己生存状态的不满，活得好象是一条狗一样。唉，一个男人的悲怆和无奈。 那句意中人，满足了多少人的少女心“我知道有一天，他会在一个万众瞩目的情况下出现，身披金甲圣衣，脚踏七色云彩来娶我”“我的意中人是个盖世英雄，有一天他会踏着云彩来娶我”这两句分别是紫霞在牛魔王娶她前的晚上和死前对孙悟空说的，多么经典的台词，以至于现在多少人还幻想着自己的意中人。 进入至尊宝内心的只有两个女人，一个是白晶晶，一个是紫霞，白晶晶问的是“他最喜欢的人是不是我”，紫霞问的是“他跟他的娘子是不是很恩爱呀？”，白晶晶的爱是一种索求的爱，而紫霞的爱则是无怨无悔的。 所以最终至尊宝回来了，在化身为孙悟空之后，身披金甲圣衣，脚踏七色云彩而来，他实现了紫霞的梦想，只不过加了一层掩饰与牵强。 从现如今这个角度反思紫霞的意中人，我是不太赞同的，童话毕竟是童话，正是这个经典的对白，让多少人活在自己的想象中，我们都渴望对方是个“意中人”的形象，可是我们却忽略乐一个”等价“的观念，你凭什么拥有你的”意中人“，你配得上吗？这不仅让我想起了另外一个命题：“不要去羡慕那些散发光芒的成功者，因为你不知道他背后付出的努力和艰辛”，这其实是一个道理，如果你仅仅是停留在幻想和计划的层面，那么你永远得不到你的”意中人“。 我猜中了开头，可是我猜不着这结局紫霞说猜得到开始，却猜不透这结局。大约直到最后，她也没能明白、没能理解至尊宝的苦心。又或者说，是至尊宝从来也未能真正了解她的心意。我曾经以为，死去的紫霞是最可怜的角色。可是，至尊宝又何尝不可怜呢，他甚至，连伤心的权利也没有了。在紫霞死去的一瞬间，他的心也已经跟着死去了。在他余下的人生里，再也不会有欢笑、快乐，再也不会有那样一个可以在他心里流下眼泪的女孩子。就算取回西经又能如何，心爱的人再也会不来了。就算成佛又能如何，没有了你，整个世界对我来说都毫无意义。 《大话》把遗憾和难题抛给了时间又一次的时空穿梭后，面对城头男女，孙悟空附身夕阳武士，给出无数人热泪纵横，内心中期盼的最后答案。没有失去过，也永远不能明白得到的快乐。附身后的孙悟空发自内心肺腑地给了女子一记深深长吻，这一吻穿越地老天荒，不再相信自欺欺人的一万年，他那般语气坚决地说出了那三个字。先前拒不让步的夕阳武士，拥抱着爱人幸福陶醉。转身远去的孙悟空了却尘缘心事，消失在大漠黄沙尽头。只是每次在紫霞被刺中或者孙悟空松手的瞬间，还是会心潮如水甚至潸然泪下。 十年大话，一群人围坐着观看《大话西游》的狂热时代过去了，心底保存的泪水也慢慢尘封直至故事终结。毫无拘束的开怀大笑渐渐沦为一个人的狂欢，难加掩饰的心底苍凉逐渐成了人生重担 如今的你，何去何从“如今的你，何去何从？”“对呀，何去何从” 真的羡慕至尊宝最初为了营救白晶晶，借用外力，使用月光宝盒穿越回500年前，为了解救紫霞，戴上紧箍咒。 而你呢？没有目标，你便是一个游荡的灵魂。 加长版加了什么1：紫霞刚出现时在沙漠和雪蛤精，孔雀王的对话以及 他们的拔剑抢婚，与影片中紫霞和至尊宝在 牛魔王婚礼上遇见时雪蛤精，孔雀王和反对结婚作了呼应。 2：约好二更相见，原版是牛夫人出现 ，然后是至尊宝直接被猪八戒和沙师弟拉去救师父，加长版中先是牛夫人出现，然后牛魔王，然后至尊宝 3：牛魔王婚礼时猪八戒和沙僧在小妖堆里跟他们“打成一片” 4：至尊宝被青霞揍晕第二次之后，早上跟紫霞说的那通话“你要让我拿点信物给他看, 你有什么项链啊,首饰啊,金银珠宝啊,月光宝盒啊什么的……”原版的这段声音不是石班瑜所配。新版中，这段声音是重新配音，换上了石班瑜的声音。 5：2K画面的修复 一切的闹闹哄哄，只是他在水帘洞躲避风沙那晚做的一个梦 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"随手记","slug":"随手记","permalink":"http://yoursite.com/categories/随手记/"}],"tags":[{"name":"随手记","slug":"随手记","permalink":"http://yoursite.com/tags/随手记/"}]},{"title":"别了青春与流年，遇见下一个自己","slug":"随手记/别了青春与流年，遇见下一个自己","date":"2016-12-20T16:35:00.000Z","updated":"2018-03-28T16:01:29.761Z","comments":true,"path":"2016/12/21/随手记/别了青春与流年，遇见下一个自己/","link":"","permalink":"http://yoursite.com/2016/12/21/随手记/别了青春与流年，遇见下一个自己/","excerpt":"如果说岁月是年轮，我们便是推行者，如果说成长是一场华丽的蜕变，我们便是领舞者。一路走来，太多不易，告别青春的年少轻狂，我们成了岁月里被磨平的棱角，静静的守在属于自己的一亩三分地。","text":"如果说岁月是年轮，我们便是推行者，如果说成长是一场华丽的蜕变，我们便是领舞者。一路走来，太多不易，告别青春的年少轻狂，我们成了岁月里被磨平的棱角，静静的守在属于自己的一亩三分地。 2016-时间是长了脚的妖怪，跑的飞快&nbsp;&nbsp;&nbsp;&nbsp;四年时光，匆匆而过，沈阳占据了我23岁之前的太多第一次，第一次一个人一包行李，第一次21个小时的硬座，第一次坐地铁，第一次谈恋爱，第一次分手，第一次旅行，第一次坐摩天轮，第一次吃棉花糖，第一次看电影，第一次接触电脑，第一次……&nbsp;&nbsp;&nbsp;&nbsp;时间是长了脚的妖怪，跑的飞快，只是好像后来我们都离开，各自生活在喧嚣未来，当时的遗憾在回忆肆虐的某些时段，重新打开，又好象我们同时都在。&nbsp;&nbsp;&nbsp;&nbsp;有人说，谈过恋爱，分过手，挂过科，拿过奖学金，当过学生干部的大学才是完美的，那么我想我还是比较幸运的，回顾我的大学生活，除了两件我极力想做的事情没有完成之外，经历了太多，谈过恋爱，分过手，当过学生干部，拿过奖学金，参加过各种志愿活动，也做过校园代理，被坑过，被骗过，也干过兼职，做过外包，送过外卖，发过传单，在这整个过程中，认识了不少人，见过不少事，看明白了不少的社会道理，看清了多少人的虚情假意，感谢那一路让我经历成长的人。&nbsp;&nbsp;&nbsp;&nbsp;大一是我大学生活里最快乐的一年，那时的我们很单纯。只是后来，大家都变了。 2016-剑未配好，已出江湖，来一场说走就走的北漂&nbsp;&nbsp;&nbsp;&nbsp;该到来的还是会到来，虽然对于工作我是做好了准备，但是还是有点措手不及。 七月，别了流年&nbsp;&nbsp;&nbsp;&nbsp;那是七月，我的心情迫切的像火辣辣的太阳，拉着行李，从大学的门前离开，没有回头，虽然这里有我牵挂的人，有我念着的事，但我还是把更多的希望寄托在充满魔性的首都，因为我相信这里是梦会是开始的地方，于是在朋友的帮助下，我开启了我的北漂生活。 广联达&nbsp;&nbsp;&nbsp;&nbsp;我来北京的第一家公司是广联达，建筑行业国内算是龙头老大了，虽然在互联网行业不是太牛逼，但对于一个初出茅庐的我还是够我学习和经历了，而且凑巧的是公司是我一个八几年的校友创立的，只是这和我没有半毛钱关系，在那的三个月里，我连个人影都没有见过。&nbsp;&nbsp;&nbsp;&nbsp;后来的后来我选择了离开，不是公司不好，不是带我的师父不优秀，不是同事不牛逼，只是我感觉那里不适合我。&nbsp;&nbsp;&nbsp;&nbsp;我的师父是项目组组长吧，人有点娘娘腔，别人都叫他梅梅，但是对我们特别好，他技术也十分厉害，离开的时候和师父聊天，他说在公司是P3和P4的技术双认证，是一个技术架构师，自己带着团队几个月为公司写了一个云测试平台，现在更到3.x版本了吧。我个人是十分佩服我师父的，为人低调，技术够强，还有好人缘。&nbsp;&nbsp;&nbsp;&nbsp;在公司的那段时候里，我主要做的是一个以课题形式展示的数据分析平台，用到的技术无非就是大学里学的那些，那个时候和另外一个同事还吹牛逼说咱也是架构师了，这仅仅是因为自己画了个水的一逼的图&nbsp;&nbsp;&nbsp;&nbsp;哈哈，如果这幅图出自架构师之手，就是系统架构了，可是出自我们这等毛小子之手就是闹着玩了吧。就好比以国家的名义去挖墓，就是考古，以个人名义去挖墓就是盗墓了。 昌平线，煎熬&nbsp;&nbsp;&nbsp;&nbsp;西二旗是中国最堵得一个地铁站了，大家都说后村厂路堵车十分钟，中国互联网经济停滞2小时。&nbsp;&nbsp;&nbsp;&nbsp;昌平线是北漂人的聚集地了，不是因为别的，是因为这条线路上的房租便宜 ，那个时候我就盘踞在沙河高教园旁边的东沙屯村里，每月800元的房租还是负担的起的，除了交通不便之外，一切还都可以接受，毕竟你是在北漂。 2016-别了流年，是现在的我&nbsp;&nbsp;&nbsp;&nbsp;九月末我面试了现在所在的公司，离开了广联达，不是因为它不优秀，它不好，只是因为那里现在还不适合我，在我的棱角被磨平之前，我想出去闯一闯。&nbsp;&nbsp;&nbsp;&nbsp;可能是我所在部门的原因，我觉得特别懒散，感觉大家都是在混日子，每天改那么点bug，每天更新一点小功能，或者这就是大公司的尴尬，或者说转型之中的公司的短板吧，大家都沉浸在以前的辉煌之中，没有创造力，没有新奇的想法，没有交流的冲动，没有那种干劲。于是我选择了离开，我想先让我去经历一番我想要的工作与生活，等我累了，说不定我就会想念这种状态了。&nbsp;&nbsp;&nbsp;&nbsp;现在所在的是一个创业公司，像我想象中一样，大家窝在一个不大的办公司，交流与合作，为了梦想一起努力着，很开心。&nbsp;&nbsp;&nbsp;&nbsp;在这里我接触到了更多知识，技术的，做人的，交流的，至今我脑海中还清晰的记着那天赵总的一句话：读书要有收获，至少要涨气场。&nbsp;&nbsp;&nbsp;&nbsp;新的环境里我接触学习了Docker，ELK，重新学习了一些机器学习的算法知识。于是在我的CSDN博客中创建了两个技术专栏，由于刚刚接触，写的也不够深入，不过我会努力的。&nbsp;&nbsp;&nbsp;&nbsp;Docker江湖：http://blog.csdn.net/column/details/13159.html &nbsp;&nbsp;&nbsp;&nbsp;ELK从入门到放弃：http://blog.csdn.net/column/details/13079.html &nbsp;&nbsp;&nbsp;&nbsp;认认真真经历才能好好成长。 2016-我在CSDN的收获 鲍大神&nbsp;&nbsp;&nbsp;&nbsp;开始在CSDN上写博客是大一的时候，是一个牛逼的学长带我走上了这条”不归路”，谢谢鲍大神这一路的指导与传授，一直以来，他都是我的榜样。我也努力赶上他，只可惜看到的永远都是背影。 梦姐姐&nbsp;&nbsp;&nbsp;&nbsp;八月份的时候偶然的机会认识梦姐姐，做了博乐，后来也申请并通过了CSDN博客专家。 结识技术爱好者&nbsp;&nbsp;&nbsp;&nbsp;其实相比这些更重要的是通过CSDN所认识的每一个技术爱好者，可以说CSDN是国内的程序员的社交平台了。感觉那些给我留言提问我的人，可能有些疑问还是没有帮你们解决，只是我个人能力有限，不像郭神，鸿洋大神技术功底深厚。在这个平台之上，我也认识到了自己的许多不足和技术缺点，在阅读博客的过程中，也学到了不少东西。 &nbsp;&nbsp;&nbsp;&nbsp;谢谢你一路陪我成长，你若不离，我定不弃。 2016-开始commit我的github&nbsp;&nbsp;&nbsp;&nbsp;有人说开源垃圾，有人说开源缩减了开发的成本和时间，不管怎样，开源是一种趋势，而且势头不会减弱，很荣幸我也投入了开源的大军，即使现在我还是一个蝼蚁。&nbsp;&nbsp;&nbsp;&nbsp;我的github：https://github.com/thinkgamer 2016-杂乱无章&nbsp;&nbsp;&nbsp;&nbsp;这一年，从一个初出茅庐的蝼蚁一步步成长，一个个经历，我给交了一份70分的答卷，我没有让我的父母和亲人失望，我没有让我的老师失望，我没有让我喜欢的人失望，我也没有让曾经看不起我的人失望，只是我让自己失望了。&nbsp;&nbsp;&nbsp;&nbsp;有些东西我没有去争取，有些机会我没有把握，有些冲动我失了控。但正是这些完美的不完美的，才让你有更大的劲头去前进。 2017-下一个自己&nbsp;&nbsp;&nbsp;&nbsp;时间不会因为你的遗憾而停留，我们能做的就是把每一天都当成最后一天来过。&nbsp;&nbsp;&nbsp;&nbsp;2017，我要完成： 一个安卓APP和对应的Web 小说《这夏未眠》 发表社区划分论文 深入学习Scala和Spark 掌握一个深度学习框架（eg：Caffe） 跟进研究Hadoop家族的最近版本，并形成文档 换一台Mackbook Pro 攒够100K+ &nbsp;&nbsp;&nbsp;&nbsp;感谢这一路有你，加油！ 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"随手记","slug":"随手记","permalink":"http://yoursite.com/categories/随手记/"}],"tags":[{"name":"随手记","slug":"随手记","permalink":"http://yoursite.com/tags/随手记/"}]},{"title":"这夏未眠.简介","slug":"夏未眠/这夏未眠-简介","date":"2016-09-15T10:13:50.000Z","updated":"2018-03-28T15:59:24.249Z","comments":true,"path":"2016/09/15/夏未眠/这夏未眠-简介/","link":"","permalink":"http://yoursite.com/2016/09/15/夏未眠/这夏未眠-简介/","excerpt":"书整体分为三部分《夏之过往》，《夏之流年》，《夏之未至》。整本书讲的是男主人公顾艾哲（小艾）与莫晨（晨晨）之间的故事，从初中到高中再到大学，从相遇到相知再到相离。","text":"书整体分为三部分《夏之过往》，《夏之流年》，《夏之未至》。整本书讲的是男主人公顾艾哲（小艾）与莫晨（晨晨）之间的故事，从初中到高中再到大学，从相遇到相知再到相离。两人同在陌乘一中念初中，同班同学，在中考来临的那段日子，两人相互鼓励，于是顾艾哲（小艾）考上了他从来都没有想过能考上的孟川一高，而莫晨（晨晨）呢，考上了预料之中的平阳一高，而她在这之前却从来没告诉过顾艾哲（小艾）她要去平阳，就这样，两个人分开了，一些都看起来那么顺理成章，一切又看起来那么暗淡失望。 在经历过高中的二年之后，顾艾哲（小艾）终于联系上了莫晨（晨晨），那天晚上，他用妈妈的电话给莫晨（晨晨）通了两个小时的电话，似乎要把两人两年里没有说的话都说完，可是有太多的话是无法用言语表达的，就这样电话欠费了，终止了聊天，可是那天晚上，小艾高兴的一宿没睡，那一晚上，他的笑容都是幸福的。 可是事情永远不会那么顺利，在香山公园里，当他拿起他买的情侣戒指送给晨晨时，晨晨没有接受，说了一堆他也没有听进去的话，就这样，又开始了分离，而谁也不知道这次分离竟然时一辈子的再也不见。 后来的后来，他又遇见了别的女孩，不知道是不是因为后来的女孩都像小艾记忆里的莫晨。只知道，他都很珍惜。 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"这夏未眠","slug":"这夏未眠","permalink":"http://yoursite.com/categories/这夏未眠/"}],"tags":[{"name":"夏未眠","slug":"夏未眠","permalink":"http://yoursite.com/tags/夏未眠/"}]},{"title":"这夏未眠.序","slug":"夏未眠/这夏未眠-序","date":"2016-09-15T10:08:51.000Z","updated":"2018-03-28T15:59:17.025Z","comments":true,"path":"2016/09/15/夏未眠/这夏未眠-序/","link":"","permalink":"http://yoursite.com/2016/09/15/夏未眠/这夏未眠-序/","excerpt":"这本书的整体构思是小主大学一年级时刚去的时候的一个想法，当时刚刚步入大学的我们，心里是那么的迷茫与懵懂，开学前两周，除了军训还是军训，晚上偶尔有个空闲时间，我想大概也许是无聊的，记得那个时候坐在图书馆靠窗的位置，看着窗外，没有明月，没有佳人，有的只是一望无际的黑暗。","text":"这本书的整体构思是小主大学一年级时刚去的时候的一个想法，当时刚刚步入大学的我们，心里是那么的迷茫与懵懂，开学前两周，除了军训还是军训，晚上偶尔有个空闲时间，我想大概也许是无聊的，记得那个时候坐在图书馆靠窗的位置，看着窗外，没有明月，没有佳人，有的只是一望无际的黑暗。那个时候，还没有遇到你所想遇到的人，或许回忆还沉淀在高中的时光里，或是幸福，或是苦涩，或是幸福之后的苦涩，回过头来，看着满屋子的学长学姐，心里是及其复杂的，有种说不出的难过，那时候我是不是在想，现在的你（们）会在哪里念大学呢？ 想着想着眼角便淌出了泪水，我想我的大学要完成一件至少我自己觉得满意的事，于是便有了你现在看到的这个序，不知道是不是受郭敬明的影响，因为我看过他的唯一一本小说，也是我看过的唯一一本小说——《夏至未至》，我想写一本书，或者更准确的说，我想写一个人的青春。 在13年军训结束之后，我构思了整个体系，定了这本书的名字——《这夏未眠》，熟悉我的朋友，也知道这是我的QQ网名，QQ作为那个时代的记忆，总会残留一些悲伤的故事，于是我到现在四年了，我从没换过QQ网名，或许是害怕，害怕那些好久不联系的朋友，找不到我吧。 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"这夏未眠","slug":"这夏未眠","permalink":"http://yoursite.com/categories/这夏未眠/"}],"tags":[{"name":"夏未眠","slug":"夏未眠","permalink":"http://yoursite.com/tags/夏未眠/"}]},{"title":"搜索引擎：MapReduce实战----倒排索引","slug":"Hadoop/搜索引擎：MapReduce实战-倒排索引","date":"2015-07-28T02:23:00.000Z","updated":"2018-04-14T19:38:20.835Z","comments":true,"path":"2015/07/28/Hadoop/搜索引擎：MapReduce实战-倒排索引/","link":"","permalink":"http://yoursite.com/2015/07/28/Hadoop/搜索引擎：MapReduce实战-倒排索引/","excerpt":"1.倒排索引简介倒排索引（Inverted index），也常被称为反向索引、置入档案或反向档案，是一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。它是文档检索系统中最常用的数据结构。","text":"1.倒排索引简介倒排索引（Inverted index），也常被称为反向索引、置入档案或反向档案，是一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。它是文档检索系统中最常用的数据结构。 有两种不同的反向索引形式： 一条记录的水平反向索引（或者反向档案索引）包含每个引用单词的文档的列表。一个单词的水平反向索引（或者完全反向索引）又包含每个单词在一个文档中的位置。后者的形式提供了更多的兼容性（比如短语搜索），但是需要更多的时间和空间来创建。 举例：以英文为例，下面是要被索引的文本：123T0 = &quot;it is what it is&quot;T1 = &quot;what is it&quot;T2 = &quot;it is a banana&quot; 我们就能得到下面的反向文件索引：12345&quot;a&quot;: &#123;2&#125;&quot;banana&quot;: &#123;2&#125;&quot;is&quot;: &#123;0, 1, 2&#125;&quot;it&quot;: &#123;0, 1, 2&#125;&quot;what&quot;: &#123;0, 1&#125; 检索的条件”what”, “is” 和 “it” 将对应这个集合：{0,1}∩{0,1,2}∩{0,1,2}={0,1}。 对相同的文字，我们得到后面这些完全反向索引，有文档数量和当前查询的单词结果组成的的成对数据。 同样，文档数量和当前查询的单词结果都从零开始。 所以，”banana”: {(2, 3)} 就是说 “banana”在第三个文档里 (T2)，而且在第三个文档的位置是第四个单词(地址为 3)。12345&quot;a&quot;: &#123;(2, 2)&#125;&quot;banana&quot;: &#123;(2, 3)&#125;&quot;is&quot;: &#123;(0, 1), (0, 4), (1, 1), (2, 1)&#125;&quot;it&quot;: &#123;(0, 0), (0, 3), (1, 2), (2, 0)&#125;&quot;what&quot;: &#123;(0, 2), (1, 0)&#125; 如果我们执行短语搜索”what is it” 我们得到这个短语的全部单词各自的结果所在文档为文档0和文档1。但是这个短语检索的连续的条件仅仅在文档1得到。 2.分析和设计1）Map过程首先使用默认的TextInputFormat类对输入文件进行处理，得到文本中每行的偏移量及其内容，Map过程首先必须分析输入的对，得到倒排索引中需要的三个信息：单词、文档URI和词频，如图所示： 存在两个问题，第一：对只能有两个值，在不使用Hadoop自定义数据类型的情况下，需要根据情况将其中的两个值合并成一个值，作为value或key值； 第二，通过一个Reduce过程无法同时完成词频统计和生成文档列表，所以必须增加一个Combine过程完成词频统计1234567891011121314151617181920public static class Map extends Mapper&lt;Object,Text,Text,Text&gt;&#123; private Text keyInfo = new Text(); private Text valueInfo = new Text(); private FileSplit split; //存储所在文件的路径 public void map(Object key,Text value,Context context) throws IOException,InterruptedException&#123; split = (FileSplit)context.getInputSplit(); //获取当前任务分割的单词所在的文件路径 StringTokenizer itr = new StringTokenizer(value.toString()); while(itr.hasMoreTokens())&#123; keyInfo.set(itr.nextToken()+&quot;+&quot;+split.getPath().toString()); //keyvalue是由单词和URI组成的 valueInfo.set(&quot;1&quot;); //value值设置成1 context.write(keyInfo,valueInfo); &#125; &#125; &#125; （2）Combine过程将key值相同的value值累加，得到一个单词在文档中的词频，如图 123456789101112131415161718 public static class Combiner extends Reducer&lt;Text,Text,Text,Text&gt;&#123; private Text info = new Text(); public void reduce(Text key,Iterable&lt;Text&gt;values,Context context) throws IOException, InterruptedException&#123; int sum = 0; for(Text value:values)&#123; sum += Integer.parseInt(value.toString()); &#125;// int index = key.toString().indexOf(&quot;+&quot;);// info.set(key.toString().substring(index+1)+&quot;:&quot;+sum); // key.set(key.toString().substring(0,index)); String record = key.toString(); String[] str = record.split(&quot;[+]&quot;); info.set(str[1]+&quot;:&quot;+sum); key.set(str[0]); context.write(key,info); &#125; &#125; （3）Reduce过程讲过上述两个过程后，Reduce过程只需将相同key值的value值组合成倒排索引文件所需的格式即可，剩下的事情就可以直接交给MapReduce框架进行处理了 1234567891011121314public static class Reduce extends Reducer&lt;Text,Text,Text,Text&gt;&#123; private Text result = new Text(); public void reduce(Text key,Iterable&lt;Text&gt;values,Context context) throws IOException, InterruptedException&#123; String value =new String(); for(Text value1:values)&#123; value += value1.toString()+&quot; ; &quot;; &#125; result.set(value); context.write(key,result); &#125; &#125; 完整代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798package ReverseIndex;import java.io.*;import java.util.StringTokenizer;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.*;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.FileSplit;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class ReverseIndex &#123; public static class Map extends Mapper&lt;Object,Text,Text,Text&gt;&#123; private Text keyInfo = new Text(); private Text valueInfo = new Text(); private FileSplit split; //存储所在文件的路径 public void map(Object key,Text value,Context context) throws IOException,InterruptedException&#123; split = (FileSplit)context.getInputSplit(); //获取当前任务分割的单词所在的文件路径 StringTokenizer itr = new StringTokenizer(value.toString()); while(itr.hasMoreTokens())&#123; keyInfo.set(itr.nextToken()+&quot;+&quot;+split.getPath().toString()); //keyvalue是由单词和URI组成的 valueInfo.set(&quot;1&quot;); //value值设置成1 context.write(keyInfo,valueInfo); &#125; &#125; &#125; public static class Combiner extends Reducer&lt;Text,Text,Text,Text&gt;&#123; private Text info = new Text(); public void reduce(Text key,Iterable&lt;Text&gt;values,Context context) throws IOException, InterruptedException&#123; int sum = 0; for(Text value:values)&#123; sum += Integer.parseInt(value.toString()); &#125;//下面三行注释和紧接着四行功能一样，只不过实现方法不一样罢了// int index = key.toString().indexOf(&quot;+&quot;);// info.set(key.toString().substring(index+1)+&quot;:&quot;+sum); // key.set(key.toString().substring(0,index));//对传进来的key进行拆分，以+为界 String record = key.toString(); String[] str = record.split(&quot;[+]&quot;); info.set(str[1]+&quot;:&quot;+sum); key.set(str[0]); context.write(key,info); &#125; &#125; public static class Reduce extends Reducer&lt;Text,Text,Text,Text&gt;&#123; private Text result = new Text(); public void reduce(Text key,Iterable&lt;Text&gt;values,Context context) throws IOException, InterruptedException&#123; String value =new String(); for(Text value1:values)&#123; value += value1.toString()+&quot; ; &quot;; &#125; result.set(value); context.write(key,result); &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException,InterruptedException &#123; // TODO Auto-generated method stub Job job = new Job(); job.setJarByClass(ReverseIndex.class); job.setNumReduceTasks(1); //设置reduce的任务数量为1，平常的小测试不需要开辟太多的reduce任务进程 job.setMapperClass(Map.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); job.setCombinerClass(Combiner.class); job.setReducerClass(Reduce.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); FileInputFormat.addInputPath(job, new Path(&quot;/thinkgamer/input&quot;)); FileOutputFormat.setOutputPath(job, new Path(&quot;/thinkgamer/output&quot;)); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"数据计算","slug":"数据计算","permalink":"http://yoursite.com/categories/数据计算/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://yoursite.com/tags/Hadoop/"},{"name":"Mapreduce","slug":"Mapreduce","permalink":"http://yoursite.com/tags/Mapreduce/"}]},{"title":"MapReducer中的多次归约处理","slug":"Hadoop/MapReducer中的多次归约处理","date":"2015-07-28T01:06:00.000Z","updated":"2018-04-14T19:38:24.827Z","comments":true,"path":"2015/07/28/Hadoop/MapReducer中的多次归约处理/","link":"","permalink":"http://yoursite.com/2015/07/28/Hadoop/MapReducer中的多次归约处理/","excerpt":"我们知道，MapReduce是分为Mapper任务和Reducer任务，Mapper任务的输出，通过网络传输到Reducer任务端，作为输入。 在Reducer任务中，通常做的事情是对数据进行归约处理。既然数据来源是Mapper任务的输出，那么是否可以在Mapper端对数据进行归约处理，业务逻辑与Reducer端做的完全相同。处理后的数据再传送到Reducer端，再做一次归约。这样的好处是减少了网络传输的数量。","text":"我们知道，MapReduce是分为Mapper任务和Reducer任务，Mapper任务的输出，通过网络传输到Reducer任务端，作为输入。 在Reducer任务中，通常做的事情是对数据进行归约处理。既然数据来源是Mapper任务的输出，那么是否可以在Mapper端对数据进行归约处理，业务逻辑与Reducer端做的完全相同。处理后的数据再传送到Reducer端，再做一次归约。这样的好处是减少了网络传输的数量。 可能有人疑惑几个问题： 为什么需要在Mapper端进行归约处理？ 为什么可以在Mapper端进行归约处理？ 既然在Mapper端可以进行归约处理，为什么在Reducer端还要处理？ 回答第一个问题：因为在Mapper进行归约后，数据量变小了，这样再通过网络传输时，传输时间就变短了，减少了整个作业的运行时间。 回答第二个问题：因为Reducer端接收的数据就是来自于Mapper端。我们在Mapper进行归约处理，无非就是把归约操作提前到Mapper端做而已。 回答第三个问题：因为Mapper端的数据仅仅是本节点处理的数据，而Reducer端处理的数据是来自于多个Mapper任务的输出。因此在Mapper不能归约的数据，在Reducer端有可能归约处理。 在Mapper进行归约的类称为Combiner。那么，怎么写Combiner哪？非常简单，就是我们自定义的Reducer类。那么，怎么用哪？更简单，见如下代码 job.setCombineClass(Mapper.class) 要注意的是，Combiner只在Mapper任务所在的节点运行，不会跨Mapper任务运行。Reduce端接收所有Mapper端的输出来作为输入。虽然两边的归约类是同一个，但是执行的位置完全不一样。 并不是所有的归约工作都可以使用Combiner来做。比如求平均值就不能使用Combiner。因为对于平均数的归约算法不能多次调用。 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"数据计算","slug":"数据计算","permalink":"http://yoursite.com/categories/数据计算/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://yoursite.com/tags/Hadoop/"},{"name":"Mapreduce","slug":"Mapreduce","permalink":"http://yoursite.com/tags/Mapreduce/"}]},{"title":"hadoop命令——hdfs","slug":"Hadoop/hadoop命令——hdfs","date":"2015-07-11T09:11:00.000Z","updated":"2018-04-14T19:38:30.475Z","comments":true,"path":"2015/07/11/Hadoop/hadoop命令——hdfs/","link":"","permalink":"http://yoursite.com/2015/07/11/Hadoop/hadoop命令——hdfs/","excerpt":"hdfs是hadoop大体系下的分布式文件管理系统，是英文Hadoop Distributed File System的简写，其常用命令如下：","text":"hdfs是hadoop大体系下的分布式文件管理系统，是英文Hadoop Distributed File System的简写，其常用命令如下： 一：fs命令（和Linux终端运行命令一致，也是hdfs最常用命令） 二：其他相关命令1、hadoop 归档文件shell： hadoop archive -archiveName file.har -p /gyt/input /gyt/output (file.har为归档后的文件 /gyt/inut/为多个文件所在目录 /gyt/output/是归档后的输出目录) 2、运行JAR程序包shell：hadoop jar /home/hadoop/hadoop-1.1.2/hadoop-examples-1.1.2.jar wordcount /user/hadoop/input output（XXX.jar是程序目录，wordcount是程序入口，XXX/input是文件输入源，output是文件输出源） 4、查看HDFS状态：hadoop dfsadmin -report比如有哪些datanode，每个datanode的情况 5、离开安全模式：hadoop dfsadmin -safemode leave 6、进入安全模式： hadoop dfsadmin -safemode enter 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"数据计算","slug":"数据计算","permalink":"http://yoursite.com/categories/数据计算/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://yoursite.com/tags/Hadoop/"},{"name":"Hdfs","slug":"Hdfs","permalink":"http://yoursite.com/tags/Hdfs/"}]},{"title":"二、hadoop伪分布搭建","slug":"Hadoop/二、hadoop伪分布搭建","date":"2015-07-07T12:14:00.000Z","updated":"2018-04-14T19:38:15.987Z","comments":true,"path":"2015/07/07/Hadoop/二、hadoop伪分布搭建/","link":"","permalink":"http://yoursite.com/2015/07/07/Hadoop/二、hadoop伪分布搭建/","excerpt":"环境虚拟机：VirtualBoxUbuntu:14.04hadoop:2.6 安装1、创建hadoop用户","text":"环境虚拟机：VirtualBoxUbuntu:14.04hadoop:2.6 安装1、创建hadoop用户 sudo useradd -m hadoop -s/bin/bash【Ubuntu终端复制粘贴快捷键】【在Ubuntu终端窗口中，复制粘贴的快捷键需要加上shift，即粘贴是 ctrl+shift+v。】使用如下命令修改密码，按提示输入两次密码 hadoop :1sudo passwd hadoop 可为 hadoop 用户增加管理员权限，方便部署，避免一些对新手来说比较棘手的权限问题：1sudo adduser hadoop sudo 2、切换到hadoop用户下1su hadoop 3、安装SSH server、配置SSH无密码登陆集群、单节点模式都需要用到SSH登陆（类似于远程登陆，你可以登录某台Linux电脑，并且在上面运行命令），Ubuntu 默认已安装了 SSH client，此外还需要安装 SSH server：1sudo apt-get install openssh-server 安装后，可以使用如下命令登陆本机：1ssh localhost 此时会有提示(SSH首次登陆提示)，输入 yes 。然后按提示输入密码hadoop，这样就登陆到本机了。 但这样登陆是需要每次输入密码的，我们需要配置成SSH无密码登陆比较方便。 首先退出刚才的 ssh，就回到了我们原先的终端窗口，然后利用 ssh-keygen 生成密钥，并将密钥加入到授权中：1234exit # 退出刚才的 ssh localhostcd ~/.ssh/ # 若没有该目录，请先执行一次ssh localhostssh-keygen -t rsa # 会有提示，都按回车就可以cat id_rsa.pub &gt;&gt; authorized_keys # 加入授权 此时再用 ssh localhost 命令，无需输入密码就可以直接登陆此时再用 ssh localhost 命令，无需输入密码就可以直接登陆 4、安装Java环境Java环境可选择 Oracle 的 JDK，或是 OpenJDK，按http://wiki.apache.org/hadoop/HadoopJavaVersions中说的，新版本在 OpenJDK 1.7 下是没问题的。为图方便，这边直接通过命令安装 OpenJDK 7。1sudo apt-get install openjdk-7-jre openjdk-7-jdk OpenJDK 默认的安装位置为: /usr/lib/jvm/java-7-openjdk-amd64 (32位系统则是 /usr/lib/jvm/java-7-openjdk-i86 ，可通过命令dpkg -L openjdk-7-jdk查看到)。安装完后就可以使用了，可以用java -version 检查一下。 接着需要配置一下 JAVA_HOME 环境变量，为方便，我们在 ~/.bashrc 中进行设置（扩展阅读:设置Linux环境变量的方法和区别）：1vi ~/.bashrc 在文件最前面添加如下单独一行（注意 = 号前后不能有空格），并保存：1export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64 如下图所示（该文件原本可能不存在，内容为空，这不影响）： 配置JAVA_HOME变量 接着还需要让该环境变量生效，执行如下代码：12source ~/.bashrc# 使变量设置生效echo $JAVA_HOME# 检验是否设置正确 设置正确的话，会输出如下结果： 成功配置JAVA_HOME变量 5、安装hadoop进入hadoop所在的目录将其解压到/usr/local/hadoop12345sudo tar -zxvf ./hadoop-2.6.0.tar.gz -C /usr/local # 解压到/usr/local中cd /usr/local/sudo mv ./hadoop-2.6.0/ ./hadoop # 将文件夹名改为hadoopsudo chown -R hadoop:hadoop ./hadoop # 修改文件权限 Hadoop解压后即可使用。输入如下命令来检查 Hadoop是否可用，成功则会显示命令用法：12cd ./hadoop./bin/hadoop 6、hadoop伪分布配置Hadoop 的配置文件位于 /usr/local/hadoop/etc/hadoop/ 中，伪分布式需要修改2个配置文件core-site.xml 和hdfs-site.xml 。Hadoop的配置文件是 xml 格式，每个配置以声明 property 的 name 和 value 的方式来实现。 修改配置文件 core-site.xml (vim /usr/local/hadoop/etc/hadoop/core-site.xml)，将当中的12&lt;configuration&gt;&lt;/configuration&gt; 修改为下面配置：1234567891011&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 同样的，修改配置文件 hdfs-site.xml：1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改配置文件yarn-site.xml12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 配置完成后，执行 namenode 的格式1bin/hdfs namenode -format 成功的话，会看到successfully formatted 的提示，且倒数第5行的提示如下，Exitting with status 0 表示成功，若为Exitting with status 1 则是出错 接着开启如下进程12sbin/start-dfs.shsbin/start-yarn.sh 至此，所有的已经安装完事，且所有服务都已经启动 验证http://127.0.0.1:8088 http://localhost:50070 提示每次进入虚拟机系统时必须先进入hadoop用户下（su hadoop），才能开启服务，否则会报错参考文章：www.powerxing.com/install-hadoop/ QQ交流：1923361654 hadoop完全分布式部署参考：http://blog.csdn.net/gamer_gyt/article/details/51991893 hadoop单机版部署参考：http://blog.csdn.net/gamer_gyt/article/details/46545303 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"数据计算","slug":"数据计算","permalink":"http://yoursite.com/categories/数据计算/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://yoursite.com/tags/Hadoop/"}]},{"title":"一、Hadoop2.6.0单机模式配置","slug":"Hadoop/一、Hadoop2-6-0单机模式配置","date":"2015-06-18T02:49:00.000Z","updated":"2018-04-14T19:38:12.291Z","comments":true,"path":"2015/06/18/Hadoop/一、Hadoop2-6-0单机模式配置/","link":"","permalink":"http://yoursite.com/2015/06/18/Hadoop/一、Hadoop2-6-0单机模式配置/","excerpt":"增加hadoop用户组，同时在该组里增加hadoop用户，后续在涉及到hadoop操作时，我们使用该用户。","text":"增加hadoop用户组，同时在该组里增加hadoop用户，后续在涉及到hadoop操作时，我们使用该用户。 一、在Ubuntu下创建hadoop组和hadoop用户1、创建hadoop用户组 2、创建hadoop用户1sudo adduser -ingroup hadoop hadoop 回车后会提示输入新的UNIX密码，这是新建用户hadoop的密码，输入回车即可。 如果不输入密码，回车后会重新提示输入密码，即密码不能为空。 最后确认信息是否正确，如果没问题，输入 Y，回车即可。 3、为hadoop用户添加权限输入：sudo gedit /etc/sudoers回车，打开sudoers文件给hadoop用户赋予和root用户同样的权限 二、用新增加的hadoop用户登录Ubuntu系统三、安装ssh1sudo apt-get install openssh-server 安装完成后，启动服务1sudo /etc/init.d/ssh start 查看服务是否正确启动1ps -e | grep ssh 设置免密码登录，生成私钥和公钥1ssh-keygen -t rsa -P &quot;&quot; 此时会在／home／hadoop/.ssh下生成两个文件：id_rsa和id_rsa.pub，前者为私钥，后者为公钥。 下面我们将公钥追加到authorized_keys中，它用户保存所有允许以当前用户身份登录到ssh客户端用户的公钥内容。1cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 登录ssh1ssh localhost 退出1exit 四、安装Java环境1sudo apt-get install openjdk-7-jdk 查看安装结果，输入命令：java -version，结果如下表示安装成功。 五、安装hadoop2.4.01、官网下载http://mirror.bit.edu.cn/apache/hadoop/common/2、安装解压1sudo tar xzf hadoop-2.4.0.tar.gz 假如我们要把hadoop安装到/usr/local下拷贝到/usr/local/下，文件夹为hadoop1sudo mv hadoop-2.4.0 /usr/local/hadoop 赋予用户对该文件夹的读写权限1sudo chmod 774 /usr/local/hadoop 3、配置1）配置~/.bashrc配置该文件前需要知道Java的安装路径，用来设置JAVA_HOME环境变量，可以使用下面命令行查看安装路径1update-alternatives - -config java 执行结果如下： 完整的路径为 1/usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java 我们只取前面的部分 /usr/lib/jvm/java-7-openjdk-amd64配置.bashrc文件1sudo gedit ~/.bashrc 该命令会打开该文件的编辑窗口，在文件末尾追加下面内容，然后保存，关闭编辑窗口。123456789101112#HADOOP VARIABLES STARTexport JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64export HADOOP_INSTALL=/usr/local/hadoopexport PATH=$PATH:$HADOOP_INSTALL/binexport PATH=$PATH:$HADOOP_INSTALL/sbinexport HADOOP_MAPRED_HOME=$HADOOP_INSTALLexport HADOOP_COMMON_HOME=$HADOOP_INSTALLexport HADOOP_HDFS_HOME=$HADOOP_INSTALLexport YARN_HOME=$HADOOP_INSTALLexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/nativeexport HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_INSTALL/lib&quot;#HADOOP VARIABLES END 最终结果如下图： 执行下面命，使添加的环境变量生效：1source ~/.bashrc 2）编辑/usr/local/hadoop/etc/hadoop/hadoop-env.sh执行下面命令，打开该文件的编辑窗口1sudo gedit /usr/local/hadoop/etc/hadoop/hadoop-env.sh 找到JAVA_HOME变量，修改此变量如下1export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64 修改hadoop-env.sh文件 六、WordCount测试单机模式安装完成，下面通过执行hadoop自带实例WordCount验证是否安装成功： 在/usr/local/hadoop路径下创建input文件夹：12mkdir input(或 sudo mkdir /usr/local/hadoop/input) 拷贝README.txt到input：1cp README.txt input 执行WordCount：1bin/hadoop jarshare/hadoop/mapreduce/sources/hadoop-mapreduce-examples-2.4.0-sources.jarorg.apache.hadoop.examples.WordCount input output 执行1cat output/* 查看字符统计结果 至此，单机模式安装成功！ hadoop伪分布部署参考：点击打开链接 hadoop完全分布式部署参考：点击打开链接 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"数据计算","slug":"数据计算","permalink":"http://yoursite.com/categories/数据计算/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://yoursite.com/tags/Hadoop/"}]},{"title":"逝去的夏天","slug":"随手记/逝去的夏天","date":"2013-10-01T06:21:17.000Z","updated":"2018-03-28T16:01:50.021Z","comments":true,"path":"2013/10/01/随手记/逝去的夏天/","link":"","permalink":"http://yoursite.com/2013/10/01/随手记/逝去的夏天/","excerpt":"那年的夏天，我们拎起书包 无奈的走向，复习班的怀抱 幻想一年后，大学生活的逍遥 寂寥与苦恼，我们一起去熬","text":"那年的夏天，我们拎起书包 无奈的走向，复习班的怀抱 幻想一年后，大学生活的逍遥 寂寥与苦恼，我们一起去熬 月光下，漫步操场，有人和心爱姑娘倾诉着衷肠 听着歌，踏进流年，哼出那动人的歌谣在嘴边 到夜晚，傻傻发呆，等待她回复美丽的笑颜 被窝里，谁流着泪，望着椭圆和磁场，画出无尽的茫然 表示从下课到上课的铃声原来一直没变 表示老班面庞带着囧样长的多荒唐 看着钟摆晃荡 时光飞扬 还有可爱姑娘 看着曾经传出的纸条 跨过几人几肩膀 我们追逐梦想，奔向希望，一路走来跌跌又撞撞 留下汗水 擦干眼泪向前一起闯 考试和爱情的守望多么令人向往 各种壮丽诗行和篇章从此不会再彷徨 多年后，你若想念，我们一直都坚持的笑颜 多年后，你若怀念，课堂上睡觉的瓜子脸 都说吃他的菜喝他的汤钱照样会花光 都说拍她的肩请她吃糖一笑为红颜 都说一分一秒努力学习或者传纸条 都说会把功劳全都记在流年的怀抱 我们即将高考，分别之后，奔波在中国不同角落 瞥见短发女孩，还会以为是我心动那个谁 蕾峰彩月金豆同桌，某某我等你 也许彼此把彼此丢进记忆的流年 但记得那排我们快乐的日子 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"随手记","slug":"随手记","permalink":"http://yoursite.com/categories/随手记/"}],"tags":[{"name":"随手记","slug":"随手记","permalink":"http://yoursite.com/tags/随手记/"}]},{"title":"十八岁，半夏锦年","slug":"随手记/十八岁，半夏锦年","date":"2012-06-10T06:25:51.000Z","updated":"2018-03-28T16:01:33.141Z","comments":true,"path":"2012/06/10/随手记/十八岁，半夏锦年/","link":"","permalink":"http://yoursite.com/2012/06/10/随手记/十八岁，半夏锦年/","excerpt":"散落一地，流离 回忆不起，青石板长巷的雨季 独自撑伞，没有你陪伴的故地 簌簌樱花，零落成思绪","text":"散落一地，流离 回忆不起，青石板长巷的雨季 独自撑伞，没有你陪伴的故地 簌簌樱花，零落成思绪 一场雨，让我离开这里 出其不意，燕啄泥 诉说那十八岁半夏锦年 未完的结局 风萧萧，雨寒寒 断了谁的琴弦，无语问寒蝉 倚窗边，空白回忆里 艰难婉转… 送走了我的青春，迎来我未知的迷惘，将来我会身在何方，我将永世不忘。 再见！ 打开微信扫一扫，关注微信公众号【数据与算法联盟】 打开微信扫一扫，加小编好友，拉你进数据算法交流群","categories":[{"name":"随手记","slug":"随手记","permalink":"http://yoursite.com/categories/随手记/"}],"tags":[{"name":"随手记","slug":"随手记","permalink":"http://yoursite.com/tags/随手记/"}]}]}