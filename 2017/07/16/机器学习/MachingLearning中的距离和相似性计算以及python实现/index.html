<!DOCTYPE HTML>
<html>

<head>
	<!-- hexo-inject:begin --><!-- hexo-inject:end --><link rel="bookmark"  type="image/x-icon"  href="img/favicon.ico"/>
	<link rel="shortcut icon" href="img/favicon.ico">
	
			    <title>
    文艺与Code | Thinkgamer的博客
    </title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="/css/mic_main.css" />
    <link rel="stylesheet" href="/css/dropdownMenu.css" />
    <meta name="keywords" content="Thinkgamer Hadoop 数据挖掘 机器学习 深度学习 IT网站 博客" />
    
    	<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	 
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css" />
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('/img/bg.jpg') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

			    
  


    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
	
</head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_coy.css" />
<link rel="stylesheet" href="/css/typo.css" />
<!-- 文章页 -->
<body class="is-loading">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="/" class="logo">Thinkgamer</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special" >
            <ul class="menu links" >
			<!-- Homepage  主页  --> 
			<li >
	            <a href="/" rel="nofollow">主页</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <li class="active">
	            <a href="#s1">分类</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="category-link" href="/categories/大数据/">大数据</a></li><li><a class="category-link" href="/categories/机器学习/">机器学习</a></li><li><a class="category-link" href="/categories/编程与系统/">编程与系统</a></li><li><a class="category-link" href="/categories/这夏未眠/">这夏未眠</a></li><li><a class="category-link" href="/categories/随手记/">随手记</a>
	                    </ul>
	        </li>
	        
	        <!-- archives  归档   --> 
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/tag/" title="标签">
		                标签
		            </a>
		        </li>
		        
		        <li>
		            <a href="/about/" title="关于我">
		                关于我
		            </a>
		        </li>
		        
		        <li>
		            <a href="/cooperation/" title="商务合作">
		                商务合作
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
		            
		                <li><a href="https://github.com/thinkgamer" class="icon fa-github"><span class="label">GitHub</span></a></li>
		            
		            
		            
		            
			</ul>
</nav>

        <div id="main" >
            <div class ="post_page_title_img" style="height: 25rem;background-image: url(/images/thumbs/ml4.jpg);background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;" >
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2 >MachingLearning中的距离和相似性计算以及python实现</h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
                <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>写这篇文章的目的不是说摘抄网上其他人的总结，刚才最近在看这方面的东西，为了让自己能够实际的去感受下每种求距离公式的差别，然后用python进行具体实现。<br><a id="more"></a><br>在机器学习中，经常要用到距离和相似性的计算公式，我么要常计算个体之间的差异大小，继而评价个人之间的差异性和相似性，最常见的就是数据分析中的相关分析，数据挖掘中的分类和聚类算法。如利用k-means进行聚类时，判断个体所属的类别，要利用距离计算公式计算个体到簇心的距离，如利用KNN进行分类时，计算个体与已知类别之间的相似性，从而判断个体所属的类别等。</p>
<p>文章编辑的过程中或许存在一个错误或者不合理的地方，欢迎指正。</p>
<p>参考：<a href="http://www.cnblogs.com/heaad/archive/2011/03/08/1977733.html" target="_blank" rel="external">http://www.cnblogs.com/heaad/archive/2011/03/08/1977733.html</a></p>
<p>推荐：<a href="https://my.oschina.net/hunglish/blog/787596" target="_blank" rel="external">https://my.oschina.net/hunglish/blog/787596</a></p>
<h1 id="欧氏距离"><a href="#欧氏距离" class="headerlink" title="欧氏距离"></a>欧氏距离</h1><p>也称欧几里得距离，是指在m维空间中两个点之间的真实距离。欧式距离在ML中使用的范围比较广，也比较通用，就比如说利用k-Means对二维平面内的数据点进行聚类，对魔都房价的聚类分析（price/m^2 与平均房价）等。</p>
<h2 id="二维空间的欧氏距离"><a href="#二维空间的欧氏距离" class="headerlink" title="二维空间的欧氏距离"></a>二维空间的欧氏距离</h2><p>二维平面上两点a(x1,y1)与b(x2,y2)间的欧氏距离</p>
<script type="math/tex; mode=display">
d12 =\sqrt{(x_{1}-x_{2})^2+(y_{1}-y_{2})^2}</script><p>python 实现为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># coding: utf-8</span><br><span class="line"></span><br><span class="line">from numpy import *</span><br><span class="line"></span><br><span class="line">def twoPointDistance(a,b):</span><br><span class="line">	d = sqrt( (a[0]-b[0])**2 + (a[1]-b[1])**2 )</span><br><span class="line">	return d</span><br><span class="line"></span><br><span class="line">print &apos;a,b 二维距离为：&apos;,twoPointDistance((1,1),(2,2))</span><br></pre></td></tr></table></figure>
<h2 id="三维空间的欧氏距离"><a href="#三维空间的欧氏距离" class="headerlink" title="三维空间的欧氏距离"></a>三维空间的欧氏距离</h2><p>三维空间两点a(x1,y1,z1)与b(x2,y2,z2)间的欧氏距离</p>
<script type="math/tex; mode=display">d12 =\sqrt{(x_{1}-x_{2})^2+(y_{1}-y_{2})^2+(z_{1}-z_{2})^2}</script><p>python 实现为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def threePointDistance(a,b):</span><br><span class="line">	d = sqrt( (a[0]-b[0])**2 + (a[1]-b[1])**2 + (a[2]-b[2])**2 )</span><br><span class="line">	return d</span><br><span class="line"></span><br><span class="line">print &apos;a,b 三维距离为：&apos;,threePointDistance((1,1,1),(2,2,2))</span><br></pre></td></tr></table></figure></p>
<h2 id="多维空间的欧氏距离"><a href="#多维空间的欧氏距离" class="headerlink" title="多维空间的欧氏距离"></a>多维空间的欧氏距离</h2><p>两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的欧氏距离</p>
<script type="math/tex; mode=display">
\sqrt{\sum_{n}^{k=1}(x_{1k}-x_{2k})^2 }</script><p>python 实现为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def distance(a,b):</span><br><span class="line">	sum = 0</span><br><span class="line">	for i in range(len(a)):</span><br><span class="line">		sum += (a[i]-b[i])**2</span><br><span class="line">	return sqrt(sum)</span><br><span class="line"></span><br><span class="line">print &apos;a,b 多维距离为：&apos;,distance((1,1,2,2),(2,2,4,4))</span><br></pre></td></tr></table></figure>
<p>这里传入的参数可以是任意维的，该公式也适应上边的二维和三维</p>
<h1 id="标准欧氏距离"><a href="#标准欧氏距离" class="headerlink" title="标准欧氏距离"></a>标准欧氏距离</h1><p>标准化欧氏距离是针对简单欧氏距离的缺点而作的一种改进方案。标准欧氏距离的思路：既然数据各维分量的分布不一样，好吧！那我先将各个分量都“标准化”到均值、方差相等吧。均值和方差标准化到多少呢？这里先复习点统计学知识吧，假设样本集X的均值(mean)为m，标准差(standard deviation)为s，那么X的“标准化变量”表示为：</p>
<p>　　而且标准化变量的数学期望为0，方差为1。因此样本集的标准化过程(standardization)用公式描述就是：
　　</p>
<script type="math/tex; mode=display">
X^* = \frac{X-m}{s}</script><p>标准化后的值 =  ( 标准化前的值  － 分量的均值 ) /分量的标准差</p>
<p>经过简单的推导就可以得到两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的标准化欧氏距离的公式：</p>
<script type="math/tex; mode=display">
d_{12} =\sqrt {\sum_{k=1}^{n} (\frac{x_{1k}-x_{2k}}{s_{k}})^2}</script><p>如果将方差的倒数看成是一个权重，这个公式可以看成是一种加权欧氏距离(Weighted Euclidean distance)。</p>
<p>python 实现为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def moreBZOSdis(a,b):</span><br><span class="line">    sumnum = 0</span><br><span class="line">    for i in range(len(a)):</span><br><span class="line">        # 计算si 分量标准差</span><br><span class="line">        avg = (a[i]-b[i])/2</span><br><span class="line">        si = sqrt( (a[i] - avg) ** 2 + (b[i] - avg) ** 2 )</span><br><span class="line">        sumnum += ((a[i]-b[i])/si ) ** 2</span><br><span class="line">	</span><br><span class="line">    return sqrt(sumnum)</span><br><span class="line"></span><br><span class="line">print &apos;a,b 标准欧式距离：&apos;,moreBZOSdis((1,2,1,2),(3,3,3,4))</span><br></pre></td></tr></table></figure></p>
<h1 id="曼哈顿距离"><a href="#曼哈顿距离" class="headerlink" title="曼哈顿距离"></a>曼哈顿距离</h1><p>又称为城市街区距离（City Block distance）, 想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。实际驾驶距离就是这个“曼哈顿距离”。而这也是曼哈顿距离名称的来源。同样曼哈顿距离也分为二维，三维和多维。</p>
<p>在计程车几何学中，一个圆是由从圆心向各个固定曼哈顿距离标示出来的点围成的区域，因此这种圆其实就是旋转了45度的正方形。如果有一群圆，且任两圆皆相交，则整群圆必在某点相交；因此曼哈顿距离会形成一个超凸度量空间。</p>
<p>这里有一篇人脸表情分类的论文采用的曼哈顿距离进行计算的，<a href="http://download.csdn.net/detail/gamer_gyt/9899825" target="_blank" rel="external">一种人脸表情分类的新方法——Manhattan距离</a></p>
<h2 id="二维曼哈顿距离"><a href="#二维曼哈顿距离" class="headerlink" title="二维曼哈顿距离"></a>二维曼哈顿距离</h2><p>二维平面两点a(x1,y1)与b(x2,y2)间的曼哈顿距离</p>
<script type="math/tex; mode=display">
d12 =\left | x_{1}-x_{2} \right |  + \left |y_{1}-y_{2}  \right |</script><p>python实现为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def twoMHDdis(a,b):</span><br><span class="line">    return abs(a[0]-b[0])+abs(a[1]-b[1])</span><br><span class="line"></span><br><span class="line">print &apos;a,b 二维曼哈顿距离为：&apos;, twoMHDdis((1,1),(2,2))</span><br></pre></td></tr></table></figure></p>
<h2 id="三维曼哈顿距离"><a href="#三维曼哈顿距离" class="headerlink" title="三维曼哈顿距离"></a>三维曼哈顿距离</h2><p>三维平面两点a(x1,y1,z1)与b(x2,y2,z2)间的曼哈顿距离</p>
<script type="math/tex; mode=display">
d12 =\left | x_{1}-x_{2} \right |  + \left |y_{1}-y_{2}  \right | + \left |z_{1}-z_{2}  \right |</script><p>python实现为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def threeMHDdis(a,b):</span><br><span class="line">	return abs(a[0]-b[0])+abs(a[1]-b[1]) + abs(a[2]-b[2])</span><br><span class="line"> </span><br><span class="line">print &apos;a,b 三维曼哈顿距离为：&apos;, threeMHDdis((1,1,1),(2,2,2))</span><br></pre></td></tr></table></figure></p>
<h2 id="多维曼哈顿距离"><a href="#多维曼哈顿距离" class="headerlink" title="多维曼哈顿距离"></a>多维曼哈顿距离</h2><p>多维平面两点a(x1,y1)与b(x2,y2)间的曼哈顿距离</p>
<script type="math/tex; mode=display">
d12 = \sum_{k=1}^{n} \left | x_{1k} - x_{2k} \right |</script><p>python实现为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def moreMHDdis(a,b):</span><br><span class="line">    sum = 0 </span><br><span class="line">    for i in range(len(a)):</span><br><span class="line">        sum += abs(a[i]-b[i])</span><br><span class="line">    return sum</span><br><span class="line"></span><br><span class="line">print &apos;a,b 多维曼哈顿距离为：&apos;, moreMHDdis((1,1,1,1),(2,2,2,2))</span><br></pre></td></tr></table></figure></p>
<p>由于维距离计算是比较灵活的，所以也同样适合二维和三维。</p>
<h1 id="切比雪夫距离"><a href="#切比雪夫距离" class="headerlink" title="切比雪夫距离"></a>切比雪夫距离</h1><p>切比雪夫距离（Chebyshev Distance）的定义为：max( | x2-x1 | , |y2-y1 | , … ), 切比雪夫距离用的时候数据的维度必须是三个以上，这篇文章中<a href="http://blog.csdn.net/jerry81333/article/details/52632687" target="_blank" rel="external">曼哈顿距离，欧式距离，明式距离，切比雪夫距离区别</a> 给了一个很形象的解释如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">比如，有同样两个人，在纽约准备到北京参拜天安门，同一个地点出发的话，按照欧式距离来计算，是完全一样的。</span><br><span class="line"></span><br><span class="line">但是按照切比雪夫距离，这是完全不同的概念了。</span><br><span class="line"></span><br><span class="line">譬如，其中一个人是土豪，另一个人是中产阶级，第一个人就能当晚直接头等舱走人，而第二个人可能就要等机票什么时候打折再去，或者选择坐船什么的。</span><br><span class="line"></span><br><span class="line">这样来看的话，距离是不是就不一样了呢？</span><br><span class="line"></span><br><span class="line">或者还是不清楚，我再说的详细点。</span><br><span class="line"></span><br><span class="line">同样是这两个人，欧式距离是直接算最短距离的，而切比雪夫距离可能还得加上财力，比如第一个人财富值100，第二个只有30，虽然物理距离一样，但是所包含的内容却是不同的。</span><br></pre></td></tr></table></figure></p>
<h2 id="二维切比雪夫距离"><a href="#二维切比雪夫距离" class="headerlink" title="二维切比雪夫距离"></a>二维切比雪夫距离</h2><p>二维平面两点a(x1,y1)与b(x2,y2)间的切比雪夫距离</p>
<script type="math/tex; mode=display">
d_{12} = max( \left | x_{1} - x_{2} \right | , \left | y_{1} - y_{2} \right |)</script><p>python 实现为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def twoQBXFdis(a,b):</span><br><span class="line">    return max( abs(a[0]-b[0]), abs(a[1]-b[1]))</span><br><span class="line"></span><br><span class="line">print &apos;a,b二维切比雪夫距离：&apos; , twoQBXFdis((1,2),(3,4))</span><br></pre></td></tr></table></figure>
<h2 id="多维切比雪夫距离"><a href="#多维切比雪夫距离" class="headerlink" title="多维切比雪夫距离"></a>多维切比雪夫距离</h2><p>两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的切比雪夫距离</p>
<script type="math/tex; mode=display">
d12 = max_{i\epsilon n}( \left | x_{1i} - x_{2i} \right | )</script><p>python 实现为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def moreQBXFdis(a,b):</span><br><span class="line">    maxnum = 0</span><br><span class="line">    for i in range(len(a)):</span><br><span class="line">        if abs(a[i]-b[i]) &gt; maxnum:</span><br><span class="line">            maxnum = abs(a[i]-b[i])</span><br><span class="line">    return maxnum</span><br><span class="line"></span><br><span class="line">print &apos;a,b多维切比雪夫距离：&apos; , moreQBXFdis((1,1,1,1),(3,4,3,4))</span><br></pre></td></tr></table></figure></p>
<h1 id="马氏距离"><a href="#马氏距离" class="headerlink" title="马氏距离"></a>马氏距离</h1><p>有M个样本向量X1~Xm，协方差矩阵记为S，均值记为向量μ，则其中样本向量X到u的马氏距离表示为</p>
<script type="math/tex; mode=display">
D(x) = \sqrt{(X-\mu )^TS^{-1}(X-\mu)}</script><p>而其中向量Xi与Xj之间的马氏距离定义为</p>
<script type="math/tex; mode=display">
D(X_{i},X_{j}) = \sqrt{(X_{i}-X_{j} )^TS^{-1}(X_{i}-X_{j} )}</script><p> 若协方差矩阵是单位矩阵（各个样本向量之间独立同分布）,则公式就成了：</p>
<script type="math/tex; mode=display">
D(X_{i},X_{j}) = \sqrt{(X_{i}-X_{j} )^T(X_{i}-X_{j} )}</script><p>也就是欧氏距离了。</p>
<p>若协方差矩阵是对角矩阵，公式变成了标准化欧氏距离。</p>
<p>马氏距离的优缺点：量纲无关，排除变量之间的相关性的干扰。</p>
<h1 id="夹角余弦"><a href="#夹角余弦" class="headerlink" title="夹角余弦"></a>夹角余弦</h1><p>几何中夹角余弦可用来衡量两个向量方向的差异，机器学习中借用这一概念来衡量样本向量之间的差异。</p>
<h2 id="二维空间向量的夹角余弦相似度"><a href="#二维空间向量的夹角余弦相似度" class="headerlink" title="二维空间向量的夹角余弦相似度"></a>二维空间向量的夹角余弦相似度</h2><p>在二维空间中向量A(x1,y1)与向量B(x2,y2)的夹角余弦公式：</p>
<script type="math/tex; mode=display">
\cos \theta  = \frac{x_{1}x_{2} + y_{1}y_{2}}{ \sqrt{ x_{1}^2+x_{2}^2 }\sqrt{ y_{1}^2+y_{2}^2 } }</script><p>python 实现为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def twoCos(a,b):</span><br><span class="line">    cos = (a[0]*b[0]+a[1]*b[1]) / (sqrt(a[0]**2 + b[0]**2) * sqrt(a[1]**2 + b[1]**2) )</span><br><span class="line"></span><br><span class="line">    return cos</span><br><span class="line">print &apos;a,b 二维夹角余弦距离：&apos;,twoCos((1,1),(2,2))</span><br></pre></td></tr></table></figure></p>
<h2 id="多维空间向量的夹角余弦相似度"><a href="#多维空间向量的夹角余弦相似度" class="headerlink" title="多维空间向量的夹角余弦相似度"></a>多维空间向量的夹角余弦相似度</h2><p>两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)的夹角余弦</p>
<p>类似的，对于两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)，可以使用类似于夹角余弦的概念来衡量它们间的相似程度。</p>
<script type="math/tex; mode=display">
\cos \theta  = \frac{a \cdot  b}{\left | a \right | \left | b \right |}</script><p>即：</p>
<script type="math/tex; mode=display">
\cos \theta  = \frac{ \sum_{k=1}^{n} x_{1k}x_{2k} }{ \sqrt{ \sum_{k=1}^{n}x_{1k}^2 }\sqrt{ \sum_{k=1}^{n} x_{2k}^2 } }</script><p>python实现为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def moreCos(a,b):</span><br><span class="line">    sum_fenzi = 0.0</span><br><span class="line">    sum_fenmu_1,sum_fenmu_2 = 0,0</span><br><span class="line">    for i in range(len(a)):</span><br><span class="line">        sum_fenzi += a[i]*b[i]</span><br><span class="line">        sum_fenmu_1 += a[i]**2 </span><br><span class="line">        sum_fenmu_2 += b[i]**2 </span><br><span class="line"></span><br><span class="line">    return sum_fenzi/( sqrt(sum_fenmu_1) * sqrt(sum_fenmu_2) )</span><br><span class="line">print &apos;a,b 多维夹角余弦距离：&apos;,moreCos((1,1,1,1),(2,2,2,2))</span><br></pre></td></tr></table></figure></p>
<p>夹角余弦取值范围为[-1,1]。夹角余弦越大表示两个向量的夹角越小，夹角余弦越小表示两向量的夹角越大。当两个向量的方向重合时夹角余弦取最大值1，当两个向量的方向完全相反夹角余弦取最小值-1。</p>
<h1 id="闵可夫斯基距离"><a href="#闵可夫斯基距离" class="headerlink" title="闵可夫斯基距离"></a>闵可夫斯基距离</h1><p>闵氏距离不是一种距离，而是一组距离的定义</p>
<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>两个n维变量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的闵可夫斯基距离定义为：</p>
<script type="math/tex; mode=display">
\sqrt[p]{ \sum_{k=1}^{n} \left | x_{1k}-x_{2k} \right |^p}</script><p>其中p是一个变参数。</p>
<p>当p=1时，就是曼哈顿距离</p>
<p>当p=2时，就是欧氏距离</p>
<p>当p→∞时，就是切比雪夫距离</p>
<p>根据变参数的不同，闵氏距离可以表示一类的距离。</p>
<h2 id="闵氏距离的缺点"><a href="#闵氏距离的缺点" class="headerlink" title="闵氏距离的缺点"></a>闵氏距离的缺点</h2><p>闵氏距离，包括曼哈顿距离、欧氏距离和切比雪夫距离都存在明显的缺点。</p>
<p>举个例子：二维样本(身高,体重)，其中身高范围是150 ~ 190，体重范围是50 ~ 60，有三个样本：a(180,50)，b(190,50)，c(180,60)。那么a与b之间的闵氏距离（无论是曼哈顿距离、欧氏距离或切比雪夫距离）等于a与c之间的闵氏距离，但是身高的10cm真的等价于体重的10kg么？因此用闵氏距离来衡量这些样本间的相似度很有问题。</p>
<p>简单说来，闵氏距离的缺点主要有两个：(1)将各个分量的量纲(scale)，也就是“单位”当作相同的看待了。(2)没有考虑各个分量的分布（期望，方差等)可能是不同的。</p>
<h1 id="汉明距离"><a href="#汉明距离" class="headerlink" title="汉明距离"></a>汉明距离</h1><h2 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h2><p>两个等长字符串s1与s2之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。例如字符串“1111”与“1001”之间的汉明距离为2。</p>
<p>应用：信息编码（为了增强容错性，应使得编码间的最小汉明距离尽可能大）。</p>
<h2 id="python-实现"><a href="#python-实现" class="headerlink" title="python 实现"></a>python 实现</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def hanmingDis(a,b):</span><br><span class="line">    sumnum = 0</span><br><span class="line">    for i in range(len(a)):</span><br><span class="line">        if a[i]!=b[i]:</span><br><span class="line">            sumnum += 1</span><br><span class="line">    return sumnum</span><br><span class="line"></span><br><span class="line">print &apos;a,b 汉明距离：&apos;,hanmingDis((1,1,2,3),(2,2,1,3))</span><br></pre></td></tr></table></figure>
<h1 id="杰卡德距离-amp-杰卡德相似系数"><a href="#杰卡德距离-amp-杰卡德相似系数" class="headerlink" title="杰卡德距离 &amp; 杰卡德相似系数"></a>杰卡德距离 &amp; 杰卡德相似系数</h1><h2 id="杰卡德距离"><a href="#杰卡德距离" class="headerlink" title="杰卡德距离"></a>杰卡德距离</h2><p>与杰卡德相似系数相反的概念是杰卡德距离(Jaccard distance)。杰卡德距离可用如下公式表示：</p>
<script type="math/tex; mode=display">
J_{\delta} (A,B) = \frac{| A \bigcup B | - | A \bigcap B |}{| A \bigcup B |}</script><p>杰卡德距离用两个集合中不同元素占所有元素的比例来衡量两个集合的区分度。</p>
<p>python 实现<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def jiekadeDis(a,b):</span><br><span class="line">    set_a = set(a)</span><br><span class="line">    set_b = set(b)</span><br><span class="line">    dis = float(len( (set_a | set_b) - (set_a &amp; set_b) ) )/ len(set_a | set_b)</span><br><span class="line">    return dis</span><br><span class="line"></span><br><span class="line">print &apos;a,b 杰卡德距离：&apos;, jiekadeDis((1,2,3),(2,3,4))</span><br></pre></td></tr></table></figure></p>
<h2 id="杰卡德相似系数"><a href="#杰卡德相似系数" class="headerlink" title="杰卡德相似系数"></a>杰卡德相似系数</h2><p>两个集合A和B的交集元素在A，B的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号J(A,B)表示。</p>
<script type="math/tex; mode=display">
J(A,B) = \frac{| A \bigcap B |}{| A \bigcup B |}</script><p>杰卡德相似系数是衡量两个集合的相似度一种指标。</p>
<p>python 实现<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def jiekadeXSDis(a,b):</span><br><span class="line">    set_a = set(a)</span><br><span class="line">    set_b = set(b)</span><br><span class="line">    dis = float(len(set_a &amp; set_b)  )/ len(set_a | set_b)</span><br><span class="line">    return dis</span><br><span class="line"></span><br><span class="line">print &apos;a,b 杰卡德相似系数：&apos;, jiekadeXSDis((1,2,3),(2,3,4))</span><br></pre></td></tr></table></figure></p>
<h2 id="杰卡德相似系数与杰卡德距离的应用"><a href="#杰卡德相似系数与杰卡德距离的应用" class="headerlink" title="杰卡德相似系数与杰卡德距离的应用"></a>杰卡德相似系数与杰卡德距离的应用</h2><p>可将杰卡德相似系数用在衡量样本的相似度上。</p>
<p>　　样本A与样本B是两个n维向量，而且所有维度的取值都是0或1。例如：A(0111)和B(1011)。我们将样本看成是一个集合，1表示集合包含该元素，0表示集合不包含该元素。</p>
<p>p ：样本A与B都是1的维度的个数</p>
<p>q ：样本A是1，样本B是0的维度的个数</p>
<p>r ：样本A是0，样本B是1的维度的个数</p>
<p>s ：样本A与B都是0的维度的个数</p>
<p>那么样本A与B的杰卡德相似系数可以表示为：</p>
<p>这里p+q+r可理解为A与B的并集的元素个数，而p是A与B的交集的元素个数。</p>
<p>而样本A与B的杰卡德距离表示为：</p>
<script type="math/tex; mode=display">
J= \frac{p}{p+q+r}</script><h1 id="相关系数-amp-相关距离"><a href="#相关系数-amp-相关距离" class="headerlink" title="相关系数 &amp; 相关距离"></a>相关系数 &amp; 相关距离</h1><h2 id="相关系数"><a href="#相关系数" class="headerlink" title="相关系数"></a>相关系数</h2><script type="math/tex; mode=display">
\rho_{XY} = \frac{Cov(X,Y)}{\sqrt{D(X)} \sqrt{D(Y)}}=\frac{ E( (X-EX) (Y-EY) ) }{ \sqrt{D(X)} \sqrt{D(Y)} }</script><p>相关系数是衡量随机变量X与Y相关程度的一种方法，相关系数的取值范围是[-1,1]。相关系数的绝对值越大，则表明X与Y相关度越高。当X与Y线性相关时，相关系数取值为1（正线性相关）或-1（负线性相关）。</p>
<p>python 实现<br>相关系数可以利用numpy库中的corrcoef函数来计算<br>例如 对于矩阵a,numpy.corrcoef(a)可计算行与行之间的相关系数，numpy.corrcoef(a,rowvar=0)用于计算各列之间的相关系数，输出为相关系数矩阵。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from numpy import  *</span><br><span class="line">a = array([[1, 1, 2, 2, 3],  </span><br><span class="line">       [2, 2, 3, 3, 5],  </span><br><span class="line">       [1, 4, 2, 2, 3]]) </span><br><span class="line"></span><br><span class="line">print corrcoef(a)</span><br><span class="line"></span><br><span class="line">&gt;&gt;array([[ 1.        ,  0.97590007,  0.10482848],</span><br><span class="line">       [ 0.97590007,  1.        ,  0.17902872],</span><br><span class="line">       [ 0.10482848,  0.17902872,  1.        ]])</span><br><span class="line"></span><br><span class="line">print corrcoef(a,rowvar=0)</span><br><span class="line"></span><br><span class="line">&gt;&gt;array([[ 1.        , -0.18898224,  1.        ,  1.        ,  1.        ],</span><br><span class="line">       [-0.18898224,  1.        , -0.18898224, -0.18898224, -0.18898224],</span><br><span class="line">       [ 1.        , -0.18898224,  1.        ,  1.        ,  1.        ],</span><br><span class="line">       [ 1.        , -0.18898224,  1.        ,  1.        ,  1.        ],</span><br><span class="line">       [ 1.        , -0.18898224,  1.        ,  1.        ,  1.        ]])</span><br></pre></td></tr></table></figure></p>
<h2 id="相关距离"><a href="#相关距离" class="headerlink" title="相关距离"></a>相关距离</h2><script type="math/tex; mode=display">
D_{xy} = 1 - \rho _{XY}</script><p>python 实现（基于相关系数）<br>同样针对矩阵a<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 行之间的相关距离</span><br><span class="line">ones(shape(corrcoef(a)),int) - corrcoef(a)</span><br><span class="line"></span><br><span class="line">&gt;&gt;array([[ 0.        ,  0.02409993,  0.89517152],</span><br><span class="line">       [ 0.02409993,  0.        ,  0.82097128],</span><br><span class="line">       [ 0.89517152,  0.82097128,  0.        ]])</span><br><span class="line">       </span><br><span class="line">       </span><br><span class="line"># 列之间的相关距离</span><br><span class="line">ones(shape(corrcoef(a,rowvar = 0)),int) - corrcoef(a,rowvar = 0)</span><br><span class="line"></span><br><span class="line">&gt;&gt;array([[ 0.        ,  1.18898224,  0.        ,  0.        ,  0.        ],</span><br><span class="line">       [ 1.18898224,  0.        ,  1.18898224,  1.18898224,  1.18898224],</span><br><span class="line">       [ 0.        ,  1.18898224,  0.        ,  0.        ,  0.        ],</span><br><span class="line">       [ 0.        ,  1.18898224,  0.        ,  0.        ,  0.        ],</span><br><span class="line">       [ 0.        ,  1.18898224,  0.        ,  0.        ,  0.        ]])</span><br></pre></td></tr></table></figure></p>
<h1 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h1><p>信息熵并不属于一种相似性度量，是衡量分布的混乱程度或分散程度的一种度量。分布越分散(或者说分布越平均)，信息熵就越大。分布越有序（或者说分布越集中），信息熵就越小。</p>
<p>计算给定的样本集X的信息熵的公式：</p>
<script type="math/tex; mode=display">
Entropy(X) = \sum_{i=1}^{n} -p_{i} log_{2}p_{i}</script><p>参数的含义：</p>
<p>n：样本集X的分类数</p>
<p>pi：X中第i类元素出现的概率</p>
<p>信息熵越大表明样本集S分类越分散，信息熵越小则表明样本集X分类越集中。。当S中n个分类出现的概率一样大时（都是1/n），信息熵取最大值log2(n)。当X只有一个分类时，信息熵取最小值0</p>
<p>python进行计算和实现可参考：<br><a href="http://blog.csdn.net/autoliuweijie/article/details/52244246" target="_blank" rel="external">http://blog.csdn.net/autoliuweijie/article/details/52244246</a></p>
<hr>
<center>
    <img src="/img/gzh.jpg" weight="250px" height="250px">
    <br>
  打开微信扫一扫，关注微信公众号【数据与算法联盟】
    <br>
    <br>
    <img src="/img/weixin.png" weight="250px" height="300px">
    <br>
    打开微信扫一扫，加小编好友，拉你进数据算法交流群

</center>
            </div>

            <!-- Post Comments -->
            
    <!-- 使用 DISQUS_CLICK -->
<div id="disqus-comment">
    <div id="disqus_thread"></div>

<!-- add animation -->
<style>
	.disqus_click_btn {
            line-height: 30px;
            margin: 0;
            min-width: 50px;
            padding: 0 14px;
            display: inline-block;
            font-family: "Roboto", "Helvetica", "Arial", sans-serif;
            font-size: 14px;
            font-weight: 400;
            text-transform: uppercase;
            letter-spacing: 0;
            overflow: hidden;
            will-change: box-shadow;
            transition: box-shadow .2s cubic-bezier(.4, 0, 1, 1), background-color .2s cubic-bezier(.4, 0, .2, 1), color .2s cubic-bezier(.4, 0, .2, 1);
            outline: 0;
            cursor: pointer;
            text-decoration: none;
            text-align: center;
            vertical-align: middle;
            border: 0;
            background: rgba(158, 158, 158, .2);
            box-shadow: 0 2px 2px 0 rgba(0, 0, 0, .14), 0 3px 1px -2px rgba(0, 0, 0, .2), 0 1px 5px 0 rgba(0, 0, 0, .12);
            color: #fff;
            background-color: #7EC0EE;
            text-shadow: 0
        }
</style>
	
<div class="btn_click_load" id="disqus_bt"> 
    <button class="disqus_click_btn">点击查看评论</button>
</div>

<!--
<script type="text/javascript">
$('.btn_click_load').click(function() {
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'http-miccall-tech'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    document.getElementById('disqus_bt').style.display = "none";
});
</script>
-->
<script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'http://yoursite.com/2017/07/16/机器学习/MachingLearning中的距离和相似性计算以及python实现/';  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'http://yoursite.com/2017/07/16/机器学习/MachingLearning中的距离和相似性计算以及python实现/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>

<script type="text/javascript">
    $('.btn_click_load').click(function() {  //click to load comments
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document;
            var s = d.createElement('script');
            s.src = '//http-miccall-tech.disqus.com/embed.js';
            s.setAttribute('data-timestamp', + new Date());
            (d.head || d.body).appendChild(s);
        })();
        $('.btn_click_load').css('display','none');
    });
</script>
</div>
<style>
    #disqus-comment{
        background-color: #eee;
        padding: 2pc;
    }
</style>


        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy;Powered By <a href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Design: <a href="http://miccall.tech " style="border-bottom: none;">miccall</a></li>
            </ul>
            
            	<span id="busuanzi_container_site_pv">2018总访问量<span id="busuanzi_value_site_pv"></span>次</span>
			
        </div>
    </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>



 	
</html>
