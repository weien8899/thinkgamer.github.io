<!DOCTYPE HTML>
<html>

<head>
	<!-- hexo-inject:begin --><!-- hexo-inject:end --><link rel="bookmark"  type="image/x-icon"  href="img/favicon.ico"/>
	<link rel="shortcut icon" href="img/favicon.ico">
	
			    <title>
    文艺与Code | Thinkgamer的博客
    </title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="/css/mic_main.css" />
    <link rel="stylesheet" href="/css/dropdownMenu.css" />
    <meta name="keywords" content="Thinkgamer Hadoop 数据挖掘 机器学习 深度学习 IT网站 博客" />
    
    	<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	 
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css" />
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('/img/bg.jpg') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

			    
  


    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
	
</head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_coy.css" />
<link rel="stylesheet" href="/css/typo.css" />
<!-- 文章页 -->
<body class="is-loading">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="/" class="logo">Thinkgamer</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special" >
            <ul class="menu links" >
			<!-- Homepage  主页  --> 
			<li >
	            <a href="/" rel="nofollow">主页</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <li class="active">
	            <a href="#s1">分类</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="category-link" href="/categories/大数据/">大数据</a></li><li><a class="category-link" href="/categories/机器学习/">机器学习</a></li><li><a class="category-link" href="/categories/编程与系统/">编程与系统</a></li><li><a class="category-link" href="/categories/这夏未眠/">这夏未眠</a></li><li><a class="category-link" href="/categories/随手记/">随手记</a>
	                    </ul>
	        </li>
	        
	        <!-- archives  归档   --> 
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/tag/" title="标签">
		                标签
		            </a>
		        </li>
		        
		        <li>
		            <a href="/about/" title="关于我">
		                关于我
		            </a>
		        </li>
		        
		        <li>
		            <a href="/cooperation/" title="商务合作">
		                商务合作
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
		            
		                <li><a href="https://github.com/thinkgamer" class="icon fa-github"><span class="label">GitHub</span></a></li>
		            
		            
		            
		            
			</ul>
</nav>

        <div id="main" >
            <div class ="post_page_title_img" style="height: 25rem;background-image: url(/images/thumbs/ml7.jpg);background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;" >
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2 >回归分析之线性回归（N元线性回归）</h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
                <p>在上一篇文章中我们介绍了 <a href="http://blog.csdn.net/gamer_gyt/article/details/78008144" target="_blank" rel="external"> 回归分析之理论篇</a>，在其中我们有聊到线性回归和非线性回归，包括广义线性回归，这一篇文章我们来聊下回归分析中的线性回归。</p>
<a id="more"></a>
<h1 id="一元线性回归"><a href="#一元线性回归" class="headerlink" title="一元线性回归"></a>一元线性回归</h1><p>预测房价：<br>输入编号    | 平方米    | 价格<br>-|-|-<br>1 |    150 |    6450<br>2    | 200    | 7450<br>3|    250    |8450<br>4|    300    |9450<br>5|    350    |11450<br>6|    400    |15450<br>7|    600|    18450</p>
<p>针对上边这种一元数据来讲，我们可以构建的一元线性回归函数为</p>
<script type="math/tex; mode=display">
H(x) = k*x + b</script><p>其中H(x)为平方米价格表，k是一元回归系数，b为常数。最小二乘法的公式：</p>
<script type="math/tex; mode=display">
k =\frac{ \sum_{1}^{n} (x_{i} - \bar{x} )(y_{i} - \bar{y}) } { \sum_{1}^{n}(x_{i}-\bar{x})^{2} }</script><p>自己使用python代码实现为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def leastsq(x,y):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    x,y分别是要拟合的数据的自变量列表和因变量列表</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    meanX = sum(x) * 1.0 / len(x)      # 求x的平均值</span><br><span class="line">    meanY = sum(y) * 1.0 / len(y)     # 求y的平均值</span><br><span class="line"></span><br><span class="line">    xSum = 0.0</span><br><span class="line">    ySum = 0.0</span><br><span class="line"></span><br><span class="line">    for i in range(len(x)):</span><br><span class="line">        xSum += (x[i] - meanX) * (y[i] - meanY)</span><br><span class="line">        ySum += (x[i] - meanX) ** 2</span><br><span class="line"></span><br><span class="line">    k = ySum/xSum</span><br><span class="line">    b = ySum - k * meanX</span><br><span class="line"></span><br><span class="line">    return k,b</span><br></pre></td></tr></table></figure></p>
<p>使用python的scipy包进行计算:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">leastsq(func, x0, args=(), Dfun=None, full_output=0, col_deriv=0, ftol=1.49012e-08, xtol=1.49012e-08, gtol=0.0, maxfev=0, epsfcn=None, factor=100, diag=None)</span><br><span class="line"></span><br><span class="line">from scipy.optimize import leastsq</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def fun(p, x):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    定义想要拟合的函数</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    k,b = p    # 从参数p获得拟合的参数</span><br><span class="line">    return k*x + b</span><br><span class="line"></span><br><span class="line">def err(p, x, y):</span><br><span class="line">    return fun(p,x) - y</span><br><span class="line"></span><br><span class="line">#定义起始的参数 即从 y = 1*x+1 开始，其实这个值可以随便设，只不过会影响到找到最优解的时间</span><br><span class="line">p0 = [1,1]</span><br><span class="line"></span><br><span class="line">#将list类型转换为 numpy.ndarray 类型，最初我直接使用</span><br><span class="line">#list 类型,结果 leastsq函数报错，后来在别的blog上看到了，原来要将类型转</span><br><span class="line">#换为numpy的类型</span><br><span class="line"></span><br><span class="line">x1 = np.array([150,200,250,300,350,400,600])</span><br><span class="line">y1 = np.array([6450,7450,8450,9450,11450,15450,18450])</span><br><span class="line"></span><br><span class="line">xishu = leastsq(err, p0, args=(x1,y1))</span><br><span class="line"></span><br><span class="line">print xishu[0]</span><br></pre></td></tr></table></figure></p>
<p>当然python的leastsq函数不仅仅局限于一元一次的应用，也可以应用到一元二次，二元二次，多元多次等，具体可以看下这篇博客：<a href="http://www.cnblogs.com/NanShan2016/p/5493429.html" target="_blank" rel="external">http://www.cnblogs.com/NanShan2016/p/5493429.html</a></p>
<h1 id="多元线性回归"><a href="#多元线性回归" class="headerlink" title="多元线性回归"></a>多元线性回归</h1><p>总之：我们可以用python leastsq函数解决几乎所有的线性回归的问题了，比如说</p>
<script type="math/tex; mode=display">y = a * x^2 + b * x + c</script><script type="math/tex; mode=display">y = a * x_1^2 + b * x_1 + c * x_2 + d</script><script type="math/tex; mode=display">y = a * x_1^3 + b * x_1^2 + c * x_1 + d</script><p>在使用时只需把参数列表和 fun 函数中的return 换一下，拿以下函数举例</p>
<script type="math/tex; mode=display">y = a * x_1^2 + b * x_1 + c * x_2 + d</script><p>对应的python 代码是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">from scipy.optimize import leastsq</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def fun(p, x1, x2):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    定义想要拟合的函数</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    a,b,c,d = p    # 从参数p获得拟合的参数</span><br><span class="line">    return a * (x1**2) + b * x1 + c * x2 + d</span><br><span class="line"></span><br><span class="line">def err(p, x1, x2, y):</span><br><span class="line">    return fun(p,x1,x2) - y</span><br><span class="line"></span><br><span class="line">#定义起始的参数 即从 y = 1*x+1 开始，其实这个值可以随便设，只不过会影响到找到最优解的时间</span><br><span class="line">p0 = [1,1,1,1]</span><br><span class="line"></span><br><span class="line">#将list类型转换为 numpy.ndarray 类型，最初我直接使用</span><br><span class="line">#list 类型,结果 leastsq函数报错，后来在别的blog上看到了，原来要将类型转</span><br><span class="line">#换为numpy的类型</span><br><span class="line"></span><br><span class="line">x1 = np.array([150,200,250,300,350,400,600])    # 面积</span><br><span class="line">x2 = np.array([4,2,7,9,12,14,15])               # 楼层</span><br><span class="line">y1 = np.array([6450,7450,8450,9450,11450,15450,18450])   # 价格/平方米</span><br><span class="line"></span><br><span class="line">xishu = leastsq(err, p0, args=(x1,x2,y1))</span><br><span class="line"></span><br><span class="line">print xishu[0]</span><br></pre></td></tr></table></figure></p>
<h1 id="sklearn中的线性回归应用"><a href="#sklearn中的线性回归应用" class="headerlink" title="sklearn中的线性回归应用"></a>sklearn中的线性回归应用</h1><h2 id="普通最小二乘回归"><a href="#普通最小二乘回归" class="headerlink" title="普通最小二乘回归"></a>普通最小二乘回归</h2><p>这里我们使用的是sklearn中的linear_model来模拟<script type="math/tex">y=a * x_1 + b * x_2 + c</script></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">In [1]: from sklearn.linear_model import LinearRegression</span><br><span class="line"></span><br><span class="line">In [2]: linreg = LinearRegression()</span><br><span class="line"></span><br><span class="line">In [3]: linreg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])</span><br><span class="line"></span><br><span class="line">In [4]: linreg.coef_</span><br><span class="line">Out[4]: array([ 0.5,  0.5])</span><br><span class="line"></span><br><span class="line">In [5]: linreg.intercept_</span><br><span class="line">Out[5]: 1.1102230246251565e-16</span><br><span class="line"></span><br><span class="line">In [6]: linreg.predict([4,4])</span><br><span class="line">Out[6]: array([ 4.])</span><br><span class="line"></span><br><span class="line">In [7]: zip([&quot;x1&quot;,&quot;x2&quot;], linreg.coef_)</span><br><span class="line">Out[7]: [(&apos;x1&apos;, 0.5), (&apos;x2&apos;, 0.49999999999999989)]</span><br></pre></td></tr></table></figure>
<p>所以可得<script type="math/tex">y = 0.5 * x_1 + 0.5 * x_2 + 1.11e-16</script></p>
<p>linreg.coef_  为系数 a,b</p>
<p>linreg.intercept_ 为截距 c</p>
<p>缺点：因为系数矩阵x与它的转置矩阵相乘得到的矩阵不能求逆，导致最小二乘法得到的回归系数不稳定，方差很大。</p>
<h2 id="多项式回归：基函数扩展线性模型"><a href="#多项式回归：基函数扩展线性模型" class="headerlink" title="多项式回归：基函数扩展线性模型"></a>多项式回归：基函数扩展线性模型</h2><p>机器学习中一种常见的模式是使用线性模型训练数据的非线性函数。这种方法保持了一般快速的线性方法的性能，同时允许它们适应更广泛的数据范围。</p>
<p>例如，可以通过构造系数的多项式特征来扩展一个简单的线性回归。在标准线性回归的情况下，你可能有一个类似于二维数据的模型：</p>
<script type="math/tex; mode=display">
y(w,x) = w_{0} + w_{1} x_{1} + w_{2} x_{2}</script><p>如果我们想把抛物面拟合成数据而不是平面，我们可以结合二阶多项式的特征，使模型看起来像这样:</p>
<script type="math/tex; mode=display">
y(w,x) = w_{0} + w_{1} x_{1} + w_{2} x_{2} + w_{3} x_{1}x_{2} + w_{4} x_{1}^2 + w_{5} x_{2}^2</script><p>我们发现，这仍然是一个线性模型，想象着创建一个新变量：</p>
<script type="math/tex; mode=display">
z = [x_{1},x_{2},x_{1} x_{2},x_{1}^2,x_{2}^2]</script><p>可以把线性回归模型写成下边这种形式：</p>
<script type="math/tex; mode=display">
y(w,x) = w_{0} + w_{1} z_{1} + w_{2} z_{2} + w_{3} z_{3} + w_{4} z_{4} + w_{5} z_{5}</script><p>我们看到，所得的多项式回归与我们上面所考虑的线性模型相同（即模型在W中是线性的），可以用同样的方法来求解。通过考虑在用这些基函数建立的高维空间中的线性拟合，该模型具有灵活性，可以适应更广泛的数据范围。</p>
<p>使用如下代码，将二维数据进行二元转换,转换规则为：</p>
<script type="math/tex; mode=display">
[x_1, x_2] => [1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">In [15]: from sklearn.preprocessing import PolynomialFeatures</span><br><span class="line"></span><br><span class="line">In [16]: import numpy as np</span><br><span class="line"></span><br><span class="line">In [17]: X = np.arange(6).reshape(3,2)</span><br><span class="line"></span><br><span class="line">In [18]: X</span><br><span class="line">Out[18]: </span><br><span class="line">array([[0, 1],</span><br><span class="line">       [2, 3],</span><br><span class="line">       [4, 5]])</span><br><span class="line"></span><br><span class="line">In [19]: poly = PolynomialFeatures(degree=2)</span><br><span class="line"></span><br><span class="line">In [20]: poly.fit_transform(X)</span><br><span class="line">Out[20]: </span><br><span class="line">array([[  1.,   0.,   1.,   0.,   0.,   1.],</span><br><span class="line">       [  1.,   2.,   3.,   4.,   6.,   9.],</span><br><span class="line">       [  1.,   4.,   5.,  16.,  20.,  25.]])</span><br></pre></td></tr></table></figure>
<p>验证：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">In [38]: from sklearn.preprocessing import PolynomialFeatures</span><br><span class="line"></span><br><span class="line">In [39]: from sklearn.linear_model import LinearRegression</span><br><span class="line"></span><br><span class="line">In [40]: from sklearn.pipeline import Pipeline</span><br><span class="line"></span><br><span class="line">In [41]: import numpy as np</span><br><span class="line"></span><br><span class="line">In [42]: </span><br><span class="line"></span><br><span class="line">In [42]: model = Pipeline( [ (&quot;poly&quot;,PolynomialFeatures(degree=3)),(&quot;linear&quot;,LinearRegression(fit_intercept=False)) ] )</span><br><span class="line"></span><br><span class="line">In [43]: model</span><br><span class="line">Out[43]: Pipeline(steps=[(&apos;poly&apos;, PolynomialFeatures(degree=3, include_bias=True, interaction_only=False)), (&apos;linear&apos;, LinearRegression(copy_X=True, fit_intercept=False, n_jobs=1, normalize=False))])</span><br><span class="line"></span><br><span class="line">In [44]: x = np.arange(5)</span><br><span class="line"></span><br><span class="line">In [45]: y = 3 - 2 * x + x ** 2 - x ** 3</span><br><span class="line"></span><br><span class="line">In [46]: y</span><br><span class="line">Out[46]: array([  3,   1,  -5, -21, -53])</span><br><span class="line"></span><br><span class="line">In [47]: model = model.fit(x[:,np.newaxis],y)</span><br><span class="line"></span><br><span class="line">In [48]: model.named_steps[&apos;linear&apos;].coef_</span><br><span class="line">Out[48]: array([ 3., -2.,  1., -1.])</span><br></pre></td></tr></table></figure></p>
<p>我们可以看出最后求出的参数和一元三次方程是一致的。</p>
<p>这里如果把degree改为2，y的方程也换一下，结果也是一致的<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">In [51]: from sklearn.linear_model import LinearRegression</span><br><span class="line"></span><br><span class="line">In [52]: from sklearn.preprocessing import PolynomialFeatures</span><br><span class="line"></span><br><span class="line">In [53]: from sklearn.pipeline import Pipeline</span><br><span class="line"></span><br><span class="line">In [54]: import numpy as np</span><br><span class="line"></span><br><span class="line">In [55]: model = Pipeline( [ (&quot;poly&quot;,PolynomialFeatures(degree=2)),(&quot;linear&quot;,LinearRegression(fit_intercept=False)) ] )</span><br><span class="line"></span><br><span class="line">In [56]: x = np.arange(5)</span><br><span class="line"></span><br><span class="line">In [57]: y = 3 + 2 * x + x ** 2</span><br><span class="line"></span><br><span class="line">In [58]: model = model.fit(x[:, np.newaxis], y)</span><br><span class="line"></span><br><span class="line">In [59]: model.named_steps[&apos;linear&apos;].coef_</span><br><span class="line">Out[59]: array([ 3., 2.,  1.])</span><br></pre></td></tr></table></figure></p>
<h2 id="线性回归的评测"><a href="#线性回归的评测" class="headerlink" title="线性回归的评测"></a>线性回归的评测</h2><p>在<a href="http://note.youdao.com/" target="_blank" rel="external">上一篇文章</a>中我们聊到了回归模型的评测方法，解下来我们详细聊聊如何来评价一个回归模型的好坏。</p>
<p>这里我们定义预测值和真实值分别为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">true = [10, 5, 3, 2]</span><br><span class="line">pred = [9, 5, 5, 3]</span><br></pre></td></tr></table></figure></p>
<p>1: 平均绝对误差（Mean Absolute Error, MAE）</p>
<script type="math/tex; mode=display">
\frac{1}{N}(\sum_{1}^{n} |y_i - \bar{y}|)</script><p>2: 均方误差（Mean Squared Error, MSE）</p>
<script type="math/tex; mode=display">
\frac{1}{N}\sum_{1}^{n}(y_i - \bar{y})^2</script><p>3: 均方根误差（Root Mean Squared Error, RMSE）</p>
<script type="math/tex; mode=display">
\frac{1}{N} \sqrt{ \sum_{1}^{n}(y_i - \bar{y})^2 }</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">In [80]: from sklearn import metrics</span><br><span class="line"></span><br><span class="line">In [81]: import numpy as np</span><br><span class="line"></span><br><span class="line">In [82]: true = [10, 5, 3, 2]</span><br><span class="line"></span><br><span class="line">In [83]: pred = [9, 5, 5, 3]</span><br><span class="line"></span><br><span class="line">In [84]: print(&quot;MAE: &quot;, metrics.mean_absolute_error(true,pred))</span><br><span class="line">(&apos;MAE: &apos;, 1.0)</span><br><span class="line"></span><br><span class="line">In [85]: print(&quot;MAE By Hand: &quot;, (1+0+2+1)/4.)</span><br><span class="line">(&apos;MAE By Hand: &apos;, 1.0)</span><br><span class="line"></span><br><span class="line">In [86]: print(&quot;MSE: &quot;, metrics.mean_squared_error(true,pred))</span><br><span class="line">(&apos;MSE: &apos;, 1.5)</span><br><span class="line"></span><br><span class="line">In [87]: print(&quot;MSE By Hand: &quot;, (1 ** 2 + 0 ** 2 + 2 ** 2 + 1 ** 2 ) / 4.)</span><br><span class="line">(&apos;MSE By Hand: &apos;, 1.5)</span><br><span class="line"></span><br><span class="line">In [88]: print(&quot;RMSE: &quot;, np.sqrt(metrics.mean_squared_error(true,pred)))</span><br><span class="line">(&apos;RMSE: &apos;, 1.2247448713915889)</span><br><span class="line"></span><br><span class="line">In [89]: print(&quot;RMSE By Hand: &quot;, np.sqrt((1 ** 2 + 0 ** 2 + 2 ** 2 + 1 ** 2 ) / 4.))</span><br><span class="line">(&apos;RMSE By Hand: &apos;, 1.2247448713915889)</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>线性回归在现实中还是可以解决很多问题的，但是并不是万能的，后续我会继续整理逻辑回归，岭回归等相关回归的知识，如果你感觉有用，欢迎分享！</p>
<hr>
<center>
    <img src="/img/gzh.jpg" weight="250px" height="250px">
    <br>
    打开微信扫一扫，关注微信公众号【数据与算法联盟】
    <br>
    <br>
    <img src="/img/weixin.png" weight="250px" height="300px">
    <br>
    打开微信扫一扫，加小编好友，拉你进数据算法交流群

</center>



            </div>

            <!-- Post Comments -->
            
    <!-- 使用 DISQUS_CLICK -->
<div id="disqus-comment">
    <div id="disqus_thread"></div>

<!-- add animation -->
<style>
	.disqus_click_btn {
            line-height: 30px;
            margin: 0;
            min-width: 50px;
            padding: 0 14px;
            display: inline-block;
            font-family: "Roboto", "Helvetica", "Arial", sans-serif;
            font-size: 14px;
            font-weight: 400;
            text-transform: uppercase;
            letter-spacing: 0;
            overflow: hidden;
            will-change: box-shadow;
            transition: box-shadow .2s cubic-bezier(.4, 0, 1, 1), background-color .2s cubic-bezier(.4, 0, .2, 1), color .2s cubic-bezier(.4, 0, .2, 1);
            outline: 0;
            cursor: pointer;
            text-decoration: none;
            text-align: center;
            vertical-align: middle;
            border: 0;
            background: rgba(158, 158, 158, .2);
            box-shadow: 0 2px 2px 0 rgba(0, 0, 0, .14), 0 3px 1px -2px rgba(0, 0, 0, .2), 0 1px 5px 0 rgba(0, 0, 0, .12);
            color: #fff;
            background-color: #7EC0EE;
            text-shadow: 0
        }
</style>
	
<div class="btn_click_load" id="disqus_bt"> 
    <button class="disqus_click_btn">点击查看评论</button>
</div>

<!--
<script type="text/javascript">
$('.btn_click_load').click(function() {
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'http-miccall-tech'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    document.getElementById('disqus_bt').style.display = "none";
});
</script>
-->
<script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'http://yoursite.com/2017/09/29/机器学习/回归分析之线性回归（N元线性回归）/';  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'http://yoursite.com/2017/09/29/机器学习/回归分析之线性回归（N元线性回归）/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>

<script type="text/javascript">
    $('.btn_click_load').click(function() {  //click to load comments
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document;
            var s = d.createElement('script');
            s.src = '//http-miccall-tech.disqus.com/embed.js';
            s.setAttribute('data-timestamp', + new Date());
            (d.head || d.body).appendChild(s);
        })();
        $('.btn_click_load').css('display','none');
    });
</script>
</div>
<style>
    #disqus-comment{
        background-color: #eee;
        padding: 2pc;
    }
</style>


        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy;Powered By <a href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Design: <a href="http://miccall.tech " style="border-bottom: none;">miccall</a></li>
            </ul>
            
            	<span id="busuanzi_container_site_pv">2018总访问量<span id="busuanzi_value_site_pv"></span>次</span>
			
        </div>
    </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>



 	
</html>
