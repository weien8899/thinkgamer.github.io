<!DOCTYPE HTML>
<html>

<head>
	<!-- hexo-inject:begin --><!-- hexo-inject:end --><link rel="bookmark"  type="image/x-icon"  href="img/favicon.ico"/>
	<link rel="shortcut icon" href="img/favicon.ico">
	
			    <title>
    文艺与Code | Thinkgamer的博客
    </title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="/css/mic_main.css" />
    <link rel="stylesheet" href="/css/dropdownMenu.css" />
    <meta name="keywords" content="Thinkgamer Hadoop 数据挖掘 机器学习 深度学习 IT网站 博客" />
    
    	<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	 
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css" />
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('/img/bg.jpg') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

			    
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script async type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
	
</head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_coy.css" />
<link rel="stylesheet" href="/css/typo.css" />
<!-- 文章页 -->
<body class="is-loading">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="/" class="logo">Thinkgamer</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special" >
            <ul class="menu links" >
			<!-- Homepage  主页  --> 
			<li >
	            <a href="/" rel="nofollow">主页</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <li class="active">
	            <a href="#s1">分类</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="category-link" href="/categories/大数据/">大数据</a></li><li><a class="category-link" href="/categories/机器学习/">机器学习</a></li><li><a class="category-link" href="/categories/编程与系统/">编程与系统</a></li><li><a class="category-link" href="/categories/这夏未眠/">这夏未眠</a></li><li><a class="category-link" href="/categories/随手记/">随手记</a>
	                    </ul>
	        </li>
	        
	        <!-- archives  归档   --> 
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/tag/" title="标签">
		                标签
		            </a>
		        </li>
		        
		        <li>
		            <a href="/about/" title="关于我">
		                关于我
		            </a>
		        </li>
		        
		        <li>
		            <a href="/cooperation/" title="商务合作">
		                商务合作
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
		            
		                <li><a href="https://github.com/thinkgamer" class="icon fa-github"><span class="label">GitHub</span></a></li>
		            
		            
		            
		            
			</ul>
</nav>

        <div id="main" >
            <div class ="post_page_title_img" style="height: 25rem;background-image: url(/images/thumbs/ml3.jpg);background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;" >
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2 >梯度算法之梯度上升和梯度下降</h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
                <p>第一次看见随机梯度上升算法是看《机器学习实战》这本书，当时也是一知半解，只是大概知道和高等数学中的函数求导有一定的关系。下边我们就好好研究下随机梯度上升（下降）和梯度上升（下降）。<br><a id="more"></a></p>
<h1 id="高数中的导数"><a href="#高数中的导数" class="headerlink" title="高数中的导数"></a>高数中的导数</h1><p>设导数 y = f(x) 在 $ x_0 $的某个邻域内有定义，当自变量从 $ x_0 $ 变成</p>
<script type="math/tex; mode=display">
x_{0} + \Delta x</script><p>函数y=f(x)的增量</p>
<script type="math/tex; mode=display">
\Delta y = f(x_0 + \Delta x) - f(x_0)</script><p>与自变量的增量 $ \Delta x $ 之比：</p>
<script type="math/tex; mode=display">
\frac{ \Delta y }{ \Delta x } = \frac{ f(x_0 + \Delta x)-f(x_0) }{ \Delta x }</script><p>称为f(x)的平均变化率。<br>如 $ \Delta x \rightarrow 0 $ 平均变化率的极限</p>
<script type="math/tex; mode=display">
\lim_{\Delta x \rightarrow 0} \frac{ \Delta y }{ \Delta x } = \lim_{\Delta x  \rightarrow 0} \frac{ f(x_0 + \Delta x)-f(x_0) }{ \Delta x }</script><p>存在，则称极限值为f(x)在$ x_0 $ 处的导数，并说f(x)在$ x_0 $ 处可导或有导数。当平均变化率极限不存在时，就说f(x)在 $ x_0 $ 处不可导或没有导数。</p>
<p>关于导数的说明</p>
<p>1）点导数是因变量在$ x_0 $ 处的变化率，它反映了因变量随自变量的变化而变化的快慢成都</p>
<p>2）如果函数y = f(x)在开区间 I 内的每点都可导，就称f(x)在开区间 I 内可导</p>
<p>3）对于任一 x 属于 I ，都对应着函数f(x)的一个导数，这个函数叫做原来函数f(x)的导函数</p>
<p>4）导函数在x1 处 为 0，若 x&lt;1 时，f’(x) &gt; 0 ，这 f(x) 递增，若f’(x)&lt;0 ，f(x)递减</p>
<p>5）f’(x0) 表示曲线y=f(x)在点 （x0,f($x_0$)）处的切线斜率</p>
<h1 id="偏导数"><a href="#偏导数" class="headerlink" title="偏导数"></a>偏导数</h1><p>函数z=f(x,y)在点(x0,y0)的某一邻域内有定义，当y固定在y0而x在 $x_0$ 处有增量$ \Delta x $ 时，相应的有函数增量</p>
<script type="math/tex; mode=display">
f(x_0 + \Delta x, y_0) - f(x_0,y_0)</script><p>如果</p>
<script type="math/tex; mode=display">
\lim_{\Delta x\rightarrow 0 } \frac {f(x_0 + \Delta x, y_0) - f(x_0,y_0)}{\Delta x}</script><p>存在，则称z=f(x,y)在点($x_0$,$y_0$)处对x的偏导数，记为：$ f_x(x_0,y_0) $</p>
<p>如果函数z=f(x,y)在区域D内任一点(x,y)处对x的偏导数都存在，那么这个偏导数就是x,y的函数，它就称为函数z=f(x,y)对自变量x的偏导数，记做</p>
<script type="math/tex; mode=display">
\frac{ \partial z }{ \partial x } , \frac{ \partial f }{ \partial x } , z_x , f_x(x,y),</script><p>偏导数的概念可以推广到二元以上的函数，如 u = f(x,y,z)在x,y,z处</p>
<script type="math/tex; mode=display">
f_x(x,y,z)=\lim_{\Delta x \rightarrow 0} \frac{f(x + \Delta x,y,z) -f(x,y,z)}{\Delta x}</script><script type="math/tex; mode=display">
f_y(x,y,z)=\lim_{\Delta y \rightarrow 0} \frac{f(x,y + \Delta y,z) -f(x,y,z)}{\Delta y}</script><script type="math/tex; mode=display">
f_z(x,y,z)=\lim_{\Delta z \rightarrow 0} \frac{f(x,y,z + \Delta z) -f(x,y,z)}{\Delta z}</script><p>可以看出导数与偏导数本质是一致的，都是自变量趋近于0时，函数值的变化与自变量的变化量比值的极限，直观的说，偏导数也就是函数在某一点沿坐标轴正方向的变化率。</p>
<p>区别：<br>导数指的是一元函数中，函数y=f(x)某一点沿x轴正方向的的变化率；<br>偏导数指的是多元函数中，函数y=f(x,y,z)在某一点沿某一坐标轴正方向的变化率。</p>
<p>偏导数的几何意义：<br>偏导数$ z = f_x(x_0,y_0)$表示的是曲面被 $ y=y_0 $ 所截得的曲线在点M处的切线$ M_0T_x $对x轴的斜率<br>偏导数$ z = f_y(x_0,y_0)$表示的是曲面被 $ x=x_0 $ 所截得的曲线在点M处的切线$ M_0T_y $对y轴的斜率</p>
<p>例子：<br>求 $z = x^2 + 3 xy+y^2 $在点(1,2)处的偏导数。</p>
<script type="math/tex; mode=display">
\frac{ \partial z}{\partial x} = 2x +3y</script><script type="math/tex; mode=display">
\frac{ \partial z}{\partial y} = 2y +3x</script><p>所以:<br>$z_x(x=1,y=2) = 8$<br>$z_y(x=1,y=2) = 7$</p>
<h1 id="方向导数"><a href="#方向导数" class="headerlink" title="方向导数"></a>方向导数</h1><script type="math/tex; mode=display">
\frac{ \partial }{ \partial l }  f(x_0,x_1,...,x_n) = \lim_{\rho \rightarrow 0} \frac{\Delta y}{ \Delta x } = \lim_{\rho \rightarrow 0} \frac{ f(x_0 + \Delta x_0,...,x_j + \Delta x_j,...,x_n + \Delta x_n)-f(x_0,...,x_j,...,x_n)}{ \rho }</script><script type="math/tex; mode=display">
\rho = \sqrt{ (\Delta x_0)^{2} +...+(\Delta x_j)^{2}+...+(\Delta x_n)^{2}}</script><p>前边导数和偏导数的定义中，均是沿坐标轴正方向讨论函数的变化率。那么当讨论函数沿任意方向的变化率时，也就引出了方向导数的定义，即：某一点在某一趋近方向上的导数值。</p>
<p>通俗的解释是： 我们不仅要知道函数在坐标轴正方向上的变化率（即偏导数），而且还要设法求得函数在其他特定方向上的变化率。而方向导数就是函数在其他特定方向上的变化率。 
　</p>
<h1 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h1><p>与方向导数有一定的关联，在微积分里面，对多元函数的参数求 $ \partial  $ 偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数f(x,y), 分别对x,y求偏导数，求得的梯度向量就是 $<br>( \frac{ \partial f }{ \partial x },\frac{ \partial f }{ \partial y })^T<br>$ ,简称grad f(x,y)或者 $▽f(x,y)$。对于在点$(x_0,y_0)$的具体梯度向量就是$( \frac{ \partial f }{ \partial x_0 },\frac{ \partial f }{ \partial y_0 })^T$.或者$▽f(x_0,y_0)$，如果是3个参数的向量梯度，就是 $( \frac{ \partial f }{ \partial x },\frac{ \partial f }{ \partial y },\frac{ \partial f }{ \partial z })^T$,以此类推。</p>
<p>那么这个梯度向量求出来有什么意义呢？他的意义从几何意义上讲，就是函数变化增加最快的地方。具体来说，对于函数f(x,y),在点$(x_0,y_0)$，沿着梯度向量的方向就是$( \frac{ \partial f }{ \partial x_0 },\frac{ \partial f }{ \partial y_0 })^T$的方向是f(x,y)增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，也就是 $-( \frac{ \partial f }{ \partial x_0 },\frac{ \partial f }{ \partial y_0 })^T$的方向，梯度减少最快，也就是更加容易找到函数的最小值。</p>
<p>例如：<br>函数 $f(x,y) = \frac{1}{x^2+y^2} $ ，分别对x，y求偏导数得：</p>
<script type="math/tex; mode=display">
 \frac{ \partial f }{ \partial x}=-\frac{2x}{ (x^2+y^2)^2}</script><script type="math/tex; mode=display">
 \frac{ \partial f }{ \partial y}=-\frac{2y}{ (x^2+y^2)^2}</script><p>所以</p>
<script type="math/tex; mode=display">
grad( \frac{1}{x^2+y^2} ) = (-\frac{2x}{ (x^2+y^2)^2} ,-\frac{2y}{ (x^2+y^2)^2})</script><p>函数在某一点的梯度是这样一个向量，它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值。 </p>
<p>注意点：<br>1）梯度是一个向量<br>2）梯度的方向是最大方向导数的方向<br>3）梯度的值是最大方向导数的值</p>
<h1 id="梯度下降与梯度上升"><a href="#梯度下降与梯度上升" class="headerlink" title="梯度下降与梯度上升"></a>梯度下降与梯度上升</h1><p>在机器学习算法中，在最小化损失函数时，可以通过梯度下降思想来求得最小化的损失函数和对应的参数值，反过来，如果要求最大化的损失函数，可以通过梯度上升思想来求取。</p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><h3 id="关于梯度下降的几个概念"><a href="#关于梯度下降的几个概念" class="headerlink" title="关于梯度下降的几个概念"></a>关于梯度下降的几个概念</h3><p>1）步长（learning rate）：步长决定了在梯度下降迭代过程中，每一步沿梯度负方向前进的长度<br>2）特征（feature）：指的是样本中输入部门，比如样本（x0，y0），（x1，y1），则样本特征为x，样本输出为y<br>3）假设函数（hypothesis function）：在监督学习中，为了拟合输入样本，而使用的假设函数，记为$h_θ(x)$。比如对于样本$（x_i,y_i）(i=1,2,…n)$,可以采用拟合函数如下： $h_θ(x) = θ0+θ1_x$。<br>4）损失函数（loss function）：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。在线性回归中，损失函数通常为样本输出和假设函数的差取平方。比如对于样本（xi,yi）(i=1,2,…n),采用线性回归，损失函数为：</p>
<script type="math/tex; mode=display">
\jmath (\theta _0,\theta _1)=\sum_{i=0}^{m}( h_\theta(x_i)-y_i )^2</script><p>其中$x_i$表示样本特征x的第i个元素，$y_i$表示样本输出y的第i个元素，$h_\theta(x_i)$ 为假设函数。</p>
<h3 id="梯度下降的代数方法描述"><a href="#梯度下降的代数方法描述" class="headerlink" title="梯度下降的代数方法描述"></a>梯度下降的代数方法描述</h3><ol>
<li><p>先决条件：确定优化模型的假设函数和损失函数<br>这里假定线性回归的假设函数为$h_\theta(x_1,x_2,…x_n)=\theta_0+\theta_1x_1+…+\theta_nx_n$，其中 $\theta _i(i=0,1,2…n)$ 为模型参数(公式中用$\theta$代替)，$x_i(i=0,1,2…n)$为每个样本的n个特征值。</p>
<p>则对应选定得损失函数为：</p>
<script type="math/tex; mode=display">
\jmath (\theta _0,\theta _1,...,,\theta _n)=\sum_{i=0}^{m}( h_\theta(x_0,x_1,...,x_n)-y_i )^2</script></li>
<li><p>算法相关参数的初始化<br>主要是初始化 $ \theta _0,\theta _1…,\theta _n$，算法终止距离 $\varepsilon $ 以及步长 $ \alpha $。在没有任何先验知识的时候，我喜欢将所有的 $\theta$ 初始化为0， 将步长初始化为1。在调优的时候再优化。</p>
</li>
<li><p>算法过程</p>
</li>
</ol>
<ul>
<li><p>1)：确定当前损失函数的梯度，对于$\theta _i $，其梯度表达式为：</p>
<script type="math/tex; mode=display">
\frac{\partial }{\partial \theta _i}\jmath (\theta _1,\theta _2,...,\theta _n)</script></li>
<li><p>2)：用步长乘以损失函数的梯度，得到当前位置的下降距离，即</p>
<script type="math/tex; mode=display">
\alpha \frac{\partial \jmath (\theta _1,\theta _2,...,\theta _n)}{\partial \theta _i}</script></li>
<li><p>3)：确定是否所有的$\theta _i$ ，梯度下降的距离都小于 $ \varepsilon $，如果小于$ \varepsilon $，则算法停止，当前所有的 $\theta _i(i=1,2,3,…,n)$ 即为最终结果。否则执行下一步。</p>
</li>
<li><p>4)：更新所有的 $\theta$，对于$\theta _i $，其更新表达式如下。更新完毕后继续转入步骤1)。</p>
<script type="math/tex; mode=display">
\theta _i = \theta _i - \alpha \frac{\partial \jmath (\theta _1,\theta _2,...,\theta _n)}{\partial \theta _i}</script><h3 id="梯度下降的矩阵方式描述"><a href="#梯度下降的矩阵方式描述" class="headerlink" title="梯度下降的矩阵方式描述"></a>梯度下降的矩阵方式描述</h3><ol>
<li>先决条件：确定优化模型的假设函数和损失函数<br>这里假定线性回归的假设函数为$h_\theta(x_1,x_2,…x_n)=\theta_0+\theta_1x_1+…+\theta_nx_n$，其中 $\theta _i(i=0,1,2…n)$ 为模型参数，$x_i(i=0,1,2…n)$为每个样本的n个特征值。<br>假设函数对应的矩阵表示为：$ h_\theta (x) = X \theta $，假设函数 $h_\theta(x)$ 为mx1的向量，$\theta $ 为nx1的向量，里面有n个代数法的模型参数。X为mxn维的矩阵。m代表样本的个数，n代表样本的特征数。<br>则对应选定得损失函数为：<script type="math/tex; mode=display">
\jmath (\theta)=(X \theta −Y)^T (X \theta−Y)</script>其中YY是样本的输出向量，维度为m*1<br><br><br>2.算法相关参数初始化:<br>$\theta$ 向量可以初始化为默认值，或者调优后的值。算法终止距离 $\varepsilon $ ，步长 $\alpha$ 和 “梯度下降的代数方法”描述中一致。<br><br><br>3.算法过程</li>
</ol>
</li>
<li><p>1)：确定当前位置的损失函数的梯度，对于 $ \theta $ 向量,其梯度表达式如下：</p>
<script type="math/tex; mode=display">
\frac{ \partial }{\partial \theta } \jmath (\theta)</script></li>
<li>2)：用步长乘以损失函数的梯度，得到当前位置下降的距离，即 $\alpha \frac{ \partial }{\partial \theta } \jmath (\theta)$ </li>
<li>3)：确定 $\theta$ 向量里面的每个值,梯度下降的距离都小于 $\varepsilon$，如果小于 $\varepsilon$ 则算法终止，当前 $\theta$ 向量即为最终结果。否则进入步骤4)</li>
<li>4)：更新 $\theta$ 向量，其更新表达式如下。更新完毕后继续转入步骤1)<script type="math/tex; mode=display">
\theta =\theta - \alpha \frac{ \partial }{\partial \theta } \jmath (\theta)</script></li>
</ul>
<h2 id="梯度上升"><a href="#梯度上升" class="headerlink" title="梯度上升"></a>梯度上升</h2><p>梯度上升和梯度下降的分析方式是一致的，只不过把 $ \theta $ 的更新中 减号变为加号。</p>
<h2 id="梯度下降的算法优化"><a href="#梯度下降的算法优化" class="headerlink" title="梯度下降的算法优化"></a>梯度下降的算法优化</h2><ol>
<li><p>算法的步长选择。在前面的算法描述中，我提到取步长为1，但是实际上取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。前面说了。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。</p>
</li>
<li><p>算法参数的初始值选择。 初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。</p>
</li>
</ol>
<p>3.归一化。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化，也就是对于每个特征x，求出它的均值 $\bar{x}$ 和标准差std(x)，然后转化为：</p>
<script type="math/tex; mode=display">
\frac{x - \bar{x}}{std(x)}</script><p>这样特征的新期望为0，新方差为1，迭代次数可以大大加快。</p>
<hr>
<p><a href="http://blog.csdn.net/walilk/article/details/50978864" target="_blank" rel="external">http://blog.csdn.net/walilk/article/details/50978864</a></p>
<p><a href="https://www.zhihu.com/question/24658302" target="_blank" rel="external">https://www.zhihu.com/question/24658302</a></p>
<p><a href="https://www.cnblogs.com/pinard/p/5970503.html" target="_blank" rel="external">https://www.cnblogs.com/pinard/p/5970503.html</a></p>
<p><a href="http://www.doc88.com/p-7844239247737.html" target="_blank" rel="external">http://www.doc88.com/p-7844239247737.html</a></p>
<hr>
<center>
    <img src="/img/gzh.jpg" weight="250px" height="250px">
    <br>
打开微信扫一扫，关注微信公众号【数据与算法联盟】
</center>
            </div>

            <!-- Post Comments -->
            
    <!-- 使用 DISQUS_CLICK -->
<div id="disqus-comment">
    <div id="disqus_thread"></div>

<!-- add animation -->
<style>
	.disqus_click_btn {
            line-height: 30px;
            margin: 0;
            min-width: 50px;
            padding: 0 14px;
            display: inline-block;
            font-family: "Roboto", "Helvetica", "Arial", sans-serif;
            font-size: 14px;
            font-weight: 400;
            text-transform: uppercase;
            letter-spacing: 0;
            overflow: hidden;
            will-change: box-shadow;
            transition: box-shadow .2s cubic-bezier(.4, 0, 1, 1), background-color .2s cubic-bezier(.4, 0, .2, 1), color .2s cubic-bezier(.4, 0, .2, 1);
            outline: 0;
            cursor: pointer;
            text-decoration: none;
            text-align: center;
            vertical-align: middle;
            border: 0;
            background: rgba(158, 158, 158, .2);
            box-shadow: 0 2px 2px 0 rgba(0, 0, 0, .14), 0 3px 1px -2px rgba(0, 0, 0, .2), 0 1px 5px 0 rgba(0, 0, 0, .12);
            color: #fff;
            background-color: #7EC0EE;
            text-shadow: 0
        }
</style>
	
<div class="btn_click_load" id="disqus_bt"> 
    <button class="disqus_click_btn">点击查看评论</button>
</div>

<!--
<script type="text/javascript">
$('.btn_click_load').click(function() {
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'http-miccall-tech'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    document.getElementById('disqus_bt').style.display = "none";
});
</script>
-->
<script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'http://yoursite.com/2017/12/14/机器学习/梯度算法之梯度上升和梯度下降/';  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'http://yoursite.com/2017/12/14/机器学习/梯度算法之梯度上升和梯度下降/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>

<script type="text/javascript">
    $('.btn_click_load').click(function() {  //click to load comments
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document;
            var s = d.createElement('script');
            s.src = '//http-miccall-tech.disqus.com/embed.js';
            s.setAttribute('data-timestamp', + new Date());
            (d.head || d.body).appendChild(s);
        })();
        $('.btn_click_load').css('display','none');
    });
</script>
</div>
<style>
    #disqus-comment{
        background-color: #eee;
        padding: 2pc;
    }
</style>


        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy;Powered By <a href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Design: <a href="http://miccall.tech " style="border-bottom: none;">miccall</a></li>
            </ul>
            
            	<span id="busuanzi_container_site_pv">2018总访问量<span id="busuanzi_value_site_pv"></span>次</span>
			
        </div>
    </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>



 	
</html>
