<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>文艺与Code | Thinkgamer的博客</title>
  
  <subtitle>机器学习／python／云计算</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-03-17T17:29:55.935Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Thinkgamer</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Scala解析XML</title>
    <link href="http://yoursite.com/2018/02/04/Spark/Scala%E8%A7%A3%E6%9E%90XML/"/>
    <id>http://yoursite.com/2018/02/04/Spark/Scala解析XML/</id>
    <published>2018-02-04T08:32:45.000Z</published>
    <updated>2018-03-17T17:29:55.935Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>在使用Spark时，有时候主函数入口参数过多的时候，会特别复杂，这个时候我们可以将相应的参数写在xml文件中，然后只要将xml文件的路径传进去即可，这里的xml路径可以是本地的，也可以是hdfs上的。</p></blockquote><a id="more"></a><p>scala提供了类似于Xpath的语法来解析xml文件，其中很重要的两个操作符是”\”<br>和 “\\”</p><ul><li>\ ：根据搜索条件得到下一个节点</li><li>\\：根据条件获取所有的节点</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;configure&gt;</span><br><span class="line">    &lt;input&gt;</span><br><span class="line">        &lt;name&gt;app_feature_goods&lt;/name&gt;</span><br><span class="line">        &lt;hdfs&gt;/user/path/to/goods&lt;/hdfs&gt;</span><br><span class="line">    &lt;/input&gt;</span><br><span class="line">    &lt;input&gt;</span><br><span class="line">        &lt;name&gt;app_feature_user&lt;/name&gt;</span><br><span class="line">        &lt;hdfs&gt;/user/path/to/user&lt;/hdfs&gt;</span><br><span class="line">    &lt;/input&gt;</span><br><span class="line">&lt;/configure&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">val input = args(0)</span><br><span class="line">val xml = XML.load(input)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 找到所有的一级节点 input</span><br><span class="line">val input_list = xml\&quot;input&quot;</span><br><span class="line">input_list.foreach(println)</span><br><span class="line"></span><br><span class="line">// 遍历每个一级节点，得到具体的值</span><br><span class="line">for(one &lt;- input_list)&#123;</span><br><span class="line">    println(one\&quot;name&quot;)</span><br><span class="line">    println((one\&quot;name&quot;).text)</span><br><span class="line">    println(one\&quot;hdfs&quot;)</span><br><span class="line">    println((one\&quot;hdfs&quot;).text)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 得到所有的name</span><br><span class="line">val name_list = xml\\&quot;name&quot;</span><br><span class="line">name_list.map(one =&gt; one.text).foreach(println)</span><br><span class="line"></span><br><span class="line">// 获取所有hdfs</span><br><span class="line">val hdfs_list = xml\\&quot;hdfs&quot;</span><br><span class="line">hdfs_list.map(one =&gt; one.text).foreach(println)</span><br><span class="line"></span><br><span class="line">// 获取具有class的值</span><br><span class="line">println(xml\&quot;input&quot;\&quot;name&quot;\\&quot;@class&quot;)</span><br><span class="line"></span><br><span class="line">// 打印出具有class属性的name值和hdfs值</span><br><span class="line">println((xml\\&quot;name&quot;).filter(_.attribute(&quot;class&quot;).exists(_.text.equals(&quot;test&quot;))).text)</span><br><span class="line">println((xml\\&quot;hdfs&quot;).filter(_.attribute(&quot;class&quot;).exists(_.text.equals(&quot;test&quot;))).text)</span><br></pre></td></tr></table></figure><p>打印出的信息为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">&lt;input&gt;</span><br><span class="line">        &lt;name&gt;app_feature_goods&lt;/name&gt;</span><br><span class="line">        &lt;hdfs&gt;/user/path/to/goods&lt;/hdfs&gt;</span><br><span class="line">    &lt;/input&gt;</span><br><span class="line">&lt;input&gt;</span><br><span class="line">        &lt;name&gt;app_feature_user&lt;/name&gt;</span><br><span class="line">        &lt;hdfs&gt;/user/path/to/user&lt;/hdfs&gt;</span><br><span class="line">    &lt;/input&gt;</span><br><span class="line">&lt;input&gt;</span><br><span class="line">        &lt;name class=&quot;test&quot;&gt;app_feature_user_test&lt;/name&gt;</span><br><span class="line">        &lt;hdfs class=&quot;test&quot;&gt;/user/path/to/user_test&lt;/hdfs&gt;</span><br><span class="line">    &lt;/input&gt;</span><br><span class="line">-------------</span><br><span class="line">&lt;name&gt;app_feature_goods&lt;/name&gt;</span><br><span class="line">app_feature_goods</span><br><span class="line">&lt;hdfs&gt;/user/path/to/goods&lt;/hdfs&gt;</span><br><span class="line">/user/path/to/goods</span><br><span class="line">&lt;name&gt;app_feature_user&lt;/name&gt;</span><br><span class="line">app_feature_user</span><br><span class="line">&lt;hdfs&gt;/user/path/to/user&lt;/hdfs&gt;</span><br><span class="line">/user/path/to/user</span><br><span class="line">&lt;name class=&quot;test&quot;&gt;app_feature_user_test&lt;/name&gt;</span><br><span class="line">app_feature_user_test</span><br><span class="line">&lt;hdfs class=&quot;test&quot;&gt;/user/path/to/user_test&lt;/hdfs&gt;</span><br><span class="line">/user/path/to/user_test</span><br><span class="line">-------------</span><br><span class="line">app_feature_goods</span><br><span class="line">app_feature_user</span><br><span class="line">app_feature_user_test</span><br><span class="line">-------------</span><br><span class="line">/user/path/to/goods</span><br><span class="line">/user/path/to/user</span><br><span class="line">/user/path/to/user_test</span><br><span class="line">-------------</span><br><span class="line">test</span><br><span class="line">-------------</span><br><span class="line">app_feature_user_test</span><br><span class="line">/user/path/to/user_test</span><br><span class="line">-------------</span><br><span class="line"></span><br><span class="line">Process finished with exit code 0</span><br></pre></td></tr></table></figure></p><p>当然还存在一种情况就是XML文件存在于hdfs之上，这时候就不能直接load xml文件里，不过可以通过下面一种方法获得<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">var rdd = sc.textFile(xml_path)</span><br><span class="line">val xml = XML.loadString(rdd.collect().mkString(&quot;\n&quot;))</span><br></pre></td></tr></table></figure></p><p>接下来便可以通过上边的方法进行解析了。</p><center>    <img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">    <br>打开微信扫一扫，关注公众号【数据与算法联盟】</center>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;在使用Spark时，有时候主函数入口参数过多的时候，会特别复杂，这个时候我们可以将相应的参数写在xml文件中，然后只要将xml文件的路径传进去即可，这里的xml路径可以是本地的，也可以是hdfs上的。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark求统计量的两种方法</title>
    <link href="http://yoursite.com/2018/02/04/Spark/Spark%E6%B1%82%E7%BB%9F%E8%AE%A1%E9%87%8F%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95/"/>
    <id>http://yoursite.com/2018/02/04/Spark/Spark求统计量的两种方法/</id>
    <published>2018-02-04T07:04:27.000Z</published>
    <updated>2018-03-17T17:32:07.879Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Spark对于统计量中的最大值，最小值，平均值和方差（均值）的计算都提供了封装，这里小编知道两种计算方法，整理一下分享给大家</p></blockquote><a id="more"></a><h1 id="DataFrame形式"><a href="#DataFrame形式" class="headerlink" title="DataFrame形式"></a>DataFrame形式</h1><h2 id="加载Json数据源"><a href="#加载Json数据源" class="headerlink" title="加载Json数据源"></a>加载Json数据源</h2><p>example.json文件格式如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;name&quot;:&quot;thinkgamer&quot;,&quot;age&quot;:23,&quot;math&quot;:78,&quot;chinese&quot;:78,&quot;english&quot;:95&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;think&quot;,&quot;age&quot;:25,&quot;math&quot;:95,&quot;chinese&quot;:88,&quot;english&quot;:93&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;gamer&quot;,&quot;age&quot;:24,&quot;math&quot;:93,&quot;chinese&quot;:68,&quot;english&quot;:88&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// persist(StorageLevel.MEMORY_AND_DISK) 当内存不够时cache到磁盘里</span><br><span class="line">val df = spark.read.json(&quot;/path/to/example.json&quot;).persist(StorageLevel.MEMORY_AND_DISK)</span><br><span class="line">df.show()</span><br><span class="line">df.describe()</span><br></pre></td></tr></table></figure><p>我们便可以看到如下的形式<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">+---+-------+-------+----+----------+</span><br><span class="line">|age|chinese|english|math|      name|</span><br><span class="line">+---+-------+-------+----+----------+</span><br><span class="line">| 23|     78|     95|  78|thinkgamer|</span><br><span class="line">| 25|     88|     93|  95|     think|</span><br><span class="line">| 24|     68|     88|  93|     gamer|</span><br><span class="line">+---+-------+-------+----+----------+</span><br><span class="line"></span><br><span class="line">+-------+----+-------+-----------------+-----------------+----------+</span><br><span class="line">|summary| age|chinese|          english|             math|      name|</span><br><span class="line">+-------+----+-------+-----------------+-----------------+----------+</span><br><span class="line">|  count|   3|      3|                3|                3|         3|</span><br><span class="line">|   mean|24.0|   78.0|             92.0|88.66666666666667|      null|</span><br><span class="line">| stddev| 1.0|   10.0|3.605551275463989| 9.29157324317757|      null|</span><br><span class="line">|    min|  23|     68|               88|               78|     gamer|</span><br><span class="line">|    max|  25|     88|               95|               95|thinkgamer|</span><br><span class="line">+-------+----+-------+-----------------+-----------------+----------+</span><br></pre></td></tr></table></figure></p><p>如果是想看某列的通知值的话，可以用下面的方式<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select(&quot;age&quot;).describe().show()</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">+-------+----+</span><br><span class="line">|summary| age|</span><br><span class="line">+-------+----+</span><br><span class="line">|  count|   3|</span><br><span class="line">|   mean|24.0|</span><br><span class="line">| stddev| 1.0|</span><br><span class="line">|    min|  23|</span><br><span class="line">|    max|  25|</span><br><span class="line">+-------+----+</span><br></pre></td></tr></table></figure><h1 id="RDD形式"><a href="#RDD形式" class="headerlink" title="RDD形式"></a>RDD形式</h1><p>假设同样还是上边的数据，只不过现在变成按\t分割的普通文本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">thinkgamer  23  78  78  95</span><br><span class="line">think   25  95  88  93</span><br><span class="line">gamer   24  93  68  88</span><br></pre></td></tr></table></figure></p><p>这里可以将rdd转换成dataframe洗形式，也可以使用rdd计算，转化为df的样例如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val new_data = data_txt</span><br><span class="line">      .map(_.split(&quot;\\s+&quot;))</span><br><span class="line">      .map(one =&gt; Person(one(0),one(1).toInt,one(2).toDouble,one(3).toDouble,one(4).toDouble))</span><br><span class="line">      .toDF()</span><br></pre></td></tr></table></figure></p><p>接下来就是进行和上边df一样的操作了。</p><p>那么对于rdd形式的文件如何操作：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.mllib.linalg.Vectors</span><br><span class="line">import org.apache.spark.mllib.stat.&#123;MultivariateStatisticalSummary, Statistics&#125;</span><br><span class="line"></span><br><span class="line">val data_txt = SparkSC.spark.sparkContext.textFile(input_txt).persist(StorageLevel.MEMORY_AND_DISK)</span><br><span class="line">    val new_data = data_txt</span><br><span class="line">      .map(_.split(&quot;\\s+&quot;))</span><br><span class="line">      .map(one =&gt; Vectors.dense(one(1).toInt,one(2).toDouble,one(3).toDouble,one(4).toDouble))</span><br><span class="line">    val summary: MultivariateStatisticalSummary = Statistics.colStats(new_data)</span><br><span class="line">    </span><br><span class="line">println(&quot;Max:&quot;+summary.max)</span><br><span class="line">println(&quot;Min:&quot;+summary.min)</span><br><span class="line">println(&quot;Count:&quot;+summary.count)</span><br><span class="line">println(&quot;Variance:&quot;+summary.variance)</span><br><span class="line">println(&quot;Mean:&quot;+summary.mean)</span><br><span class="line">println(&quot;NormL1:&quot;+summary.normL1)</span><br><span class="line">println(&quot;Norml2:&quot;+summary.normL2)</span><br></pre></td></tr></table></figure></p><p>输出结果为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Max:[25.0,95.0,88.0,95.0]</span><br><span class="line">Min:[23.0,78.0,68.0,88.0]</span><br><span class="line">Count:3</span><br><span class="line">Variance:[1.0,86.33333333333331,100.0,13.0]</span><br><span class="line">Mean:[24.0,88.66666666666667,78.0,92.0]</span><br><span class="line">NormL1:[72.0,266.0,234.0,276.0]</span><br><span class="line">Norml2:[41.593268686170845,154.1363033162532,135.83813897429545,159.43023552638942]</span><br></pre></td></tr></table></figure></p><p>这里可以得到相关的统计信息，主要区别在于dataframe得到的是标准差，而使用mllib得到的统计值中是方差，但这并不矛盾，两者可以相互转化得到。</p><p>当然如果要求四分位数，可以转化成df，使用sql语句进行查询<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Select PERCENTILE(col,&lt;0.25,0.75&gt;) from tableName;</span><br></pre></td></tr></table></figure></p><h1 id="自己实现"><a href="#自己实现" class="headerlink" title="自己实现"></a>自己实现</h1><p>下面是我自己实现的一个方法，传入的参数是一个rdd，返回的是一个字符串<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">// 计算最大值，最小值，平均值，方差，标准差，四分位数</span><br><span class="line">def getStat(data: RDD[String]):String= &#123;</span><br><span class="line">   val sort_data = data</span><br><span class="line">      .filter(one =&gt; Verify.istoDouble(one))</span><br><span class="line">      .map(_.toDouble)</span><br><span class="line">      .sortBy(line=&gt;line)</span><br><span class="line">      .persist(StorageLevel.MEMORY_AND_DISK)  // 默认是true 升序，false为降序</span><br><span class="line"></span><br><span class="line">   val data_list = sort_data.collect()</span><br><span class="line">   val len = data_list.length</span><br><span class="line">   val min = data_list(0)</span><br><span class="line">   val max = data_list(len-1)</span><br><span class="line">   val mean = sort_data.reduce((a,b) =&gt; a+b) / len</span><br><span class="line">   val variance = sort_data.map(one =&gt; math.pow(one-mean,2)).reduce((a,b)=&gt;a+b)/len</span><br><span class="line">   val stdder = math.sqrt(variance)</span><br><span class="line">   var quant = &quot;&quot;</span><br><span class="line">   if(len&lt;4)&#123;</span><br><span class="line">      val q1 = min</span><br><span class="line">      val q2 = min</span><br><span class="line">      val q3 = max</span><br><span class="line">      quant = q1+&quot;\t&quot;+q2+&quot;\t&quot;+q3</span><br><span class="line">   &#125;else &#123;</span><br><span class="line">      val q1 = data_list((len * 0.25).toInt - 1)</span><br><span class="line">      val q2 = data_list((len * 0.5).toInt - 1)</span><br><span class="line">      val q3 = data_list((len * 0.75).toInt - 1)</span><br><span class="line">      quant = q1+&quot;\t&quot;+q2+&quot;\t&quot;+q3</span><br><span class="line">   &#125;</span><br><span class="line">   max+&quot;\t&quot;+min+&quot;\t&quot;+mean+&quot;\t&quot;+variance+&quot;\t&quot;+stdder+&quot;\t&quot;+quant</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="本地碰见的一个错误"><a href="#本地碰见的一个错误" class="headerlink" title="本地碰见的一个错误"></a>本地碰见的一个错误</h1><p>1：错误1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala.Predef$.refArrayOps([Ljava/lang/Object;)Lscala/collection/mutable/Array</span><br></pre></td></tr></table></figure></p><p>原因是Spark中spark-sql_2.11-2.2.1 ，是用scala 2.11版本上编译的，而我的本地的scala版本为2.12.4，所以就错了，可以在<br>里边把相应的scala版本就行修改就行了</p><p>2：错误2<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NoSuchMethodError: scala.Product.$init$(Lscala/Product;)V</span><br></pre></td></tr></table></figure></p><p>原因也是因为我下载安装的scala2.12版本，换成scala2.11版本就可以了</p><hr><center>    <img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">    <br>打开微信扫一扫，关注公众号【数据与算法联盟】</center>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;Spark对于统计量中的最大值，最小值，平均值和方差（均值）的计算都提供了封装，这里小编知道两种计算方法，整理一下分享给大家&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>记一次百G数据的聚类算法实施过程</title>
    <link href="http://yoursite.com/2018/01/27/Spark/%E8%AE%B0%E4%B8%80%E6%AC%A1%E7%99%BEG%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AE%9E%E6%96%BD%E8%BF%87%E7%A8%8B/"/>
    <id>http://yoursite.com/2018/01/27/Spark/记一次百G数据的聚类算法实施过程/</id>
    <published>2018-01-26T16:08:24.000Z</published>
    <updated>2018-03-17T17:28:20.495Z</updated>
    
    <content type="html"><![CDATA[<p>如题，记一次百G数据的聚类算法实施过程，用的技术都不难，spark和kmeans，我想你会认为这没有什么难度，我接到这个任务的时候也认为没有难度，可是一周之后我发现我错了，数据量100G的确不大，但难度在于我需要对 kmeans 的 train过程执行将近3000次，而且需要高效的完成。那么问题就来了，如何保证高效和准确性。（声明小编对Spark也不是说很熟悉）</p><a id="more"></a><h1 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h1><p>数据格式为三列，第一列为类别ID，第二列为商品ID，第三列为价格，数据格式如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1000    2000    45.3</span><br><span class="line">1000    2001    121.3</span><br><span class="line">1001    2002    4125.3</span><br><span class="line">1000    2003    225.3</span><br><span class="line">1001    2004    3415.3</span><br><span class="line">1000    2005    12245.3   </span><br><span class="line">...     ...     ....</span><br></pre></td></tr></table></figure></p><p>数据有很多条，数据量为将近100G，存储在hdfs上，第一列品类ID不唯一，每个品类ID下有多个商品ID，商品ID唯一，价格为浮点型数据</p><p>现在要对每个品类下的价格进行聚类，得到1~7个价格level（7level的价格要比6level的价格高，以此类推）</p><h1 id="第一次尝试"><a href="#第一次尝试" class="headerlink" title="第一次尝试"></a>第一次尝试</h1><p>第一次尝试很天真，思路也很正常，如下：<br>1：全量加载数据，形成rdd<br>2：数据split之后，按key进行groupby<br>3：针对每个key（也就是类别ID）进行kmeans聚类和预测，并将结果写入hdfs<br>4：加载每个类别的结果，进行聚合形成最终结果</p><p>那么开始写代码。papapa写了一堆，发现groupBy之后的数据格式是CompactBuffer，转化成spark kmeans train所需要的格式之后，代码卡着不会动，不明所以（我估计是格式没有转正确，不是kmeans 所需要的格式，但是如果不是kmeans 需要的格式，应该会报错呀），后来当我把代码打包，提交到集群上运行时，提示我kmeans train所在的函数中没有指定master url，可是我明明指定了，后来才发现是因为，我在rdd操作过程中能够，嵌套了函数，函数中又重新使用了rdd，也就是说rdd 不能嵌套rdd使用，具体可参考 <a href="https://www.zhihu.com/question/54439266" target="_blank" rel="external">Spark 为什么 不允许 RDD 嵌套-如 RDD[RDD[T]]</a>，而我在本地测试时指的都是local，没有进行报错，至此这条路行不通，也就是说不能按这样的思路执行</p><p>在该思路的基础上进行改进：<br>既然rdd不能嵌套rdd使用，何不先得到所有的类别id，然后在全量数据总filter单个类别id进行kmeans操作呢？</p><p>该代码，测试，伪代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">leibieID_list = XXXXX</span><br><span class="line">leibieID_list.map(one =&gt; kmeans(one,path))</span><br></pre></td></tr></table></figure></p><p>需要注意的是 leibieID_list.map 操作并不是分布式的，而是for 循环，这样3000个类别id运行完，时间可想而知，是极其耗时的，所以这条路也失败了（不是说行不通，是因为耗时）</p><h1 id="第二次尝试"><a href="#第二次尝试" class="headerlink" title="第二次尝试"></a>第二次尝试</h1><p>经过上边的尝试发现不行，那么我想是不是先全量读取数据，然后按照类别ID，将同一个类别ID的数据写到一个文件（或者文件夹下），然后再对之操作</p><p>开始写classify by ID 的代码，这里遇到了问题是如何让同一个类别ID的数据写到一个文件中，上网查了一些资料，可以参考之前整理的笔记</p><p><a href="http://blog.csdn.net/gamer_gyt/article/details/79157055" target="_blank" rel="external"> Spark多路径输出和二次排序</a></p><p>这里边有实现的办法，但是还有一个问题，对全量数据（100G）进行shuffle的时候，由于数据量特别大，也特别占用资源，往往会出现一些内存上的错误。</p><p>这里采用的策略是将全量数据rdd进行random split，然后for循环遍历split之后的rdd，进行saveAsTestFile，保存的目录这样设计<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/path/split=0/</span><br><span class="line">/path/split=1/</span><br><span class="line">/path/split=2/</span><br><span class="line">/path/split=3/</span><br><span class="line">    ...  ...</span><br></pre></td></tr></table></figure></p><p>这样的话，就可以避免大量数据 shuffle 耗费资源的问题了，而且也不影响后续数据的使用，同时这一步也会把类别id提取出来，保存在hdfs上，供下一步使用。</p><p>经历了上一步的数据准备，开始step 2的开发，第二步的思路：<br>加载第一步保存的类别id list文件，分成5份，启动5个spark任务进行train，至此，思路是正确的，但却忽略了一个很严正的问题：数据倾斜</p><p>由于是随机对类别 id 进行分组操作，那么不能保证没组中每个类别id对应的数据条数的大概一致性，也就是存在某个ID 数据条数只有几十条，而有些ID 数据条数千万条，这种情况下就会导致代码在运行过程中，有些task很快运行完了，有些执行了好久也没完事。</p><h1 id="第三次尝试"><a href="#第三次尝试" class="headerlink" title="第三次尝试"></a>第三次尝试</h1><p>有了第二次的经验，想法就是如何将数据条数差不多的分到同一组里，我采用的方法是进行统计，按照10的X次方形式进行分组，比如说<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1~10    1</span><br><span class="line">10~100  2</span><br><span class="line">100~1000    3</span><br><span class="line">....</span><br></pre></td></tr></table></figure></p><p>但是这样也有一个问题，就是这样大概符合正太分布，4、5、6这样的组里数据条数比较多，1、2、3和7、8、9这样的数据条数少，这样就会因为4、5、6组的程序运行时间较长，整体任务运行时间也较长。</p><p>所以这里采用合并和拆分的策略，比如说将1,2,3合并到一组，4、5、6分别拆成两组，7、8、9合成一组，这样就会保证每组运行的时间是差不多的。（实际情况中，要根据数据的分布进行合理的拆分和合并）</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>至此，问题算是最终解决了，相比原先的MR版本，时间缩减了将近8个小时，在整个优化的过程中，其实对于经验足够的开发者来说，可能很快就会解决，但对于我们这些新手，可能就要耗费些时间，涨涨记性了，在整个过程中对spark也算是有进一步的了解了。</p><p>其他的相关笔记：</p><ul><li><p><a href="http://blog.csdn.net/gamer_gyt/article/details/79157055" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt/article/details/79157055</a></p></li><li><p><a href="http://blog.csdn.net/gamer_gyt/article/details/79135118" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt/article/details/79135118</a></p></li></ul><hr><center>    <img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">    <br>打开微信扫一扫，关注公众号【数据与算法联盟】</center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;如题，记一次百G数据的聚类算法实施过程，用的技术都不难，spark和kmeans，我想你会认为这没有什么难度，我接到这个任务的时候也认为没有难度，可是一周之后我发现我错了，数据量100G的确不大，但难度在于我需要对 kmeans 的 train过程执行将近3000次，而且需要高效的完成。那么问题就来了，如何保证高效和准确性。（声明小编对Spark也不是说很熟悉）&lt;/p&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark多路径输出和二次排序</title>
    <link href="http://yoursite.com/2018/01/25/Spark/Spark%E5%A4%9A%E8%B7%AF%E5%BE%84%E8%BE%93%E5%87%BA%E5%92%8C%E4%BA%8C%E6%AC%A1%E6%8E%92%E5%BA%8F/"/>
    <id>http://yoursite.com/2018/01/25/Spark/Spark多路径输出和二次排序/</id>
    <published>2018-01-24T16:22:03.000Z</published>
    <updated>2018-03-17T17:30:39.099Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>在实际应用场景中，我们对于Spark往往有各式各样的需求，比如说想MR中的二次排序，Top N，多路劲输出等。那么这篇文章我们就来看下这几个问题。</p></blockquote><a id="more"></a><h1 id="二次排序"><a href="#二次排序" class="headerlink" title="二次排序"></a>二次排序</h1><p>假设我们的数据是这样的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1   2</span><br><span class="line">1   3</span><br><span class="line">1   1</span><br><span class="line">1   6</span><br><span class="line">1   4</span><br><span class="line">2   5</span><br><span class="line">2   8</span><br><span class="line">2   3</span><br></pre></td></tr></table></figure></p><p>我们想要实现第一列按降序排列，当第一列相同时，第二列按降序排列</p><p>定义一个SecondSortKey类：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class SecondSortKey(val first: Int, val second: Int)</span><br><span class="line">  extends Ordered[SecondSortKey] with Serializable &#123;</span><br><span class="line">  override def compare(that: SecondSortKey): Int = &#123;</span><br><span class="line">    if (this.first - that.first == 0) &#123;</span><br><span class="line">      this.second - that.second</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      this.first - that.first</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>然后这样去使用<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val lines = sc.textFile(&quot;test.txt&quot;)</span><br><span class="line">val pairs = lines.map &#123; x =&gt;</span><br><span class="line">      (new SecondSortKey(x.split(&quot;\\s+&quot;)(0).toInt,</span><br><span class="line">        x.split(&quot;\\s+&quot;)(1).toInt), x)</span><br><span class="line">    &#125;</span><br><span class="line">val sortedPairs = pairs.sortByKey(false);</span><br><span class="line">sortedPairs.map(_._2).foreach(println)</span><br></pre></td></tr></table></figure></p><p>当然这里如果想按第一列升序，当第一列相同时，第二列升序的顺序排列，只需要对SecondSoryKey做如下修改即可<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class SecondSortKey(val first: Int, val second: Int)</span><br><span class="line">  extends Ordered[SecondSortKey] with Serializable &#123;</span><br><span class="line">  override def compare(that: SecondSortKey): Int = &#123;</span><br><span class="line">    if (this.first - that.first !== 0) &#123;</span><br><span class="line">      this.second - that.second</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      this.first - that.first</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>当时使用的使用去掉<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pairs.sortByKey(false)</span><br></pre></td></tr></table></figure></p><p>中的false</p><h1 id="Top-N"><a href="#Top-N" class="headerlink" title="Top N"></a>Top N</h1><p>同样还是上边的数据，假设我们要得到第一列中的前五位<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val lines = sc.textFile(&quot;test.txt&quot;)</span><br><span class="line">val rdd = lines</span><br><span class="line">        .map(x =&gt; x.split(&quot;\\s+&quot;))</span><br><span class="line">        .map(x =&gt; (x(0),x(1)))</span><br><span class="line">        .sortByKey()</span><br><span class="line">rdd.take(N).foreach(println)</span><br></pre></td></tr></table></figure></p><h1 id="多路径输出"><a href="#多路径输出" class="headerlink" title="多路径输出"></a>多路径输出</h1><p>自己在使用的过程中，通过搜索发现了两种方法<br>1：调用saveAsHadoopFile函数并自定义一个OutputFormat类</p><p>自定义RDDMultipleTextOutputFormat类</p><p>RDDMultipleTextOutputFormat类中的generateFileNameForKeyValue函数有三个参数，key和value就是我们RDD的Key和Value，而name参数是每个Reduce的编号。本例中没有使用该参数，而是直接将同一个Key的数据输出到同一个文件中。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.hadoop.mapred.lib.MultipleTextOutputFormat  </span><br><span class="line">  </span><br><span class="line">class RDDMultipleTextOutputFormat extends MultipleTextOutputFormat[Any, Any] &#123;  </span><br><span class="line">  override def generateFileNameForKeyValue(key: Any, value: Any, name: String): String =  </span><br><span class="line">    key.asInstanceOf[String]  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>调用<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize(List((&quot;w&quot;, &quot;www&quot;), (&quot;b&quot;, &quot;blog&quot;), (&quot;c&quot;, &quot;com&quot;), (&quot;w&quot;, &quot;bt&quot;)))  </span><br><span class="line">      .map(value =&gt; (value._1, value._2 + &quot;Test&quot;))  </span><br><span class="line">      .partitionBy(new HashPartitioner(3))  </span><br><span class="line">      .saveAsHadoopFile(&quot;/iteblog&quot;, classOf[String],classOf[String],classOf[RDDMultipleTextOutputFormat])</span><br></pre></td></tr></table></figure></p><p>这里的<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">new HashPartitioner(3)</span><br></pre></td></tr></table></figure></p><p>中的3是有key的种类决定的，当然在实际应用场景中，我们可能并不知道有多少k，这个时候就可以通过一个rdd 的 distinct操作来得到唯一key的数目。</p><p>2：使用dataframe<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">people_rdd = sc.parallelize([(1, &quot;alice&quot;), (1, &quot;bob&quot;), (2,&quot;charlie&quot;)])</span><br><span class="line">people_df = people_rdd.toDF([&quot;number&quot;, &quot;name&quot;])</span><br><span class="line">people_df.write.partitionBy(&quot;number&quot;).format(&quot;text&quot;).save(path  )</span><br></pre></td></tr></table></figure></p><p>当然这两种方法都有一个缺陷，就是当数据量特别大的时候，数据在repartition的过程中特别耗费资源，也会容易出现任务failed的情况，小编采用的解决办法是，适当的对原rdd进行split，然后遍历每个rdd，进行multioutput操作</p><p>形似如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.textFile(input)</span><br><span class="line">var split_rdd = rdd.randomSplit(Array(1.0,1.0,1.0,1.0))</span><br><span class="line">for (one &lt;- Array(1,2,3,4))</span><br><span class="line">&#123;</span><br><span class="line">    split_rdd(one)XXXX</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><hr><p>参考：</p><ul><li><a href="https://andone1cc.github.io/2017/03/04/Spark/%E4%BA%8C%E6%AC%A1%E6%8E%92%E5%BA%8Ftopn/" target="_blank" rel="external">Spark学习笔记——二次排序，TopN，TopNByGroup</a></li><li><a href="https://www.iteblog.com/archives/1281.html" target="_blank" rel="external">Spark多文件输出(MultipleOutputFormat)</a></li><li><a href="https://code.i-harness.com/en/q/16e22a0" target="_blank" rel="external">scala - Write to multiple outputs by key Spark - one Spark job</a></li><li><a href="https://stackoverflow.com/questions/23995040/write-to-multiple-outputs-by-key-spark-one-spark-job/26051042#26051042" target="_blank" rel="external">Write to multiple outputs by key Spark - one Spark job</a></li></ul><hr><center><img src="http://img.blog.csdn.net/20171231121058707?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;在实际应用场景中，我们对于Spark往往有各式各样的需求，比如说想MR中的二次排序，Top N，多路劲输出等。那么这篇文章我们就来看下这几个问题。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark提交参数说明和常见优化</title>
    <link href="http://yoursite.com/2018/01/23/Spark/Spark%E6%8F%90%E4%BA%A4%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E%E5%92%8C%E5%B8%B8%E8%A7%81%E4%BC%98%E5%8C%96/"/>
    <id>http://yoursite.com/2018/01/23/Spark/Spark提交参数说明和常见优化/</id>
    <published>2018-01-22T16:30:15.000Z</published>
    <updated>2018-03-17T17:31:39.887Z</updated>
    
    <content type="html"><![CDATA[<p>最近在搞一个价格分类模型，虽说是分类，用的是kmeans算法，求出聚类中心，对每个价格进行级别定级。虽然说起来简单，但做起来却是并没有那么容易，不只是因为数据量大，在执行任务时要不是效率问题就是shuffle报错等。但在这整个过程中对scala编程,Spark rdd 机制，以及海量数据背景下对算法的认知都有很大的提升，这一篇文章主要是总结一些Spark在shell 终端提交jar包任务的时候的相关知识，在后续文章会具体涉及到相关的”实战经历“。<br><a id="more"></a></p><h1 id="对Spark的认识"><a href="#对Spark的认识" class="headerlink" title="对Spark的认识"></a>对Spark的认识</h1><p>由于之前接触过Hadoop，对Spark也是了解一些皮毛，但中间隔了好久才重新使用spark，期间也产生过一些错误的认识。</p><p>之前觉得MapReduce耗费时间，写一个同等效果的Spark程序很快就能执行完，很长一段时间自己都是在本地的单机环境进行测试学习，所以这种错误的认知就会更加深刻，但事实却并非如此，MR之所以慢是因为每一次操作数据都写在了磁盘上，大量的IO造成了时间和资源的浪费，但是Spark是基于内存的计算引擎，相比MR，减少的是大量的IO，但并不是说给一个Spark程序足够的资源，就可以为所欲为了，在提交一个spark程序时，不仅要考虑所在资源队列的总体情况，还要考虑代码本身的高效性，要尽量避免大量的shuffle操作和action操作，尽量使用同一个rdd。</p><p>会用spark，会调api和能用好spark是两回事，在进行开发的过程中，不仅要了解运行原理，还要了解业务，将合适的方法和业务场景合适的结合在一起，才能发挥最大的价值。</p><h1 id="spark-submit"><a href="#spark-submit" class="headerlink" title="spark-submit"></a>spark-submit</h1><p>进入spark的home目录，执行以下命令查看帮助<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --help</span><br></pre></td></tr></table></figure></p><p>spark提交任务常见的两种模式<br>1：local/local[K]</p><ul><li>本地使用一个worker线程运行spark程序</li><li>本地使用K个worker线程运行spark程序</li></ul><p>此种模式下适合小批量数据在本地调试代码</p><p>2：yarn-client/yarn-cluster</p><ul><li>yarn-client：以client方式连接到YARN集群，集群的定位由环境变量HADOOP_CONF_DIR定义，该方式driver在client运行。</li><li>yarn-cluster：以cluster方式连接到YARN集群，集群的定位由环境变量HADOOP_CONF_DIR定义，该方式driver也在集群中运行。</li></ul><p>注意：若使用的是本地文件需要在file路径前加：file://</p><p>在提交任务时的几个重要参数</p><ul><li>executor-cores —— 每个executor使用的内核数，默认为1</li><li>num-executors —— 启动executors的数量，默认为2</li><li>executor-memory —— executor内存大小，默认1G</li><li>driver-cores —— driver使用内核数，默认为1</li><li>driver-memory —— driver内存大小，默认512M</li></ul><p>下边给一个提交任务的样式<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">  --master local[5]  \</span><br><span class="line">  --driver-cores 2   \</span><br><span class="line">  --driver-memory 8g \</span><br><span class="line">  --executor-cores 4 \</span><br><span class="line">  --num-executors 10 \</span><br><span class="line">  --executor-memory 8g \</span><br><span class="line">  --class PackageName.ClassName XXXX.jar \</span><br><span class="line">  --name &quot;Spark Job Name&quot; \</span><br><span class="line">  InputPath      \</span><br><span class="line">  OutputPath</span><br><span class="line">  </span><br><span class="line">如果这里通过--queue 指定了队列，那么可以免去写--master</span><br></pre></td></tr></table></figure></p><p>以上就是通过spark-submit来提交一个任务</p><h1 id="几个参数的常规设置"><a href="#几个参数的常规设置" class="headerlink" title="几个参数的常规设置"></a>几个参数的常规设置</h1><ul><li><p>executor_cores*num_executors<br>表示的是能够并行执行Task的数目<br>不宜太小或太大！一般不超过总队列 cores 的 25%，比如队列总 cores 400，最大不要超过100，最小不建议低于 40，除非日志量很小。</p></li><li><p>executor_cores<br>不宜为1！否则 work 进程中线程数过少，一般 2~4 为宜。</p></li><li><p>executor_memory<br>一般 6~10g 为宜，最大不超过20G，否则会导致GC代价过高，或资源浪费严重。</p></li><li><p>driver-memory<br>driver 不做任何计算和存储，只是下发任务与yarn资源管理器和task交互，除非你是 spark-shell，否则一般 1-2g</p></li></ul><hr><p>增加每个executor的内存量，增加了内存量以后，对性能的提升，有三点：</p><ul><li>1、如果需要对RDD进行cache，那么更多的内存，就可以缓存更多的数据，将更少的数据写入磁盘，<br>甚至不写入磁盘。减少了磁盘IO。</li><li>2、对于shuffle操作，reduce端，会需要内存来存放拉取的数据并进行聚合。如果内存不够，也会写入磁盘。如果给executor分配更多内存以后，就有更少的数据，需要写入磁盘，甚至不需要写入磁盘。减少了磁盘IO，提升了性能。</li><li>3、对于task的执行，可能会创建很多对象。如果内存比较小，可能会频繁导致JVM堆内存满了，然后频繁GC，垃圾回收，minor GC和full GC。（速度很慢）。内存加大以后，带来更少的GC，垃圾回收，避免了速度变慢，性能提升。</li></ul><h1 id="常规注意事项"><a href="#常规注意事项" class="headerlink" title="常规注意事项"></a>常规注意事项</h1><ul><li>预处理数据，丢掉一些不必要的数据</li><li>增加Task的数量</li><li>过滤掉一些容易导致发生倾斜的key</li><li>避免创建重复的RDD</li><li>尽可能复用一个RDD</li><li>对多次使用的RDD进行持久化</li><li>尽量避免使用shuffle算子</li><li>在要使用groupByKey算子的时候,尽量用reduceByKey或者aggregateByKey算子替代.因为调用groupByKey时候,按照相同的key进行分组,形成RDD[key,Iterable[value]]的形式,此时所有的键值对都将被重新洗牌,移动,对网络数据传输造成理论上的最大影响.</li><li>使用高性能的算子</li></ul><hr><p>参考：<br>1：<a href="http://www.cnblogs.com/haozhengfei/p/e570f24c43fa15f23ebb97929a1b7fe6.html" target="_blank" rel="external">http://www.cnblogs.com/haozhengfei/p/e570f24c43fa15f23ebb97929a1b7fe6.html</a><br>2：<a href="https://www.jianshu.com/p/4c584a3bac7d" target="_blank" rel="external">https://www.jianshu.com/p/4c584a3bac7d</a></p><center>    <img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">    <br>打开微信扫一扫，关注公众号【数据与算法联盟】</center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在搞一个价格分类模型，虽说是分类，用的是kmeans算法，求出聚类中心，对每个价格进行级别定级。虽然说起来简单，但做起来却是并没有那么容易，不只是因为数据量大，在执行任务时要不是效率问题就是shuffle报错等。但在这整个过程中对scala编程,Spark rdd 机制，以及海量数据背景下对算法的认知都有很大的提升，这一篇文章主要是总结一些Spark在shell 终端提交jar包任务的时候的相关知识，在后续文章会具体涉及到相关的”实战经历“。&lt;br&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>用大把的时间仿徨，却用几个瞬间成长</title>
    <link href="http://yoursite.com/2017/12/31/%E9%9A%8F%E6%89%8B%E8%AE%B0/%E7%94%A8%E5%A4%A7%E6%8A%8A%E7%9A%84%E6%97%B6%E9%97%B4%E4%BB%BF%E5%BE%A8%EF%BC%8C%E5%8D%B4%E7%94%A8%E5%87%A0%E4%B8%AA%E7%9E%AC%E9%97%B4%E6%88%90%E9%95%BF/"/>
    <id>http://yoursite.com/2017/12/31/随手记/用大把的时间仿徨，却用几个瞬间成长/</id>
    <published>2017-12-31T03:02:00.000Z</published>
    <updated>2018-03-17T17:15:39.467Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>人总要在特定的阶段去完成特定的事情，然后转身告诉自己，继续往前。</p></blockquote><p>提笔写这篇文章，内心是毫无波澜的，或许是因为曾经的经历让我意识到那一切都是缘的力所能及。以下的内容可能没有章节，没有顺序，但都是心灵能够触及的地方。<br><a id="more"></a></p><p>2017年对我个人来讲是比较重要的一年，可以说是完成了自我的一个改变吧，但是脱壳之后，需要的是更加努力的完善自我，这样才能够避免被淘汰。为什么这么说？因为这一年我觉得对我个人影响最大的两件事是：买了首套房（虽然只是付了首付）；在年末之时选择离开，侥幸的入职了京东。一个是为“家庭”做了必要的准备，一个是为“事业”做了点缀。而至于其他的大大小小的事情，则是17年的五味杂粮，并不是那么重要，但却也缺一不可。感谢17年，那些我认识的，或者我不认识的，帮助过我的，或者我帮助过的人。</p><h1 id="过往"><a href="#过往" class="headerlink" title="过往"></a>过往</h1><p>人上了年纪，总爱回忆！</p><p>刚好这两天，“18岁”刷爆了朋友圈和QQ空间，或许这是连腾讯有没有预料到的起死回生或者苟延残喘吧。趁机翻了下我的QQ空间相册，突然发现了，这二十多年来以来，我也是经历颇丰。</p><p>从上海到江苏，从沈阳到长春，从南京到天津，最后到北京，这一条路一走便是4年。<br>从初中到高中，从大学到社会，这一条路一走便是11年。</p><p>过往的这条路上遇见了很多人，碰见很多事，但终究还是走散了，看淡了，不过庆幸的是那些一直还有联系的，我们还能彼此叫出姓名的人，不管是18岁之前，还是18岁之后，我想我们是幸运的，</p><p>年少的我们曾经难免会埋下羞涩的种子，在记忆深处藏着一些不可告人的秘密，直到有一天我们盘膝而坐，三巡酒过，才道出那些现在我们认为可笑的不能再可笑的羞涩，从此，稚嫩青春里的唯一一朵留恋，也该告一段落。</p><p>一杯敬明天，一杯敬过往。灵魂不再无处安放。</p><h1 id="遇见"><a href="#遇见" class="headerlink" title="遇见"></a>遇见</h1><p>人有了目标，便爱胡闹！</p><p>2017年，不管是工作，还是自我学习都收获了挺多知识，感谢万维接受了我这个毫无工作经验的“学生”吧，在这里的确收获了挺多，让我明白没有结合业务的技术，只是向别人吹嘘而毫无创造价值的垃圾，不管你在哪，技术都是为了推动业务的增长。</p><p>2017年，还是习惯性的写一些学习笔记，只不过是频率明显了降了下来，这只能归结于自己变得懒惰了，从开通博客到现在，所有的表层的收获只能通过这些浅显的数据来表达：</p><center><img src="http://img.blog.csdn.net/20171231110350995?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><p>只是没有17年尾的时候访问量达到100W，不过一切随缘吧。</p><p>下边这几张图是最近一个月的访问分析情况，不管怎样，你学习了，也帮助其他人了，这就是成长。</p><center><img src="http://img.blog.csdn.net/20171231111249749?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">    <img src="http://img.blog.csdn.net/20171231111301875?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">    <img src="http://img.blog.csdn.net/20171231111312908?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">    <img src="http://img.blog.csdn.net/20171231111325084?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">    </center><p>2017年，开通了微信公众号【数据与算法联盟】（原名为码农故事多），没有刻意的运营，没有刻意的传播积累用户，一切随缘。当然如果你想加入我们的数据与算法学习交流群的话，欢迎加我的微信，拉你入群，群里有很多大牛，不定时进行“扯淡”。我们的宗旨就是以技术会友，分享与进步！感谢2017年和以往遇见的所有好友！</p><center>    <img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">    <br>长按二维码识别，关注微信公众号【数据与算法联盟】</center><p>2017年，很侥幸的加入了JD这个大家庭，未来的一切都是未知数，但你能做的就是向他人学习，围绕着工作进行深度的学习和成长。</p><h1 id="新生"><a href="#新生" class="headerlink" title="新生"></a>新生</h1><p>人过了18，要努力发芽！</p><p>过去的一年里定了太多的目标，结果大部分都没有实现，哎，分析一下，大部门的目标都是盲目的，没有围绕工作的目标（学习规划吧算是），其实实现起来是有难度的，所以新的一年里调整计划，重新出发。</p><p>2018不会定太多的目标，主要是想在技术和业务层面提升下自己，不管是学习大数据还是算法，都会围绕功能工作和一条主旋律进行展开。</p><p>感谢一路以来，遇见的所有人！</p><hr><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;人总要在特定的阶段去完成特定的事情，然后转身告诉自己，继续往前。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;提笔写这篇文章，内心是毫无波澜的，或许是因为曾经的经历让我意识到那一切都是缘的力所能及。以下的内容可能没有章节，没有顺序，但都是心灵能够触及的地方。&lt;br&gt;
    
    </summary>
    
      <category term="随手记" scheme="http://yoursite.com/categories/%E9%9A%8F%E6%89%8B%E8%AE%B0/"/>
    
    
      <category term="随手记" scheme="http://yoursite.com/tags/%E9%9A%8F%E6%89%8B%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Hexo-Yilia加入相册功能</title>
    <link href="http://yoursite.com/2017/12/14/%E9%9A%8F%E6%89%8B%E8%AE%B0/Hexo-Yilia%E5%8A%A0%E5%85%A5%E7%9B%B8%E5%86%8C%E5%8A%9F%E8%83%BD/"/>
    <id>http://yoursite.com/2017/12/14/随手记/Hexo-Yilia加入相册功能/</id>
    <published>2017-12-14T09:55:29.000Z</published>
    <updated>2018-03-17T17:13:17.011Z</updated>
    
    <content type="html"><![CDATA[<p>参考：<a href="http://maker997.com/2017/07/01/hexo-Yilia-%E4%B8%BB%E9%A2%98%E5%A6%82%E4%BD%95%E6%B7%BB%E5%8A%A0%E7%9B%B8%E5%86%8C%E5%8A%9F%E8%83%BD" target="_blank" rel="external">点击查看</a></p><p>但是其中有一些小问题，自己便重新整理了一下（本文适用于使用github存放照片）</p><a id="more"></a><h1 id="主页新建相册链接"><a href="#主页新建相册链接" class="headerlink" title="主页新建相册链接"></a>主页新建相册链接</h1><p>主题_config.json文件的menu 中加入 相册和对应的链接<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">themes/yilia/_config.json</span><br><span class="line"></span><br><span class="line">menu:</span><br><span class="line">  主页: /</span><br><span class="line">  ... ...</span><br><span class="line">  相册: /photos</span><br></pre></td></tr></table></figure></p><h1 id="新建目录并拷贝相应文件"><a href="#新建目录并拷贝相应文件" class="headerlink" title="新建目录并拷贝相应文件"></a>新建目录并拷贝相应文件</h1><p>使用的是litten 大神的博客 photos文件夹，对应的路径为：<br><a href="https://github.com/litten/BlogBackup/tree/master/source/photos" target="_blank" rel="external">https://github.com/litten/BlogBackup/tree/master/source/photos</a></p><p>自己的项目根目录下的source文件夹下新建photos文件夹，将下载的几个文件放在该文件夹中，或者不用新建，直接将下载的photos文件夹放在source目录下。</p><h1 id="文件修改"><a href="#文件修改" class="headerlink" title="文件修改"></a>文件修改</h1><ol><li>修改 ins.js 文件的 render()函数<br>这个函数是用来渲染数据的<br>修改图片的路径地址.minSrc 小图的路径. src 大图的路径.修改为自己的图片路径(github的路径)<br>例如我的为：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">var minSrc = &apos;https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/&apos; + data.link[i] + &apos;.min.jpg&apos;;</span><br><span class="line">var src = &apos;https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/photos/&apos; + data.link[i];</span><br></pre></td></tr></table></figure></li></ol><h1 id="生成json"><a href="#生成json" class="headerlink" title="生成json"></a>生成json</h1><p>1：下载相应python工具文件</p><ul><li>tools.py</li><li>ImageProcess.py</li></ul><p>下载地址：<a href="https://github.com/Thinkgamer/GitBlog" target="_blank" rel="external">https://github.com/Thinkgamer/GitBlog</a></p><p>2：新建photos和min_photos文件夹<br>在项目根目录下创建，用来存放照片和压缩后的照片<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir photos</span><br><span class="line">mkdir min_photos</span><br></pre></td></tr></table></figure></p><p>3：py文件和文件夹都放在项目根目录下</p><p>4：生成json<br>执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tools.py</span><br></pre></td></tr></table></figure></p><p>如果提示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;tools.py&quot;, line 13, in &lt;module&gt;</span><br><span class="line">    from PIL import Image</span><br><span class="line">ImportError: No module named PIL</span><br></pre></td></tr></table></figure></p><p>说明你没有安装pillow，执行以下命令安装即可<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pillow</span><br></pre></td></tr></table></figure></p><p>如果报错：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ValueError: time data &apos;DSC&apos; does not match format &apos;%Y-%m-%d&apos;</span><br></pre></td></tr></table></figure></p><p>说明你照片的命名方式不合格，这里必须命名为以下这样的格式（当然时间是随意的）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2016-10-12_xxx.jpg/png</span><br></pre></td></tr></table></figure></p><p>ok，至此会在min_photos文件夹下生成同名的文件，但是大小会小很多</p><h1 id="本地预览和部署"><a href="#本地预览和部署" class="headerlink" title="本地预览和部署"></a>本地预览和部署</h1><h2 id="本地预览"><a href="#本地预览" class="headerlink" title="本地预览"></a>本地预览</h2><p>项目根目录下执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure></p><p>浏览器4000端口访问，按照上边的方式进行配置，正常情况下你是看不到图片的，通过调试可以发现图片的url中后缀变成了 xxx.jpg.jpg，所以我们要去掉一个jpg</p><p>改正方法<br>ins.js/render 函数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">var minSrc = &apos;https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/&apos; + data.link[i] + &apos;.min.jpg&apos;;</span><br><span class="line"></span><br><span class="line">换成</span><br><span class="line"></span><br><span class="line">var minSrc = &apos;https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/&apos; + data.link[i];</span><br><span class="line"></span><br><span class="line">注释掉该行：</span><br><span class="line">src += &apos;.jpg&apos;;</span><br></pre></td></tr></table></figure></p><p>到这里没完，路径都对了，但是在浏览器中还是不能看到图片，调试发现，下载大神的photos文件夹的ins.js中有一行代码，饮用了一张图片，默认情况下，在你的项目中，这张图片是不存在的，改正办法就是在对应目录下放一张图片，并修改相应的名字</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">src=&quot;/assets/img/empty.png</span><br></pre></td></tr></table></figure><p>ok，至此刷新浏览器是可以看到图片的，如果还看不到，应该就是浏览器缓存问题了，如果还有问题，可以加我微信进行沟通：gyt13342445911</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参考：&lt;a href=&quot;http://maker997.com/2017/07/01/hexo-Yilia-%E4%B8%BB%E9%A2%98%E5%A6%82%E4%BD%95%E6%B7%BB%E5%8A%A0%E7%9B%B8%E5%86%8C%E5%8A%9F%E8%83%BD&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;点击查看&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;但是其中有一些小问题，自己便重新整理了一下（本文适用于使用github存放照片）&lt;/p&gt;
    
    </summary>
    
      <category term="随手记" scheme="http://yoursite.com/categories/%E9%9A%8F%E6%89%8B%E8%AE%B0/"/>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>梯度算法之批量梯度下降，随机梯度下降和小批量梯度下降</title>
    <link href="http://yoursite.com/2017/12/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95%E4%B9%8B%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%8C%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%92%8C%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    <id>http://yoursite.com/2017/12/14/机器学习/梯度算法之批量梯度下降，随机梯度下降和小批量梯度下降/</id>
    <published>2017-12-14T06:40:43.000Z</published>
    <updated>2018-03-17T17:03:37.127Z</updated>
    
    <content type="html"><![CDATA[<p>在机器学习领域，体梯度下降算法分为三种</p><ul><li>批量梯度下降算法（BGD，Batch gradient descent algorithm）</li><li>随机梯度下降算法（SGD，Stochastic gradient descent algorithm）</li><li>小批量梯度下降算法（MBGD，Mini-batch gradient descent algorithm）</li></ul><a id="more"></a><h1 id="批量梯度下降算法"><a href="#批量梯度下降算法" class="headerlink" title="批量梯度下降算法"></a>批量梯度下降算法</h1><p>BGD是最原始的梯度下降算法，每一次迭代使用全部的样本，即权重的迭代公式中(公式中用$\theta$代替$\theta_i$)，</p><script type="math/tex; mode=display">\jmath (\theta _0,\theta _1,...,\theta _n)=\sum_{i=0}^{m}( h_\theta(x_0,x_1,...,x_n)-y_i )^2</script><script type="math/tex; mode=display">\theta _i = \theta _i - \alpha \frac{\partial \jmath (\theta _1,\theta _2,...,\theta _n)}{\partial \theta _i}</script><script type="math/tex; mode=display">公式(1)</script><p>这里的m代表所有的样本，表示从第一个样本遍历到最后一个样本。</p><p>特点：</p><ul><li>能达到全局最优解，易于并行实现</li><li>当样本数目很多时，训练过程缓慢</li></ul><h1 id="随机梯度下降算法"><a href="#随机梯度下降算法" class="headerlink" title="随机梯度下降算法"></a>随机梯度下降算法</h1><p>SGD的思想是更新每一个参数时都使用一个样本来进行更新，即公式（1）中m为1。每次更新参数都只使用一个样本，进行多次更新。这样在样本量很大的情况下，可能只用到其中的一部分样本就能得到最优解了。<br>但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。</p><p>特点：</p><ul><li>训练速度快</li><li>准确度下降，并不是最优解，不易于并行实现</li></ul><h1 id="小批量梯度下降算法"><a href="#小批量梯度下降算法" class="headerlink" title="小批量梯度下降算法"></a>小批量梯度下降算法</h1><p>MBGD的算法思想就是在更新每一参数时都使用一部分样本来进行更新，也就是公式（1）中的m的值大于1小于所有样本的数量。</p><p>相对于随机梯度下降，Mini-batch梯度下降降低了收敛波动性，即降低了参数更新的方差，使得更新更加稳定。相对于批量梯度下降，其提高了每次学习的速度。并且其不用担心内存瓶颈从而可以利用矩阵运算进行高效计算。一般而言每次更新随机选择[50,256]个样本进行学习，但是也要根据具体问题而选择，实践中可以进行多次试验，选择一个更新速度与更次次数都较适合的样本数。mini-batch梯度下降可以保证收敛性，常用于神经网络中。</p><h1 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h1><p>在样本量较小的情况下，可以使用批量梯度下降算法，样本量较大的情况或者线上，可以使用随机梯度下降算法或者小批量梯度下降算法。</p><p>在机器学习中的无约束优化算法，除了梯度下降以外，还有前面提到的最小二乘法，此外还有牛顿法和拟牛顿法。</p><p>梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。</p><p>梯度下降法和牛顿法/拟牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法/拟牛顿法是用二阶的海森矩阵的逆矩阵或伪逆矩阵求解。相对而言，使用牛顿法/拟牛顿法收敛更快。但是每次迭代的时间比梯度下降法长。</p><h1 id="sklearn中的SGD"><a href="#sklearn中的SGD" class="headerlink" title="sklearn中的SGD"></a>sklearn中的SGD</h1><p>sklearn官网上查了一下，并没有找到BGD和MBGD的相关文档，只是看到可SGD的，感兴趣的可以直接去官网看英文文档，点击SGD查看：<a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html" target="_blank" rel="external">SGD</a>，这也有一个中文的 <a href="http://sklearn.lzjqsdd.com/modules/sgd.html" target="_blank" rel="external">SGD</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">In [1]: from sklearn.linear_model import SGDClassifier</span><br><span class="line"></span><br><span class="line">In [2]: X = [[0., 0.], [1., 1.]]</span><br><span class="line"></span><br><span class="line">In [3]: y = [0, 1]</span><br><span class="line"></span><br><span class="line">In [4]: clf = SGDClassifier(loss=&quot;hinge&quot;, penalty=&quot;l2&quot;)</span><br><span class="line"></span><br><span class="line">In [5]: clf.fit(X, y)</span><br><span class="line">Out[5]: </span><br><span class="line">SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,</span><br><span class="line">       eta0=0.0, fit_intercept=True, l1_ratio=0.15,</span><br><span class="line">       learning_rate=&apos;optimal&apos;, loss=&apos;hinge&apos;, n_iter=5, n_jobs=1,</span><br><span class="line">       penalty=&apos;l2&apos;, power_t=0.5, random_state=None, shuffle=True,</span><br><span class="line">       verbose=0, warm_start=False)</span><br><span class="line"></span><br><span class="line">In [6]:  clf.predict([[2., 2.]])</span><br><span class="line">Out[6]: array([1])</span><br><span class="line"></span><br><span class="line">In [7]: clf.coef_ </span><br><span class="line">Out[7]: array([[ 9.91080278,  9.91080278]])</span><br><span class="line"></span><br><span class="line">In [8]: clf.intercept_ </span><br><span class="line">Out[8]: array([-9.97004991])</span><br></pre></td></tr></table></figure><p>参考：</p><ul><li><a href="https://www.cnblogs.com/pinard/p/5970503.html" target="_blank" rel="external">https://www.cnblogs.com/pinard/p/5970503.html</a></li><li><a href="http://blog.csdn.net/uestc_c2_403/article/details/74910107" target="_blank" rel="external">http://blog.csdn.net/uestc_c2_403/article/details/74910107</a></li></ul><hr><center>    <img src="/img/gzh.jpg" weight="250px" height="250px">    <br>打开微信扫一扫，关注微信公众号【数据与算法联盟】</center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在机器学习领域，体梯度下降算法分为三种&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;批量梯度下降算法（BGD，Batch gradient descent algorithm）&lt;/li&gt;
&lt;li&gt;随机梯度下降算法（SGD，Stochastic gradient descent algorithm）&lt;/li&gt;
&lt;li&gt;小批量梯度下降算法（MBGD，Mini-batch gradient descent algorithm）&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="梯度下降" scheme="http://yoursite.com/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>梯度算法之梯度上升和梯度下降</title>
    <link href="http://yoursite.com/2017/12/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95%E4%B9%8B%E6%A2%AF%E5%BA%A6%E4%B8%8A%E5%8D%87%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    <id>http://yoursite.com/2017/12/14/机器学习/梯度算法之梯度上升和梯度下降/</id>
    <published>2017-12-14T06:11:11.000Z</published>
    <updated>2018-03-17T17:04:40.231Z</updated>
    
    <content type="html"><![CDATA[<p>第一次看见随机梯度上升算法是看《机器学习实战》这本书，当时也是一知半解，只是大概知道和高等数学中的函数求导有一定的关系。下边我们就好好研究下随机梯度上升（下降）和梯度上升（下降）。<br><a id="more"></a></p><h1 id="高数中的导数"><a href="#高数中的导数" class="headerlink" title="高数中的导数"></a>高数中的导数</h1><p>设导数 y = f(x) 在 $ x_0 $的某个邻域内有定义，当自变量从 $ x_0 $ 变成</p><script type="math/tex; mode=display">x_{0} + \Delta x</script><p>函数y=f(x)的增量</p><script type="math/tex; mode=display">\Delta y = f(x_0 + \Delta x) - f(x_0)</script><p>与自变量的增量 $ \Delta x $ 之比：</p><script type="math/tex; mode=display">\frac{ \Delta y }{ \Delta x } = \frac{ f(x_0 + \Delta x)-f(x_0) }{ \Delta x }</script><p>称为f(x)的平均变化率。<br>如 $ \Delta x \rightarrow 0 $ 平均变化率的极限</p><script type="math/tex; mode=display">\lim_{\Delta x \rightarrow 0} \frac{ \Delta y }{ \Delta x } = \lim_{\Delta x  \rightarrow 0} \frac{ f(x_0 + \Delta x)-f(x_0) }{ \Delta x }</script><p>存在，则称极限值为f(x)在$ x_0 $ 处的导数，并说f(x)在$ x_0 $ 处可导或有导数。当平均变化率极限不存在时，就说f(x)在 $ x_0 $ 处不可导或没有导数。</p><p>关于导数的说明</p><p>1）点导数是因变量在$ x_0 $ 处的变化率，它反映了因变量随自变量的变化而变化的快慢成都</p><p>2）如果函数y = f(x)在开区间 I 内的每点都可导，就称f(x)在开区间 I 内可导</p><p>3）对于任一 x 属于 I ，都对应着函数f(x)的一个导数，这个函数叫做原来函数f(x)的导函数</p><p>4）导函数在x1 处 为 0，若 x&lt;1 时，f’(x) &gt; 0 ，这 f(x) 递增，若f’(x)&lt;0 ，f(x)递减</p><p>5）f’(x0) 表示曲线y=f(x)在点 （x0,f($x_0$)）处的切线斜率</p><h1 id="偏导数"><a href="#偏导数" class="headerlink" title="偏导数"></a>偏导数</h1><p>函数z=f(x,y)在点(x0,y0)的某一邻域内有定义，当y固定在y0而x在 $x_0$ 处有增量$ \Delta x $ 时，相应的有函数增量</p><script type="math/tex; mode=display">f(x_0 + \Delta x, y_0) - f(x_0,y_0)</script><p>如果</p><script type="math/tex; mode=display">\lim_{\Delta x\rightarrow 0 } \frac {f(x_0 + \Delta x, y_0) - f(x_0,y_0)}{\Delta x}</script><p>存在，则称z=f(x,y)在点($x_0$,$y_0$)处对x的偏导数，记为：$ f_x(x_0,y_0) $</p><p>如果函数z=f(x,y)在区域D内任一点(x,y)处对x的偏导数都存在，那么这个偏导数就是x,y的函数，它就称为函数z=f(x,y)对自变量x的偏导数，记做</p><script type="math/tex; mode=display">\frac{ \partial z }{ \partial x } , \frac{ \partial f }{ \partial x } , z_x , f_x(x,y),</script><p>偏导数的概念可以推广到二元以上的函数，如 u = f(x,y,z)在x,y,z处</p><script type="math/tex; mode=display">f_x(x,y,z)=\lim_{\Delta x \rightarrow 0} \frac{f(x + \Delta x,y,z) -f(x,y,z)}{\Delta x}</script><script type="math/tex; mode=display">f_y(x,y,z)=\lim_{\Delta y \rightarrow 0} \frac{f(x,y + \Delta y,z) -f(x,y,z)}{\Delta y}</script><script type="math/tex; mode=display">f_z(x,y,z)=\lim_{\Delta z \rightarrow 0} \frac{f(x,y,z + \Delta z) -f(x,y,z)}{\Delta z}</script><p>可以看出导数与偏导数本质是一致的，都是自变量趋近于0时，函数值的变化与自变量的变化量比值的极限，直观的说，偏导数也就是函数在某一点沿坐标轴正方向的变化率。</p><p>区别：<br>导数指的是一元函数中，函数y=f(x)某一点沿x轴正方向的的变化率；<br>偏导数指的是多元函数中，函数y=f(x,y,z)在某一点沿某一坐标轴正方向的变化率。</p><p>偏导数的几何意义：<br>偏导数$ z = f_x(x_0,y_0)$表示的是曲面被 $ y=y_0 $ 所截得的曲线在点M处的切线$ M_0T_x $对x轴的斜率<br>偏导数$ z = f_y(x_0,y_0)$表示的是曲面被 $ x=x_0 $ 所截得的曲线在点M处的切线$ M_0T_y $对y轴的斜率</p><p>例子：<br>求 $z = x^2 + 3 xy+y^2 $在点(1,2)处的偏导数。</p><script type="math/tex; mode=display">\frac{ \partial z}{\partial x} = 2x +3y</script><script type="math/tex; mode=display">\frac{ \partial z}{\partial y} = 2y +3x</script><p>所以:<br>$z_x(x=1,y=2) = 8$<br>$z_y(x=1,y=2) = 7$</p><h1 id="方向导数"><a href="#方向导数" class="headerlink" title="方向导数"></a>方向导数</h1><script type="math/tex; mode=display">\frac{ \partial }{ \partial l }  f(x_0,x_1,...,x_n) = \lim_{\rho \rightarrow 0} \frac{\Delta y}{ \Delta x } = \lim_{\rho \rightarrow 0} \frac{ f(x_0 + \Delta x_0,...,x_j + \Delta x_j,...,x_n + \Delta x_n)-f(x_0,...,x_j,...,x_n)}{ \rho }</script><script type="math/tex; mode=display">\rho = \sqrt{ (\Delta x_0)^{2} +...+(\Delta x_j)^{2}+...+(\Delta x_n)^{2}}</script><p>前边导数和偏导数的定义中，均是沿坐标轴正方向讨论函数的变化率。那么当讨论函数沿任意方向的变化率时，也就引出了方向导数的定义，即：某一点在某一趋近方向上的导数值。</p><p>通俗的解释是： 我们不仅要知道函数在坐标轴正方向上的变化率（即偏导数），而且还要设法求得函数在其他特定方向上的变化率。而方向导数就是函数在其他特定方向上的变化率。 　</p><h1 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h1><p>与方向导数有一定的关联，在微积分里面，对多元函数的参数求 $ \partial  $ 偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数f(x,y), 分别对x,y求偏导数，求得的梯度向量就是 $<br>( \frac{ \partial f }{ \partial x },\frac{ \partial f }{ \partial y })^T<br>$ ,简称grad f(x,y)或者 $▽f(x,y)$。对于在点$(x_0,y_0)$的具体梯度向量就是$( \frac{ \partial f }{ \partial x_0 },\frac{ \partial f }{ \partial y_0 })^T$.或者$▽f(x_0,y_0)$，如果是3个参数的向量梯度，就是 $( \frac{ \partial f }{ \partial x },\frac{ \partial f }{ \partial y },\frac{ \partial f }{ \partial z })^T$,以此类推。</p><p>那么这个梯度向量求出来有什么意义呢？他的意义从几何意义上讲，就是函数变化增加最快的地方。具体来说，对于函数f(x,y),在点$(x_0,y_0)$，沿着梯度向量的方向就是$( \frac{ \partial f }{ \partial x_0 },\frac{ \partial f }{ \partial y_0 })^T$的方向是f(x,y)增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，也就是 $-( \frac{ \partial f }{ \partial x_0 },\frac{ \partial f }{ \partial y_0 })^T$的方向，梯度减少最快，也就是更加容易找到函数的最小值。</p><p>例如：<br>函数 $f(x,y) = \frac{1}{x^2+y^2} $ ，分别对x，y求偏导数得：</p><script type="math/tex; mode=display"> \frac{ \partial f }{ \partial x}=-\frac{2x}{ (x^2+y^2)^2}</script><script type="math/tex; mode=display"> \frac{ \partial f }{ \partial y}=-\frac{2y}{ (x^2+y^2)^2}</script><p>所以</p><script type="math/tex; mode=display">grad( \frac{1}{x^2+y^2} ) = (-\frac{2x}{ (x^2+y^2)^2} ,-\frac{2y}{ (x^2+y^2)^2})</script><p>函数在某一点的梯度是这样一个向量，它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值。 </p><p>注意点：<br>1）梯度是一个向量<br>2）梯度的方向是最大方向导数的方向<br>3）梯度的值是最大方向导数的值</p><h1 id="梯度下降与梯度上升"><a href="#梯度下降与梯度上升" class="headerlink" title="梯度下降与梯度上升"></a>梯度下降与梯度上升</h1><p>在机器学习算法中，在最小化损失函数时，可以通过梯度下降思想来求得最小化的损失函数和对应的参数值，反过来，如果要求最大化的损失函数，可以通过梯度上升思想来求取。</p><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><h3 id="关于梯度下降的几个概念"><a href="#关于梯度下降的几个概念" class="headerlink" title="关于梯度下降的几个概念"></a>关于梯度下降的几个概念</h3><p>1）步长（learning rate）：步长决定了在梯度下降迭代过程中，每一步沿梯度负方向前进的长度<br>2）特征（feature）：指的是样本中输入部门，比如样本（x0，y0），（x1，y1），则样本特征为x，样本输出为y<br>3）假设函数（hypothesis function）：在监督学习中，为了拟合输入样本，而使用的假设函数，记为$h_θ(x)$。比如对于样本$（x_i,y_i）(i=1,2,…n)$,可以采用拟合函数如下： $h_θ(x) = θ0+θ1_x$。<br>4）损失函数（loss function）：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。在线性回归中，损失函数通常为样本输出和假设函数的差取平方。比如对于样本（xi,yi）(i=1,2,…n),采用线性回归，损失函数为：</p><script type="math/tex; mode=display">\jmath (\theta _0,\theta _1)=\sum_{i=0}^{m}( h_\theta(x_i)-y_i )^2</script><p>其中$x_i$表示样本特征x的第i个元素，$y_i$表示样本输出y的第i个元素，$h_\theta(x_i)$ 为假设函数。</p><h3 id="梯度下降的代数方法描述"><a href="#梯度下降的代数方法描述" class="headerlink" title="梯度下降的代数方法描述"></a>梯度下降的代数方法描述</h3><ol><li><p>先决条件：确定优化模型的假设函数和损失函数<br>这里假定线性回归的假设函数为$h_\theta(x_1,x_2,…x_n)=\theta_0+\theta_1x_1+…+\theta_nx_n$，其中 $\theta _i(i=0,1,2…n)$ 为模型参数(公式中用$\theta$代替)，$x_i(i=0,1,2…n)$为每个样本的n个特征值。</p><p>则对应选定得损失函数为：</p><script type="math/tex; mode=display">\jmath (\theta _0,\theta _1,...,,\theta _n)=\sum_{i=0}^{m}( h_\theta(x_0,x_1,...,x_n)-y_i )^2</script></li><li><p>算法相关参数的初始化<br>主要是初始化 $ \theta _0,\theta _1…,\theta _n$，算法终止距离 $\varepsilon $ 以及步长 $ \alpha $。在没有任何先验知识的时候，我喜欢将所有的 $\theta$ 初始化为0， 将步长初始化为1。在调优的时候再优化。</p></li><li><p>算法过程</p></li></ol><ul><li><p>1)：确定当前损失函数的梯度，对于$\theta _i $，其梯度表达式为：</p><script type="math/tex; mode=display">\frac{\partial }{\partial \theta _i}\jmath (\theta _1,\theta _2,...,\theta _n)</script></li><li><p>2)：用步长乘以损失函数的梯度，得到当前位置的下降距离，即</p><script type="math/tex; mode=display">\alpha \frac{\partial \jmath (\theta _1,\theta _2,...,\theta _n)}{\partial \theta _i}</script></li><li><p>3)：确定是否所有的$\theta _i$ ，梯度下降的距离都小于 $ \varepsilon $，如果小于$ \varepsilon $，则算法停止，当前所有的 $\theta _i(i=1,2,3,…,n)$ 即为最终结果。否则执行下一步。</p></li><li><p>4)：更新所有的 $\theta$，对于$\theta _i $，其更新表达式如下。更新完毕后继续转入步骤1)。</p><script type="math/tex; mode=display">\theta _i = \theta _i - \alpha \frac{\partial \jmath (\theta _1,\theta _2,...,\theta _n)}{\partial \theta _i}</script><h3 id="梯度下降的矩阵方式描述"><a href="#梯度下降的矩阵方式描述" class="headerlink" title="梯度下降的矩阵方式描述"></a>梯度下降的矩阵方式描述</h3><ol><li>先决条件：确定优化模型的假设函数和损失函数<br>这里假定线性回归的假设函数为$h_\theta(x_1,x_2,…x_n)=\theta_0+\theta_1x_1+…+\theta_nx_n$，其中 $\theta _i(i=0,1,2…n)$ 为模型参数，$x_i(i=0,1,2…n)$为每个样本的n个特征值。<br>假设函数对应的矩阵表示为：$ h_\theta (x) = X \theta $，假设函数 $h_\theta(x)$ 为mx1的向量，$\theta $ 为nx1的向量，里面有n个代数法的模型参数。X为mxn维的矩阵。m代表样本的个数，n代表样本的特征数。<br>则对应选定得损失函数为：<script type="math/tex; mode=display">\jmath (\theta)=(X \theta −Y)^T (X \theta−Y)</script>其中YY是样本的输出向量，维度为m*1<br><br><br>2.算法相关参数初始化:<br>$\theta$ 向量可以初始化为默认值，或者调优后的值。算法终止距离 $\varepsilon $ ，步长 $\alpha$ 和 “梯度下降的代数方法”描述中一致。<br><br><br>3.算法过程</li></ol></li><li><p>1)：确定当前位置的损失函数的梯度，对于 $ \theta $ 向量,其梯度表达式如下：</p><script type="math/tex; mode=display">\frac{ \partial }{\partial \theta } \jmath (\theta)</script></li><li>2)：用步长乘以损失函数的梯度，得到当前位置下降的距离，即 $\alpha \frac{ \partial }{\partial \theta } \jmath (\theta)$ </li><li>3)：确定 $\theta$ 向量里面的每个值,梯度下降的距离都小于 $\varepsilon$，如果小于 $\varepsilon$ 则算法终止，当前 $\theta$ 向量即为最终结果。否则进入步骤4)</li><li>4)：更新 $\theta$ 向量，其更新表达式如下。更新完毕后继续转入步骤1)<script type="math/tex; mode=display">\theta =\theta - \alpha \frac{ \partial }{\partial \theta } \jmath (\theta)</script></li></ul><h2 id="梯度上升"><a href="#梯度上升" class="headerlink" title="梯度上升"></a>梯度上升</h2><p>梯度上升和梯度下降的分析方式是一致的，只不过把 $ \theta $ 的更新中 减号变为加号。</p><h2 id="梯度下降的算法优化"><a href="#梯度下降的算法优化" class="headerlink" title="梯度下降的算法优化"></a>梯度下降的算法优化</h2><ol><li><p>算法的步长选择。在前面的算法描述中，我提到取步长为1，但是实际上取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。前面说了。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。</p></li><li><p>算法参数的初始值选择。 初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。</p></li></ol><p>3.归一化。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化，也就是对于每个特征x，求出它的均值 $\bar{x}$ 和标准差std(x)，然后转化为：</p><script type="math/tex; mode=display">\frac{x - \bar{x}}{std(x)}</script><p>这样特征的新期望为0，新方差为1，迭代次数可以大大加快。</p><hr><p><a href="http://blog.csdn.net/walilk/article/details/50978864" target="_blank" rel="external">http://blog.csdn.net/walilk/article/details/50978864</a></p><p><a href="https://www.zhihu.com/question/24658302" target="_blank" rel="external">https://www.zhihu.com/question/24658302</a></p><p><a href="https://www.cnblogs.com/pinard/p/5970503.html" target="_blank" rel="external">https://www.cnblogs.com/pinard/p/5970503.html</a></p><p><a href="http://www.doc88.com/p-7844239247737.html" target="_blank" rel="external">http://www.doc88.com/p-7844239247737.html</a></p><hr><center>    <img src="/img/gzh.jpg" weight="250px" height="250px">    <br>打开微信扫一扫，关注微信公众号【数据与算法联盟】</center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;第一次看见随机梯度上升算法是看《机器学习实战》这本书，当时也是一知半解，只是大概知道和高等数学中的函数求导有一定的关系。下边我们就好好研究下随机梯度上升（下降）和梯度上升（下降）。&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="梯度下降" scheme="http://yoursite.com/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="梯度上升" scheme="http://yoursite.com/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8A%E5%8D%87/"/>
    
  </entry>
  
  <entry>
    <title>异常检测之指数平滑（利用elasticsearch来实现）</title>
    <link href="http://yoursite.com/2017/11/20/ELK/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%8C%87%E6%95%B0%E5%B9%B3%E6%BB%91%EF%BC%88%E5%88%A9%E7%94%A8elasticsearch%E6%9D%A5%E5%AE%9E%E7%8E%B0%EF%BC%89/"/>
    <id>http://yoursite.com/2017/11/20/ELK/异常检测之指数平滑（利用elasticsearch来实现）/</id>
    <published>2017-11-20T09:18:54.000Z</published>
    <updated>2018-03-17T17:19:52.675Z</updated>
    
    <content type="html"><![CDATA[<p>指数平滑法是一种特殊的加权平均法，加权的特点是对离预测值较近的历史数据给予较大的权数，对离预测期较远的历史数据给予较小的权数，权数由近到远按指数规律递减，所以，这种预测方法被称为指数平滑法。它可分为一次指数平滑法、二次指数平滑法及更高次指数平滑法。<br><a id="more"></a></p><h1 id="关于指数平滑的得相关资料："><a href="#关于指数平滑的得相关资料：" class="headerlink" title="关于指数平滑的得相关资料："></a>关于指数平滑的得相关资料：</h1><ul><li><p>ES API接口：</p><blockquote><p><a href="https://github.com/IBBD/IBBD.github.io/blob/master/elk/aggregations-pipeline.md" target="_blank" rel="external">https://github.com/IBBD/IBBD.github.io/blob/master/elk/aggregations-pipeline.md</a><br><br><br><br><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-pipeline-movavg-aggregation.html" target="_blank" rel="external">https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-pipeline-movavg-aggregation.html</a></p></blockquote></li><li><p>理论概念</p><blockquote><p><a href="http://blog.sina.com.cn/s/blog_4b9acb5201016nkd.html" target="_blank" rel="external">http://blog.sina.com.cn/s/blog_4b9acb5201016nkd.html</a></p></blockquote></li></ul><h1 id="ES移动平均聚合：Moving-Average的四种模型"><a href="#ES移动平均聚合：Moving-Average的四种模型" class="headerlink" title="ES移动平均聚合：Moving Average的四种模型"></a>ES移动平均聚合：Moving Average的四种模型</h1><h2 id="simple"><a href="#simple" class="headerlink" title="simple"></a>simple</h2><p>就是使用窗口内的值的和除于窗口值，通常窗口值越大，最后的结果越平滑: (a1 + a2 + … + an) / n<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">curl -XPOST &apos;localhost:9200/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class="line">&#123;</span><br><span class="line">    &quot;size&quot;: 0,</span><br><span class="line">    &quot;aggs&quot;: &#123;</span><br><span class="line">        &quot;my_date_histo&quot;:&#123;</span><br><span class="line">            &quot;date_histogram&quot;:&#123;</span><br><span class="line">                &quot;field&quot;:&quot;date&quot;,</span><br><span class="line">                &quot;interval&quot;:&quot;1M&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;aggs&quot;:&#123;</span><br><span class="line">                &quot;the_sum&quot;:&#123;</span><br><span class="line">                    &quot;sum&quot;:&#123; &quot;field&quot;: &quot;price&quot; &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                &quot;the_movavg&quot;:&#123;</span><br><span class="line">                    &quot;moving_avg&quot;:&#123;</span><br><span class="line">                        &quot;buckets_path&quot;: &quot;the_sum&quot;,</span><br><span class="line">                        &quot;window&quot; : 30,</span><br><span class="line">                        &quot;model&quot; : &quot;simple&quot;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">&apos;</span><br></pre></td></tr></table></figure></p><h2 id="线性模型：Linear"><a href="#线性模型：Linear" class="headerlink" title="线性模型：Linear"></a>线性模型：Linear</h2><p>对窗口内的值先做线性变换处理，再求平均：(a1 <em> 1 + a2 </em> 2 + … + an * n) / (1 + 2 + … + n)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">curl -XPOST &apos;localhost:9200/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class="line">&#123;</span><br><span class="line">    &quot;size&quot;: 0,</span><br><span class="line">    &quot;aggs&quot;: &#123;</span><br><span class="line">        &quot;my_date_histo&quot;:&#123;</span><br><span class="line">            &quot;date_histogram&quot;:&#123;</span><br><span class="line">                &quot;field&quot;:&quot;date&quot;,</span><br><span class="line">                &quot;interval&quot;:&quot;1M&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;aggs&quot;:&#123;</span><br><span class="line">                &quot;the_sum&quot;:&#123;</span><br><span class="line">                    &quot;sum&quot;:&#123; &quot;field&quot;: &quot;price&quot; &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                &quot;the_movavg&quot;: &#123;</span><br><span class="line">                    &quot;moving_avg&quot;:&#123;</span><br><span class="line">                        &quot;buckets_path&quot;: &quot;the_sum&quot;,</span><br><span class="line">                        &quot;window&quot; : 30,</span><br><span class="line">                        &quot;model&quot; : &quot;linear&quot;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">&apos;</span><br></pre></td></tr></table></figure><h2 id="指数平滑模型"><a href="#指数平滑模型" class="headerlink" title="指数平滑模型"></a>指数平滑模型</h2><h3 id="指数模型：EWMA-Exponentially-Weighted"><a href="#指数模型：EWMA-Exponentially-Weighted" class="headerlink" title="指数模型：EWMA (Exponentially Weighted)"></a>指数模型：EWMA (Exponentially Weighted)</h3><p>即： 一次指数平滑模型</p><p>EWMA模型通常也成为单指数模型（single-exponential）, 和线性模型的思路类似，离当前点越远的点，重要性越低，具体化为数值的指数下降，对应的参数是alpha。 alpha值越小，下降越慢。（估计是用1 - alpha去计算的）默认的alpha=0.3</p><p>计算模型：s2 = α <em> x2 + (1 - α) </em> s1</p><p>其中α是平滑系数，si是之前i个数据的平滑值，α取值为[0,1]，越接近1，平滑后的值越接近当前时间的数据值，数据越不平滑，α越接近0，平滑后的值越接近前i个数据的平滑值，数据越平滑，α的值通常可以多尝试几次以达到最佳效果。 一次指数平滑算法进行预测的公式为：xi+h=si，其中i为当前最后的一个数据记录的坐标，亦即预测的时间序列为一条直线，不能反映时间序列的趋势和季节性。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">curl -XPOST &apos;localhost:9200/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class="line">&#123;</span><br><span class="line">    &quot;size&quot;: 0,</span><br><span class="line">    &quot;aggs&quot;: &#123;</span><br><span class="line">        &quot;my_date_histo&quot;:&#123;</span><br><span class="line">            &quot;date_histogram&quot;:&#123;</span><br><span class="line">                &quot;field&quot;:&quot;date&quot;,</span><br><span class="line">                &quot;interval&quot;:&quot;1M&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;aggs&quot;:&#123;</span><br><span class="line">                &quot;the_sum&quot;:&#123;</span><br><span class="line">                    &quot;sum&quot;:&#123; &quot;field&quot;: &quot;price&quot; &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                &quot;the_movavg&quot;: &#123;</span><br><span class="line">                    &quot;moving_avg&quot;:&#123;</span><br><span class="line">                        &quot;buckets_path&quot;: &quot;the_sum&quot;,</span><br><span class="line">                        &quot;window&quot; : 30,</span><br><span class="line">                        &quot;model&quot; : &quot;ewma&quot;,</span><br><span class="line">                        &quot;settings&quot; : &#123;</span><br><span class="line">                            &quot;alpha&quot; : 0.5</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">&apos;</span><br></pre></td></tr></table></figure><h3 id="二次指数平滑模型-Holt-Linear"><a href="#二次指数平滑模型-Holt-Linear" class="headerlink" title="二次指数平滑模型: Holt-Linear"></a>二次指数平滑模型: Holt-Linear</h3><p>计算模型：</p><p>s2 = α <em> x2 + (1 - α) </em> (s1 + t1)</p><p>t2 = ß <em> (s2 - s1) + (1 - ß) </em> t1</p><p>默认alpha = 0.3 and beta = 0.1</p><p>二次指数平滑保留了趋势的信息，使得预测的时间序列可以包含之前数据的趋势。二次指数平滑的预测公式为 xi+h=si+hti 二次指数平滑的预测结果是一条斜的直线。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">curl -XPOST &apos;localhost:9200/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class="line">&#123;</span><br><span class="line">    &quot;size&quot;: 0,</span><br><span class="line">    &quot;aggs&quot;: &#123;</span><br><span class="line">        &quot;my_date_histo&quot;:&#123;</span><br><span class="line">            &quot;date_histogram&quot;:&#123;</span><br><span class="line">                &quot;field&quot;:&quot;date&quot;,</span><br><span class="line">                &quot;interval&quot;:&quot;1M&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;aggs&quot;:&#123;</span><br><span class="line">                &quot;the_sum&quot;:&#123;</span><br><span class="line">                    &quot;sum&quot;:&#123; &quot;field&quot;: &quot;price&quot; &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                &quot;the_movavg&quot;: &#123;</span><br><span class="line">                    &quot;moving_avg&quot;:&#123;</span><br><span class="line">                        &quot;buckets_path&quot;: &quot;the_sum&quot;,</span><br><span class="line">                        &quot;window&quot; : 30,</span><br><span class="line">                        &quot;model&quot; : &quot;holt&quot;,</span><br><span class="line">                        &quot;settings&quot; : &#123;</span><br><span class="line">                            &quot;alpha&quot; : 0.5,</span><br><span class="line">                            &quot;beta&quot; : 0.5</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">&apos;</span><br></pre></td></tr></table></figure><h3 id="三次指数平滑模型：Holt-Winters无季节模型"><a href="#三次指数平滑模型：Holt-Winters无季节模型" class="headerlink" title="三次指数平滑模型：Holt-Winters无季节模型"></a>三次指数平滑模型：Holt-Winters无季节模型</h3><p>三次指数平滑在二次指数平滑的基础上保留了季节性的信息，使得其可以预测带有季节性的时间序列。三次指数平滑添加了一个新的参数p来表示平滑后的趋势。</p><p>1: Additive Holt-Winters：Holt-Winters加法模型</p><p>下面是累加的三次指数平滑<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">si=α(xi-pi-k)+(1-α)(si-1+ti-1)</span><br><span class="line">ti=ß(si-si-1)+(1-ß)ti-1</span><br><span class="line">pi=γ(xi-si)+(1-γ)pi-k</span><br></pre></td></tr></table></figure></p><p>其中k为周期</p><p>累加三次指数平滑的预测公式为： xi+h=si+hti+pi-k+(h mod k)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">curl -XPOST &apos;localhost:9200/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class="line">&#123;</span><br><span class="line">    &quot;size&quot;: 0,</span><br><span class="line">    &quot;aggs&quot;: &#123;</span><br><span class="line">        &quot;my_date_histo&quot;:&#123;</span><br><span class="line">            &quot;date_histogram&quot;:&#123;</span><br><span class="line">                &quot;field&quot;:&quot;date&quot;,</span><br><span class="line">                &quot;interval&quot;:&quot;1M&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;aggs&quot;:&#123;</span><br><span class="line">                &quot;the_sum&quot;:&#123;</span><br><span class="line">                    &quot;sum&quot;:&#123; &quot;field&quot;: &quot;price&quot; &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                &quot;the_movavg&quot;: &#123;</span><br><span class="line">                    &quot;moving_avg&quot;:&#123;</span><br><span class="line">                        &quot;buckets_path&quot;: &quot;the_sum&quot;,</span><br><span class="line">                        &quot;window&quot; : 30,</span><br><span class="line">                        &quot;model&quot; : &quot;holt_winters&quot;,</span><br><span class="line">                        &quot;settings&quot; : &#123;</span><br><span class="line">                            &quot;type&quot; : &quot;add&quot;,</span><br><span class="line">                            &quot;alpha&quot; : 0.5,</span><br><span class="line">                            &quot;beta&quot; : 0.5,</span><br><span class="line">                            &quot;gamma&quot; : 0.5,</span><br><span class="line">                            &quot;period&quot; : 7</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">&apos;</span><br></pre></td></tr></table></figure><p>2: Multiplicative Holt-Winters：Holt-Winters乘法模型</p><p>下式为累乘的三次指数平滑：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">si=αxi/pi-k+(1-α)(si-1+ti-1)</span><br><span class="line">ti=ß(si-si-1)+(1-ß)ti-1</span><br><span class="line">pi=γxi/si+(1-γ)pi-k  其中k为周期</span><br></pre></td></tr></table></figure></p><p>累乘三次指数平滑的预测公式为： xi+h=(si+hti)pi-k+(h mod k)</p><p>α，ß，γ的值都位于[0,1]之间，可以多试验几次以达到最佳效果。</p><p>s,t,p初始值的选取对于算法整体的影响不是特别大，通常的取值为s0=x0,t0=x1-x0,累加时p=0,累乘时p=1.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">curl -XPOST &apos;localhost:9200/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class="line">&#123;</span><br><span class="line">    &quot;size&quot;: 0,</span><br><span class="line">    &quot;aggs&quot;: &#123;</span><br><span class="line">        &quot;my_date_histo&quot;:&#123;</span><br><span class="line">            &quot;date_histogram&quot;:&#123;</span><br><span class="line">                &quot;field&quot;:&quot;date&quot;,</span><br><span class="line">                &quot;interval&quot;:&quot;1M&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;aggs&quot;:&#123;</span><br><span class="line">                &quot;the_sum&quot;:&#123;</span><br><span class="line">                    &quot;sum&quot;:&#123; &quot;field&quot;: &quot;price&quot; &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                &quot;the_movavg&quot;: &#123;</span><br><span class="line">                    &quot;moving_avg&quot;:&#123;</span><br><span class="line">                        &quot;buckets_path&quot;: &quot;the_sum&quot;,</span><br><span class="line">                        &quot;window&quot; : 30,</span><br><span class="line">                        &quot;model&quot; : &quot;holt_winters&quot;,</span><br><span class="line">                        &quot;settings&quot; : &#123;</span><br><span class="line">                            &quot;type&quot; : &quot;mult&quot;,</span><br><span class="line">                            &quot;alpha&quot; : 0.5,</span><br><span class="line">                            &quot;beta&quot; : 0.5,</span><br><span class="line">                            &quot;gamma&quot; : 0.5,</span><br><span class="line">                            &quot;period&quot; : 7,</span><br><span class="line">                            &quot;pad&quot; : true</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">&apos;</span><br></pre></td></tr></table></figure><h2 id="预测模型：Prediction"><a href="#预测模型：Prediction" class="headerlink" title="预测模型：Prediction"></a>预测模型：Prediction</h2><p>使用当前值减去前一个值，其实就是环比增长</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">curl -XPOST &apos;localhost:9200/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class="line">&#123;</span><br><span class="line">    &quot;size&quot;: 0,</span><br><span class="line">    &quot;aggs&quot;: &#123;</span><br><span class="line">        &quot;my_date_histo&quot;:&#123;</span><br><span class="line">            &quot;date_histogram&quot;:&#123;</span><br><span class="line">                &quot;field&quot;:&quot;date&quot;,</span><br><span class="line">                &quot;interval&quot;:&quot;1M&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;aggs&quot;:&#123;</span><br><span class="line">                &quot;the_sum&quot;:&#123;</span><br><span class="line">                    &quot;sum&quot;:&#123; &quot;field&quot;: &quot;price&quot; &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                &quot;the_movavg&quot;: &#123;</span><br><span class="line">                    &quot;moving_avg&quot;:&#123;</span><br><span class="line">                        &quot;buckets_path&quot;: &quot;the_sum&quot;,</span><br><span class="line">                        &quot;window&quot; : 30,</span><br><span class="line">                        &quot;model&quot; : &quot;simple&quot;,</span><br><span class="line">                        &quot;predict&quot; : 10</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">&apos;</span><br></pre></td></tr></table></figure><h2 id="最小化：Minimization"><a href="#最小化：Minimization" class="headerlink" title="最小化：Minimization"></a>最小化：Minimization</h2><p>某些模型（EWMA，Holt-Linear，Holt-Winters）需要配置一个或多个参数。参数选择可能会非常棘手，有时不直观。此外，这些参数的小偏差有时会对输出移动平均线产生剧烈的影响。</p><p>出于这个原因，三个“可调”模型可以在算法上最小化。最小化是一个参数调整的过程，直到模型生成的预测与输出数据紧密匹配为止。最小化并不是完全防护的，并且可能容易过度配合，但是它往往比手动调整有更好的结果。</p><p>ewma和holt_linear默认情况下禁用最小化，而holt_winters默认启用最小化。 Holt-Winters最小化是最有用的，因为它有助于提高预测的准确性。 EWMA和Holt-Linear不是很好的预测指标，主要用于平滑数据，所以最小化对于这些模型来说不太有用。</p><p>通过最小化参数启用/禁用最小化：”minimize” : true</p><h1 id="原始数据"><a href="#原始数据" class="headerlink" title="原始数据"></a>原始数据</h1><p>数据为SSH login数据其中 IP／user已处理<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;_index&quot;: &quot;logstash-sshlogin-others-success-2017-10&quot;,</span><br><span class="line">    &quot;_type&quot;: &quot;sshlogin&quot;,</span><br><span class="line">    &quot;_id&quot;: &quot;AV-weLF8c2nHCDojUbat&quot;,</span><br><span class="line">    &quot;_version&quot;: 2,</span><br><span class="line">    &quot;_score&quot;: 1,</span><br><span class="line">    &quot;_source&quot;: &#123;</span><br><span class="line">        &quot;srcip&quot;: &quot;222.221.238.162&quot;,</span><br><span class="line">        &quot;dstport&quot;: &quot;&quot;,</span><br><span class="line">        &quot;pid&quot;: &quot;20604&quot;,</span><br><span class="line">        &quot;program&quot;: &quot;sshd&quot;,</span><br><span class="line">        &quot;message&quot;: &quot;dwasw-ibb01:Oct 19 23:38:02 176.231.228.130 sshd[20604]: Accepted publickey for nmuser from 222.221.238.162 port 49484 ssh2&quot;,</span><br><span class="line">        &quot;type&quot;: &quot;zhongcai-sshlogin&quot;,</span><br><span class="line">        &quot;ssh_type&quot;: &quot;ssh_successful_login&quot;,</span><br><span class="line">        &quot;forwarded&quot;: &quot;false&quot;,</span><br><span class="line">        &quot;manufacturer&quot;: &quot;others&quot;,</span><br><span class="line">        &quot;IndexTime&quot;: &quot;2017-10&quot;,</span><br><span class="line">        &quot;path&quot;: &quot;/home/logstash/log/logstash_data/audit10/sshlogin/11.txt&quot;,</span><br><span class="line">        &quot;number&quot;: 1,</span><br><span class="line">        &quot;hostname&quot;: &quot;176.231.228.130&quot;,</span><br><span class="line">        &quot;protocol&quot;: &quot;ssh2&quot;,</span><br><span class="line">        &quot;@timestamp&quot;: &quot;2017-10-19T15:38:02.000Z&quot;,</span><br><span class="line">        &quot;ssh_method&quot;: &quot;publickey&quot;,</span><br><span class="line">        &quot;_hostname&quot;: &quot;dwasw-ibb01&quot;,</span><br><span class="line">        &quot;@version&quot;: &quot;1&quot;,</span><br><span class="line">        &quot;host&quot;: &quot;localhost&quot;,</span><br><span class="line">        &quot;srcport&quot;: &quot;49484&quot;,</span><br><span class="line">        &quot;dstip&quot;: &quot;&quot;,</span><br><span class="line">        &quot;category&quot;: &quot;sshlogin&quot;,</span><br><span class="line">        &quot;user&quot;: &quot;nmuser&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="利用ES-API接口去调用查询数据"><a href="#利用ES-API接口去调用查询数据" class="headerlink" title="利用ES API接口去调用查询数据"></a>利用ES API接口去调用查询数据</h1><p>“interval”: “hour”: hour为单位，这里可以是分钟，小时，天，周，月</p><p>“format”: “yyyy-MM-dd HH”: 聚合结果得日期格式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&quot;the_sum&quot;: &#123;</span><br><span class="line">    &quot;sum&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;number&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>number为要聚合得字段</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">curl -POST  &apos;localhost:9200/logstash-sshlogin-others-success-2017-10/sshlogin/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;size&quot;: 0,</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;term&quot;: &#123;</span><br><span class="line">      &quot;ssh_type&quot;: &quot;ssh_successful_login&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;aggs&quot;: &#123;</span><br><span class="line">    &quot;hour_sum&quot;: &#123;</span><br><span class="line">      &quot;date_histogram&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;@timestamp&quot;,</span><br><span class="line">        &quot;interval&quot;: &quot;hour&quot;,</span><br><span class="line">        &quot;format&quot;: &quot;yyyy-MM-dd HH&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;aggs&quot;: &#123;</span><br><span class="line">        &quot;the_sum&quot;: &#123;</span><br><span class="line">          &quot;sum&quot;: &#123;</span><br><span class="line">            &quot;field&quot;: &quot;number&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;the_movavg&quot;: &#123;</span><br><span class="line">          &quot;moving_avg&quot;: &#123;</span><br><span class="line">            &quot;buckets_path&quot;: &quot;the_sum&quot;,</span><br><span class="line">            &quot;window&quot;: 30,</span><br><span class="line">            &quot;model&quot;: &quot;holt&quot;,</span><br><span class="line">            &quot;settings&quot;: &#123;</span><br><span class="line">              &quot;alpha&quot;: 0.5,</span><br><span class="line">              &quot;beta&quot;: 0.7</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;&apos;</span><br></pre></td></tr></table></figure><p>得到的结果形式为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;took&quot; : 35,</span><br><span class="line">  &quot;timed_out&quot; : false,</span><br><span class="line">  &quot;_shards&quot; : &#123;</span><br><span class="line">    &quot;total&quot; : 1,</span><br><span class="line">    &quot;successful&quot; : 1,</span><br><span class="line">    &quot;failed&quot; : 0</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;hits&quot; : &#123;</span><br><span class="line">    &quot;total&quot; : 206821,</span><br><span class="line">    &quot;max_score&quot; : 0.0,</span><br><span class="line">    &quot;hits&quot; : [ ]</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;aggregations&quot; : &#123;</span><br><span class="line">    &quot;hour_sum&quot; : &#123;</span><br><span class="line">      &quot;buckets&quot; : [</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;key_as_string&quot; : &quot;2017-09-30 16&quot;,</span><br><span class="line">          &quot;key&quot; : 1506787200000,</span><br><span class="line">          &quot;doc_count&quot; : 227,</span><br><span class="line">          &quot;the_sum&quot; : &#123;</span><br><span class="line">            &quot;value&quot; : 227.0</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;key_as_string&quot; : &quot;2017-09-30 17&quot;,</span><br><span class="line">          &quot;key&quot; : 1506790800000,</span><br><span class="line">          &quot;doc_count&quot; : 210,</span><br><span class="line">          &quot;the_sum&quot; : &#123;</span><br><span class="line">            &quot;value&quot; : 210.0</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;the_movavg&quot; : &#123;</span><br><span class="line">            &quot;value&quot; : 113.5</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;key_as_string&quot; : &quot;2017-09-30 18&quot;,</span><br><span class="line">          &quot;key&quot; : 1506794400000,</span><br><span class="line">          &quot;doc_count&quot; : 365,</span><br><span class="line">          &quot;the_sum&quot; : &#123;</span><br><span class="line">            &quot;value&quot; : 365.0</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;the_movavg&quot; : &#123;</span><br><span class="line">            &quot;value&quot; : 210.0</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">    ...</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="对应得python代码（查询数据到画图）"><a href="#对应得python代码（查询数据到画图）" class="headerlink" title="对应得python代码（查询数据到画图）"></a>对应得python代码（查询数据到画图）</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"># coding: utf-8</span><br><span class="line">from elasticsearch import Elasticsearch</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from matplotlib.font_manager import FontManager, FontProperties</span><br><span class="line"></span><br><span class="line">class Smooth:</span><br><span class="line">    def __init__(self,index):</span><br><span class="line">        self.es = Elasticsearch([&apos;localhost:9200&apos;])</span><br><span class="line">        self.index = index</span><br><span class="line">        </span><br><span class="line">    # 处理mac中文编码错误</span><br><span class="line">    def getChineseFont(self):</span><br><span class="line">        return FontProperties(fname=&apos;/System/Library/Fonts/PingFang.ttc&apos;)</span><br><span class="line">    </span><br><span class="line">    # 对index进行聚合</span><br><span class="line">    def agg(self):</span><br><span class="line">        # &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:SS&quot;</span><br><span class="line">        dsl = &apos;&apos;&apos;</span><br><span class="line">                &#123;</span><br><span class="line">                  &quot;size&quot;: 0,</span><br><span class="line">                  &quot;query&quot;: &#123;</span><br><span class="line">                    &quot;term&quot;: &#123;</span><br><span class="line">                      &quot;ssh_type&quot;: &quot;ssh_successful_login&quot;</span><br><span class="line">                    &#125;</span><br><span class="line">                  &#125;,</span><br><span class="line">                  &quot;aggs&quot;: &#123;</span><br><span class="line">                    &quot;hour_sum&quot;: &#123;</span><br><span class="line">                      &quot;date_histogram&quot;: &#123;</span><br><span class="line">                        &quot;field&quot;: &quot;@timestamp&quot;,</span><br><span class="line">                        &quot;interval&quot;: &quot;day&quot;,</span><br><span class="line">                        &quot;format&quot;: &quot;dd&quot;</span><br><span class="line">                      &#125;,</span><br><span class="line">                      &quot;aggs&quot;: &#123;</span><br><span class="line">                        &quot;the_sum&quot;: &#123;</span><br><span class="line">                          &quot;sum&quot;: &#123;</span><br><span class="line">                            &quot;field&quot;: &quot;number&quot;</span><br><span class="line">                          &#125;</span><br><span class="line">                        &#125;,</span><br><span class="line">                        &quot;the_movavg&quot;: &#123;</span><br><span class="line">                          &quot;moving_avg&quot;: &#123;</span><br><span class="line">                            &quot;buckets_path&quot;: &quot;the_sum&quot;,</span><br><span class="line">                            &quot;window&quot;: 30,</span><br><span class="line">                            &quot;model&quot;: &quot;holt_winters&quot;,</span><br><span class="line">                            &quot;settings&quot;: &#123;</span><br><span class="line">                              &quot;alpha&quot;: 0.5,</span><br><span class="line">                              &quot;beta&quot;: 0.7</span><br><span class="line">                            &#125;</span><br><span class="line">                          &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                      &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                  &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                &apos;&apos;&apos;</span><br><span class="line">        res = self.es.search(index=self.index, body=dsl)</span><br><span class="line">        return res[&apos;aggregations&apos;][&apos;hour_sum&apos;][&apos;buckets&apos;]</span><br><span class="line">    </span><br><span class="line">    # 画图</span><br><span class="line">    def draw(self):</span><br><span class="line">        x,y_true,y_pred = [],[],[]</span><br><span class="line">        for one in self.agg():</span><br><span class="line">            x.append(one[&apos;key_as_string&apos;])</span><br><span class="line">            y_true.append(one[&apos;the_sum&apos;][&apos;value&apos;])</span><br><span class="line">            if &apos;the_movavg&apos; in one.keys():       # 前几条数据没有 the_movavg 字段，故将真实值赋值给pred值</span><br><span class="line">                y_pred.append(one[&apos;the_movavg&apos;][&apos;value&apos;])</span><br><span class="line">            else:</span><br><span class="line">                y_pred.append(one[&apos;the_sum&apos;][&apos;value&apos;])</span><br><span class="line">        </span><br><span class="line">        x_line = range(len(x))</span><br><span class="line">        </span><br><span class="line">        plt.figure(figsize=(10,5))</span><br><span class="line">        plt.plot(x_line,y_true,color=&quot;r&quot;)</span><br><span class="line">        plt.plot(x_line,y_pred,color=&quot;g&quot;)</span><br><span class="line">        </span><br><span class="line">        plt.xlabel(u&quot;每单位时间&quot;,fontproperties=self.getChineseFont()) #X轴标签 </span><br><span class="line">        plt.xticks(range(len(x)), x)</span><br><span class="line">        plt.ylabel(u&quot;聚合结果&quot;,fontproperties=self.getChineseFont()) #Y轴标签  </span><br><span class="line">        plt.title(u&quot;10月份 SSH 主机登录成功聚合图&quot;,fontproperties=self.getChineseFont()) # 标题</span><br><span class="line">        plt.legend([u&quot;True value&quot;,u&quot;Predict value&quot;])</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line">smooth = Smooth(&quot;logstash-sshlogin-others-success-2017-10&quot;)</span><br><span class="line">print smooth.draw()</span><br></pre></td></tr></table></figure><p>结果图示为：<br><img src="http://img.blog.csdn.net/20171120171404972?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;指数平滑法是一种特殊的加权平均法，加权的特点是对离预测值较近的历史数据给予较大的权数，对离预测期较远的历史数据给予较小的权数，权数由近到远按指数规律递减，所以，这种预测方法被称为指数平滑法。它可分为一次指数平滑法、二次指数平滑法及更高次指数平滑法。&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="ELK" scheme="http://yoursite.com/tags/ELK/"/>
    
      <category term="ES" scheme="http://yoursite.com/tags/ES/"/>
    
      <category term="异常检测" scheme="http://yoursite.com/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch-DSL部分集合</title>
    <link href="http://yoursite.com/2017/11/14/ELK/Elasticsearch-DSL%E9%83%A8%E5%88%86%E9%9B%86%E5%90%88/"/>
    <id>http://yoursite.com/2017/11/14/ELK/Elasticsearch-DSL部分集合/</id>
    <published>2017-11-14T09:26:48.000Z</published>
    <updated>2018-03-17T17:20:45.563Z</updated>
    
    <content type="html"><![CDATA[<p>ELK是日志收集分析神器，在这篇文章中将会介绍一些ES的常用命令。</p><p>点击阅读：<a href="http://blog.csdn.net/column/details/13079.html" target="_blank" rel="external">ELK Stack 从入门到放弃</a><br><a id="more"></a></p><h1 id="DSL中遇到的错误及解决办法"><a href="#DSL中遇到的错误及解决办法" class="headerlink" title="DSL中遇到的错误及解决办法"></a>DSL中遇到的错误及解决办法</h1><h2 id="分片限制错误"><a href="#分片限制错误" class="headerlink" title="分片限制错误"></a>分片限制错误</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Trying to query 2632 shards, which is over the limit of 1000. This limit exists because querying many shards at the same time can make the job of the coordinating node very CPU and/or memory intensive. It is usually a better idea to have a smaller number of larger shards. Update [action.search.shard_count.limit] to a greater value if you really want to query that many shards at the same time.</span><br></pre></td></tr></table></figure><p>解决办法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">修改该限制数目</span><br><span class="line"></span><br><span class="line">curl -k -u admin:admin -XPUT &apos;http://localhost:9200/_cluster/settings&apos; -H &apos;Content-Type: application/json&apos; -d&apos; </span><br><span class="line">&#123;</span><br><span class="line">    &quot;persistent&quot; : &#123;</span><br><span class="line">        &quot;action.search.shard_count.limit&quot; : &quot;5000&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">&apos;</span><br><span class="line"></span><br><span class="line">-k -u admin:admin 表述如果有权限保护的话可以加上</span><br></pre></td></tr></table></figure></p><h2 id="Fileddate-错误"><a href="#Fileddate-错误" class="headerlink" title="Fileddate 错误"></a>Fileddate 错误</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Fielddata is disabled on text fields by default. Set fielddata=true on [make] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.</span><br></pre></td></tr></table></figure><p>解决办法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cars: 索引名</span><br><span class="line">transactions：索引对应的类型</span><br><span class="line">color：字段</span><br><span class="line"></span><br><span class="line">curl -XPUT -k -u admin:admin &apos;localhost:9200/cars/_mapping/transactions?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;properties&quot;: &#123;</span><br><span class="line">    &quot;color&quot;: &#123; </span><br><span class="line">      &quot;type&quot;:     &quot;text&quot;,</span><br><span class="line">      &quot;fielddata&quot;: true</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">&apos;</span><br></pre></td></tr></table></figure></p><h1 id="指定关键词查询，排序和函数统计"><a href="#指定关键词查询，排序和函数统计" class="headerlink" title="指定关键词查询，排序和函数统计"></a>指定关键词查询，排序和函数统计</h1><h2 id="指定关键词"><a href="#指定关键词" class="headerlink" title="指定关键词"></a>指定关键词</h2><p>from 为首个偏移量，size为返回数据的条数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">http://10.10.11.139:9200/logstash-nginx-access-*/nginx-access/_search?pretty</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;from&quot;:0,size&quot;:1000,</span><br><span class="line">    &quot;query&quot; : &#123;</span><br><span class="line">        &quot;term&quot; : &#123; </span><br><span class="line">        &quot;major&quot; : &quot;55&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="添加排序"><a href="#添加排序" class="headerlink" title="添加排序"></a>添加排序</h2><p>(需要进行mapping设置，asc 为升序  desc为降序)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;from&quot;:0,&quot;size&quot;:1000,</span><br><span class="line">    &quot;sort&quot;:[</span><br><span class="line">        &#123;&quot;offset&quot;:&quot;desc&quot;&#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;query&quot; : &#123;</span><br><span class="line">        &quot;term&quot; : &#123; </span><br><span class="line">            &quot;major&quot; : &quot;55&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="mode-方法"><a href="#mode-方法" class="headerlink" title="mode 方法"></a>mode 方法</h2><p>mode方法包括 min／max／avg／sum／median</p><p>假如现在要对price字段进行排序，但是price字段有多个值，这个时候就可以使用mode 方法了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">   &quot;query&quot; : &#123;</span><br><span class="line">      &quot;term&quot; : &#123; &quot;product&quot; : &quot;chocolate&quot; &#125;</span><br><span class="line">   &#125;,</span><br><span class="line">   &quot;sort&quot; : [</span><br><span class="line">      &#123;&quot;price&quot; : &#123;&quot;order&quot; : &quot;asc&quot;, &quot;mode&quot; : &quot;avg&quot;&#125;&#125;</span><br><span class="line">   ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="IP范围和网段查询"><a href="#IP范围和网段查询" class="headerlink" title="IP范围和网段查询"></a>IP范围和网段查询</h1><h2 id="IP-range-搜索"><a href="#IP-range-搜索" class="headerlink" title="IP range 搜索"></a>IP range 搜索</h2><p>错误：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Fielddata is disabled on text fields by default. Set fielddata=true on [clientip] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.</span><br></pre></td></tr></table></figure></p><p>解决办法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">curl -k -u admin:admin -XPUT &apos;10.10.11.139:9200/logstash-nginx-access-*/_mapping/nginx-access?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;properties&quot;: &#123;</span><br><span class="line">    &quot;clientip&quot;: &#123; </span><br><span class="line">      &quot;type&quot;:     &quot;text&quot;,</span><br><span class="line">      &quot;fielddata&quot;: true,</span><br><span class="line">      &quot;norms&quot;: false</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">&apos;</span><br></pre></td></tr></table></figure></p><p>查看某个索引的mapping<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -k -u admin:admin -XGET http://10.10.11.139:9200/logstash-nginx-access-*/_mapping?pretty</span><br></pre></td></tr></table></figure></p><p>(当IP为不可解析使就会出现错误)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">http://10.10.11.139:9200/logstash-sshlogin-others-success-*/zhongcai/_search?pretty</span><br><span class="line">&#123;</span><br><span class="line">    &quot;size&quot;:100,</span><br><span class="line">    &quot;aggs&quot; : &#123;</span><br><span class="line">        &quot;ip_ranges&quot; : &#123;</span><br><span class="line">            &quot;ip_range&quot; : &#123;</span><br><span class="line">                &quot;field&quot; : &quot;clientip&quot;,</span><br><span class="line">                &quot;ranges&quot; : [</span><br><span class="line">                    &#123; &quot;to&quot; : &quot;40.77.167.73&quot; &#125;,</span><br><span class="line">                    &#123; &quot;from&quot; : &quot;40.77.167.75&quot; &#125;</span><br><span class="line">                ]</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="网段查询"><a href="#网段查询" class="headerlink" title="网段查询"></a>网段查询</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">http://10.10.11.139:9200/logstash-sshlogin-others-success-*/zhongcai/_search?pretty</span><br><span class="line">&#123;</span><br><span class="line">    &quot;aggs&quot; : &#123;</span><br><span class="line">        &quot;ip_ranges&quot; : &#123;</span><br><span class="line">            &quot;ip_range&quot; : &#123;</span><br><span class="line">                &quot;field&quot; : &quot;ip&quot;,</span><br><span class="line">                &quot;ranges&quot; : [</span><br><span class="line">                    &#123; &quot;mask&quot; : &quot;172.21.202.0/24&quot; &#125;,</span><br><span class="line">                    &#123; &quot;mask&quot; : &quot;172.21.202.0/24&quot; &#125;</span><br><span class="line">                ]</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="关于索引的操作"><a href="#关于索引的操作" class="headerlink" title="关于索引的操作"></a>关于索引的操作</h1><h2 id="删除某个索引"><a href="#删除某个索引" class="headerlink" title="删除某个索引"></a>删除某个索引</h2><p>-k -u admin:admin 为用户名：密码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -XDELETE  -k -u admin:admin &apos;http://localhost:9200/my_index&apos;</span><br></pre></td></tr></table></figure></p><h2 id="查看某个索引的Mapping"><a href="#查看某个索引的Mapping" class="headerlink" title="查看某个索引的Mapping"></a>查看某个索引的Mapping</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -XGET &quot;http://127.0.0.1:9200/my_index/_mapping?pretty&quot;</span><br></pre></td></tr></table></figure><h2 id="索引数据迁移"><a href="#索引数据迁移" class="headerlink" title="索引数据迁移"></a>索引数据迁移</h2><p>Es索引reindex(从ip_remote上迁移到本地)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">curl -XPOST &apos;localhost:9200/_reindex?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;remote&quot;: &#123;</span><br><span class="line">      &quot;host&quot;: &quot;http://ip_remote:9200&quot;,</span><br><span class="line">      &quot;username&quot;: &quot;username&quot;,</span><br><span class="line">      &quot;password&quot;: &quot;passwd&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;index&quot;: &quot;old_index&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;new_index&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">&apos;</span><br></pre></td></tr></table></figure></p><h2 id="为某个索引添加字段"><a href="#为某个索引添加字段" class="headerlink" title="为某个索引添加字段"></a>为某个索引添加字段</h2><p>添加number字段：</p><h3 id="唯一ID"><a href="#唯一ID" class="headerlink" title="唯一ID"></a>唯一ID</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">curl -POST &apos;http://127.0.0.1:9200/my_idnex/my_index_type/id/_update?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class="line">&#123;</span><br><span class="line">   &quot;doc&quot; : &#123;</span><br><span class="line">      &quot;number&quot; : 1</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line">&apos;</span><br></pre></td></tr></table></figure><h3 id="批量操作"><a href="#批量操作" class="headerlink" title="批量操作"></a>批量操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">curl -XPOST &apos;localhost:9200/logstash-sshlogin-others-success-2017-*/_update_by_query?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;script&quot;: &#123;</span><br><span class="line">    &quot;inline&quot;: &quot;ctx._source.number=1&quot;,</span><br><span class="line">    &quot;lang&quot;: &quot;painless&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;match_all&quot;: &#123;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">&apos;</span><br></pre></td></tr></table></figure><h1 id="根据指定条件进行聚合"><a href="#根据指定条件进行聚合" class="headerlink" title="根据指定条件进行聚合"></a>根据指定条件进行聚合</h1><p>每小时成功登录的次数进行聚合<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">curl -POST &apos;http://127.0.0.1:9200/logstash-sshlogin-others-success-2017-*/zhongcai-sshlogin/_search?pretty&apos; -H &apos;Content-Type: application/json&apos; -d&apos;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;term&quot;: &#123;</span><br><span class="line">      &quot;ssh_type&quot;: &quot;ssh_successful_login&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;aggs&quot;: &#123;</span><br><span class="line">    &quot;sums&quot;: &#123;</span><br><span class="line">      &quot;date_histogram&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;@timestamp&quot;,</span><br><span class="line">        &quot;interval&quot;: &quot;hour&quot;,</span><br><span class="line">        &quot;format&quot;: &quot;yyyy-MM-dd HH&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">&apos;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ELK是日志收集分析神器，在这篇文章中将会介绍一些ES的常用命令。&lt;/p&gt;
&lt;p&gt;点击阅读：&lt;a href=&quot;http://blog.csdn.net/column/details/13079.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;ELK Stack 从入门到放弃&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="ELK" scheme="http://yoursite.com/tags/ELK/"/>
    
      <category term="ES" scheme="http://yoursite.com/tags/ES/"/>
    
  </entry>
  
  <entry>
    <title>数据结构算法之链表</title>
    <link href="http://yoursite.com/2017/11/13/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AE%97%E6%B3%95%E4%B9%8B%E9%93%BE%E8%A1%A8/"/>
    <id>http://yoursite.com/2017/11/13/数据结构/数据结构算法之链表/</id>
    <published>2017-11-12T16:58:37.000Z</published>
    <updated>2018-03-17T17:11:36.783Z</updated>
    
    <content type="html"><![CDATA[<p>链表面试总结，使用python实现，参考：<a href="https://www.cnblogs.com/lixiaohui-ambition/archive/2012/09/25/2703195.html" target="_blank" rel="external">https://www.cnblogs.com/lixiaohui-ambition/archive/2012/09/25/2703195.html</a><br><a id="more"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line"></span><br><span class="line"># 定义链表</span><br><span class="line">class ListNode:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.data = None</span><br><span class="line">        self.pnext = None</span><br><span class="line"></span><br><span class="line"># 链表操作类</span><br><span class="line">class ListNode_handle:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.cur_node = None</span><br><span class="line">    </span><br><span class="line">    # 链表添加元素</span><br><span class="line">    def add(self,data):</span><br><span class="line">        ln = ListNode()</span><br><span class="line">        ln.data = data</span><br><span class="line">        </span><br><span class="line">        ln.pnext = self.cur_node</span><br><span class="line">        self.cur_node = ln</span><br><span class="line">        return ln</span><br><span class="line">    </span><br><span class="line">    # 打印链表</span><br><span class="line">    def prt(self,ln):</span><br><span class="line">        while ln:</span><br><span class="line">            print(ln.data,end=&quot;  &quot;)</span><br><span class="line">            ln = ln.pnext</span><br><span class="line">    # 逆序输出</span><br><span class="line">    def _reverse(self,ln):</span><br><span class="line">        _list = []</span><br><span class="line">        while ln:</span><br><span class="line">            _list.append(ln.data)</span><br><span class="line">            ln = ln.pnext</span><br><span class="line">        ln_2 = ListNode()</span><br><span class="line">        ln_h = ListNode_handle()</span><br><span class="line">        for i in _list:</span><br><span class="line">            ln_2 = ln_h.add(i)</span><br><span class="line">        return ln_2</span><br><span class="line">    </span><br><span class="line">    # 求链表的长度</span><br><span class="line">    def _length(self,ln):</span><br><span class="line">        _len = 0</span><br><span class="line">        while ln:</span><br><span class="line">            _len += 1</span><br><span class="line">            ln = ln.pnext</span><br><span class="line">        return _len</span><br><span class="line">    </span><br><span class="line">    # 查找指定位置的节点</span><br><span class="line">    def _find_loc(self,ln,loc):</span><br><span class="line">        _sum = 0</span><br><span class="line">        while ln and _sum != loc:</span><br><span class="line">            _sum += 1</span><br><span class="line">            ln = ln.pnext</span><br><span class="line">        return ln.data</span><br><span class="line">    </span><br><span class="line">    # 判断某个节点是否在链表中</span><br><span class="line">    def _exist(self,ln,data):</span><br><span class="line">        flag = False</span><br><span class="line">        while ln and data != ln.data:</span><br><span class="line">            ln = ln.pnext</span><br><span class="line">        return flag</span><br><span class="line"></span><br><span class="line"># 创建链表   </span><br><span class="line">ln = ListNode()</span><br><span class="line">ln_h = ListNode_handle()</span><br><span class="line">a = [1,4,2,5,8,5,7,9]</span><br><span class="line">for i in a:</span><br><span class="line">    ln = ln_h.add(i)</span><br><span class="line"></span><br><span class="line">print(&quot;正序输出...&quot;)</span><br><span class="line">ln_h.prt(ln)</span><br><span class="line"></span><br><span class="line">print(&quot;\n\n逆序输出...&quot;)</span><br><span class="line">ln_2 = ln_h._reverse(ln)</span><br><span class="line">ln_h.prt(ln_2)</span><br><span class="line"></span><br><span class="line"># 求链表ln的长度</span><br><span class="line">length = ln_h._length(ln)</span><br><span class="line">print(&quot;\n\nln的长度为:&quot;,length)</span><br><span class="line"></span><br><span class="line"># 查找链表ln中的倒数第３个节点</span><br><span class="line">data = ln_h._find_loc(ln,ln_h._length(ln)-3)</span><br><span class="line">print(&quot;\n\n倒数第三个节点为:&quot;,data)</span><br><span class="line"></span><br><span class="line"># 返回某个节点在链表中的位置</span><br><span class="line">loc = ln_h._loc(ln,5)</span><br><span class="line"></span><br><span class="line">#　判断某个节点是否在链表中</span><br><span class="line">flag = ln_h._exist(ln,5)</span><br><span class="line">print(&quot;\n\n５是否存在与链表ln中:&quot;,end=&quot; &quot;)</span><br><span class="line">if flag:</span><br><span class="line">    print(&quot;Yes&quot;)</span><br><span class="line">else:</span><br><span class="line">    print(&quot;No&quot;)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;链表面试总结，使用python实现，参考：&lt;a href=&quot;https://www.cnblogs.com/lixiaohui-ambition/archive/2012/09/25/2703195.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://www.cnblogs.com/lixiaohui-ambition/archive/2012/09/25/2703195.html&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="编程与系统" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B%E4%B8%8E%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="数据结构" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>数据结构算法之合并两个有序序列</title>
    <link href="http://yoursite.com/2017/11/13/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AE%97%E6%B3%95%E4%B9%8B%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E5%BA%8F%E5%88%97/"/>
    <id>http://yoursite.com/2017/11/13/数据结构/数据结构算法之合并两个有序序列/</id>
    <published>2017-11-12T16:55:29.000Z</published>
    <updated>2018-03-17T17:11:20.223Z</updated>
    
    <content type="html"><![CDATA[<p>有序序列的合并，python实现。<br><a id="more"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#coding:utf-8</span><br><span class="line"></span><br><span class="line">a = [2,4,6,8,10]</span><br><span class="line">b = [3,5,7,9,11,13,15]</span><br><span class="line">c = []</span><br><span class="line"></span><br><span class="line">def merge(a,b):</span><br><span class="line">    i,j = 0,0</span><br><span class="line">    while i&lt;=len(a)-1 and j&lt;=len(b)-1:</span><br><span class="line">        if a[i]&lt;b[j]:</span><br><span class="line">            c.append(a[i])</span><br><span class="line">            i+=1</span><br><span class="line">        else:</span><br><span class="line">            c.append(b[j])</span><br><span class="line">            j+=1</span><br><span class="line">    if i&lt;=len(a)-1:</span><br><span class="line">        for m in a[i:]:</span><br><span class="line">            c.append(m)</span><br><span class="line">    </span><br><span class="line">    if j&lt;=len(b)-1:</span><br><span class="line">        for n in b[j:]:</span><br><span class="line">            c.append(n)</span><br><span class="line">    print(c)</span><br><span class="line"></span><br><span class="line">merge(a,b)</span><br></pre></td></tr></table></figure><p>运行结果为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15]</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;有序序列的合并，python实现。&lt;br&gt;
    
    </summary>
    
      <category term="编程与系统" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B%E4%B8%8E%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="数据结构" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>数据结构算法之排序</title>
    <link href="http://yoursite.com/2017/11/13/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AE%97%E6%B3%95%E4%B9%8B%E6%8E%92%E5%BA%8F/"/>
    <id>http://yoursite.com/2017/11/13/数据结构/数据结构算法之排序/</id>
    <published>2017-11-12T16:51:28.000Z</published>
    <updated>2018-03-17T17:11:30.123Z</updated>
    
    <content type="html"><![CDATA[<p>数据结构面试中经常会被问到篇排序相关的问题，那么这篇文章会研究下怎么用python来实现排序。</p><a id="more"></a><h1 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#coding：utf-8</span><br><span class="line"></span><br><span class="line"># 冒泡排序</span><br><span class="line">def maopao():</span><br><span class="line">    a = [2,1,4,3,9,5,6,8,7]</span><br><span class="line">    for i in range(len(a)-1):</span><br><span class="line">        for j in range(len(a)-1-i):</span><br><span class="line">            if a[j]&gt;a[j+1]:</span><br><span class="line">                temp = a[j]</span><br><span class="line">                a[j] = a[j+1]</span><br><span class="line">                a[j+1] = temp</span><br><span class="line">    print(a)</span><br><span class="line">maopao()</span><br></pre></td></tr></table></figure><p>结果为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br></pre></td></tr></table></figure></p><hr><h1 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"># 归并排序</span><br><span class="line">def merge(a,b):</span><br><span class="line">    i,j = 0,0</span><br><span class="line">    c = []</span><br><span class="line">    while i&lt;=len(a)-1 and j&lt;=len(b)-1:</span><br><span class="line">        if a[i]&lt;b[j]:</span><br><span class="line">            c.append(a[i])</span><br><span class="line">            i+=1</span><br><span class="line">        else:</span><br><span class="line">            c.append(b[j])</span><br><span class="line">            j+=1</span><br><span class="line">    if i&lt;=len(a)-1:</span><br><span class="line">        for m in a[i:]:</span><br><span class="line">            c.append(m)</span><br><span class="line">    </span><br><span class="line">    if j&lt;=len(b)-1:</span><br><span class="line">        for n in b[j:]:</span><br><span class="line">            c.append(n)</span><br><span class="line">    return c</span><br><span class="line"></span><br><span class="line">def guibing(a):</span><br><span class="line">    if len(a)&lt;=1:</span><br><span class="line">        return a</span><br><span class="line">    center = int(len(a)/2)</span><br><span class="line">    left = guibing(a[:center])</span><br><span class="line">    right = guibing(a[center:])</span><br><span class="line">    return merge(left,right)</span><br><span class="line"></span><br><span class="line">print(guibing([2,1,4,3,9,5,6,8,7]))</span><br></pre></td></tr></table></figure><p>结果为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br></pre></td></tr></table></figure></p><hr><h1 id="快速排序"><a href="#快速排序" class="headerlink" title="　快速排序"></a>　快速排序</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">#　快速排序    </span><br><span class="line">def kpsort(left,right,a):</span><br><span class="line">    based = a[left]</span><br><span class="line">    i = left</span><br><span class="line">    j = right</span><br><span class="line">    while i &lt; j:</span><br><span class="line">        # 从数组右边开始遍历</span><br><span class="line">        while a[j]&gt;=based and i&lt;j:</span><br><span class="line">            j -= 1</span><br><span class="line">        a[i] = a[j]</span><br><span class="line">        while a[i]&lt;=based and i&lt;j:</span><br><span class="line">            i += 1</span><br><span class="line">        </span><br><span class="line">        a[j]= a[i]</span><br><span class="line">        a[i] = based</span><br><span class="line"></span><br><span class="line">    return i</span><br><span class="line">    </span><br><span class="line">def kuaipai(left,right,a):</span><br><span class="line">    if left&lt;right:</span><br><span class="line">        p = kpsort(left,right,a)</span><br><span class="line">        kuaipai(left,p-1,a)</span><br><span class="line">        kuaipai(p+1,right,a)</span><br><span class="line"></span><br><span class="line">    return a</span><br><span class="line">            </span><br><span class="line">print(kuaipai(0,8,a =[2,1,4,3,9,5,6,8,7]))</span><br></pre></td></tr></table></figure><p>结果为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数据结构面试中经常会被问到篇排序相关的问题，那么这篇文章会研究下怎么用python来实现排序。&lt;/p&gt;
    
    </summary>
    
      <category term="编程与系统" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B%E4%B8%8E%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="数据结构" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>数据结构算法之二叉树</title>
    <link href="http://yoursite.com/2017/11/13/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AE%97%E6%B3%95%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%A0%91/"/>
    <id>http://yoursite.com/2017/11/13/数据结构/数据结构算法之二叉树/</id>
    <published>2017-11-12T16:44:41.000Z</published>
    <updated>2018-03-17T17:11:13.519Z</updated>
    
    <content type="html"><![CDATA[<p>数据结构面试中经常会被问到篇二叉树相关的问题，那么这篇文章会研究下怎么用python来进行二叉树的构建和遍历。</p><a id="more"></a><p>注意：py2中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print root.elem,</span><br></pre></td></tr></table></figure></p><p>在py3中要换成<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print (root.elem,end=&quot;  &quot;)</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line"># coding:utf-8</span><br><span class="line"></span><br><span class="line"># 定义节点类</span><br><span class="line">class Node:</span><br><span class="line">    def __init__(self,elem = -1,):</span><br><span class="line">        self.elem = elem</span><br><span class="line">        self.left = None</span><br><span class="line">        self.right = None</span><br><span class="line">        </span><br><span class="line"># 定义二叉树</span><br><span class="line">class Tree:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.root = Node()</span><br><span class="line">        self.myqu = []</span><br><span class="line">    </span><br><span class="line">    # 添加节点</span><br><span class="line">    def add(self,elem):</span><br><span class="line">        node = Node(elem)</span><br><span class="line">        if self.root.elem == -1:         # 判断如果是根节点</span><br><span class="line">            self.root  = node</span><br><span class="line">            self.myqu.append(self.root)</span><br><span class="line">        else:</span><br><span class="line">            treenode = self.myqu[0]</span><br><span class="line">            if treenode.left == None:</span><br><span class="line">                treenode.left = node</span><br><span class="line">                self.myqu.append(treenode.left)</span><br><span class="line">            else:</span><br><span class="line">                treenode.right = node</span><br><span class="line">                self.myqu.append(treenode.right)</span><br><span class="line">                self.myqu.pop(0)</span><br><span class="line">        </span><br><span class="line">    # 利用递归实现树的先序遍历</span><br><span class="line">    def xianxu(self,root):</span><br><span class="line">        if root == None:</span><br><span class="line">            return</span><br><span class="line">        print root.elem,</span><br><span class="line">        self.xianxu(root.left)</span><br><span class="line">        self.xianxu(root.right)</span><br><span class="line">        </span><br><span class="line">    # 利用递归实现树的中序遍历</span><br><span class="line">    def zhongxu(self,root):</span><br><span class="line">        if root == None:</span><br><span class="line">            return </span><br><span class="line">        self.zhongxu(root.left)</span><br><span class="line">        print root.elem,</span><br><span class="line">        self.zhongxu(root.right)</span><br><span class="line">        </span><br><span class="line">    # 利用递归实现树的后序遍历</span><br><span class="line">    def houxu(self,root):</span><br><span class="line">        if root == None:</span><br><span class="line">            return </span><br><span class="line">        self.houxu(root.left)</span><br><span class="line">        self.houxu(root.right)</span><br><span class="line">        print root.elem,</span><br><span class="line">    </span><br><span class="line">    # 利用队列实现层次遍历</span><br><span class="line">    def cengci(self,root):</span><br><span class="line">        if root == None:</span><br><span class="line">            return</span><br><span class="line">        myq = []</span><br><span class="line">        node = root</span><br><span class="line">        myq.append(node)</span><br><span class="line">        while myq:</span><br><span class="line">            node = myq.pop(0)</span><br><span class="line">            print node.elem,</span><br><span class="line">            if node.left != None:</span><br><span class="line">                myq.append(node.left)</span><br><span class="line">            if node.right != None:</span><br><span class="line">                myq.append(node.right)</span><br><span class="line">    </span><br><span class="line">    # 求树的叶子节点</span><br><span class="line">    def getYeJiedian(self,root):</span><br><span class="line">        if root == None:</span><br><span class="line">            return 0</span><br><span class="line">        if root.left == None and root.right == None:</span><br><span class="line">            return 1</span><br><span class="line"></span><br><span class="line">        return self.getYeJiedian(root.left) + self.getYeJiedian(root.right)</span><br><span class="line"></span><br><span class="line">    # 由先序和中序,还原二叉树</span><br><span class="line">    def preMidToHou(self,pre,mid):</span><br><span class="line">        if len(pre)==0:</span><br><span class="line">            return None</span><br><span class="line">        if len(pre)==1:</span><br><span class="line">            Node(mid[0])</span><br><span class="line">        root = Node(pre[0])</span><br><span class="line">        root_index = mid.index(pre[0])</span><br><span class="line">        root.left = self.preMidToHou(pre[1:root_index + 1],mid[:root_index])</span><br><span class="line">        root.right = self.preMidToHou(pre[root_index + 1:],mid[root_index + 1:])</span><br><span class="line">        return root</span><br><span class="line"></span><br><span class="line">    # 由后序和中序,还原二叉树</span><br><span class="line">    def preMidToHou(self,mid,hou):</span><br><span class="line">        if len(hou)==0:</span><br><span class="line">            return None</span><br><span class="line">        if len(hou)==1:</span><br><span class="line">            Node(mid[0])</span><br><span class="line">        root = Node(hou[-1])</span><br><span class="line">        root_index = mid.index(hou[-1])</span><br><span class="line">        root.left = self.preMidToHou(mid[:root_index],hou[:root_index])</span><br><span class="line">        root.right = self.preMidToHou(mid[root_index + 1:],mid[root_index + 1:])</span><br><span class="line">        return root</span><br><span class="line"></span><br><span class="line"># 创建一个树，添加节点</span><br><span class="line">tree = Tree()</span><br><span class="line">for i in range(10):</span><br><span class="line">    tree.add(i)</span><br><span class="line">    </span><br><span class="line">print(&quot;二叉树的先序遍历:&quot;)</span><br><span class="line">print(tree.xianxu(tree.root))</span><br><span class="line"></span><br><span class="line">print(&quot;二叉树的中序遍历:&quot;)</span><br><span class="line">print(tree.zhongxu(tree.root))</span><br><span class="line"></span><br><span class="line">print(&quot;二叉树的后序遍历:&quot;)</span><br><span class="line">print(tree.houxu(tree.root))</span><br><span class="line"></span><br><span class="line">print(&quot;二叉树的层次遍历&quot;)</span><br><span class="line">print(tree.cengci(tree.root))</span><br><span class="line"></span><br><span class="line">print(&quot;\n二叉树的叶子节点为:&quot;)</span><br><span class="line">print(tree.getYeJiedian(tree.root))</span><br><span class="line"></span><br><span class="line">print(&quot;\n已知二叉树先序遍历和中序遍历，求后序:&quot;)</span><br><span class="line">print(&quot;先序:&quot;)</span><br><span class="line">print(tree.xianxu(tree.root))</span><br><span class="line">print(&quot;中序:&quot;)</span><br><span class="line">print(tree.zhongxu(tree.root))</span><br><span class="line">print(&quot;后序:&quot;)</span><br><span class="line">root = tree.preMidToHou([0,1,3,7,8,4,9,2,5,6],[7,3,8,1,9,4,0,5,2,6])</span><br><span class="line">print(tree.houxu(root))</span><br><span class="line"></span><br><span class="line">print(&quot;\n已知二叉树后序遍历和中序遍历，求前序:&quot;)</span><br><span class="line">print(&quot;后序:&quot;)</span><br><span class="line">print(tree.houxu(tree.root))</span><br><span class="line">print(&quot;中序:&quot;)</span><br><span class="line">print(tree.zhongxu(tree.root))</span><br><span class="line">print(&quot;前序:&quot;)</span><br><span class="line">root = tree.preMidToHou([7,3,8,1,9,4,0,5,2,6],[7,8,3,9,4,1,5,6,2,0])</span><br><span class="line">print(tree.xianxu(root))</span><br></pre></td></tr></table></figure><p>运行结果为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">二叉树的先序遍历:</span><br><span class="line">0 1 3 7 8 4 9 2 5 6 None</span><br><span class="line"></span><br><span class="line">二叉树的中序遍历:</span><br><span class="line">7 3 8 1 9 4 0 5 2 6 None</span><br><span class="line"></span><br><span class="line">二叉树的后序遍历:</span><br><span class="line">7 8 3 9 4 1 5 6 2 0 None</span><br><span class="line"></span><br><span class="line">二叉树的层次遍历</span><br><span class="line">0 1 2 3 4 5 6 7 8 9 None</span><br><span class="line"></span><br><span class="line">二叉树的叶子节点为:</span><br><span class="line">5</span><br><span class="line"></span><br><span class="line">已知二叉树先序遍历和中序遍历，求后序:</span><br><span class="line">先序:</span><br><span class="line">0  1  3  7  8  4  9  2  5  6  None</span><br><span class="line">中序:</span><br><span class="line">7  3  8  1  9  4  0  5  2  6  None</span><br><span class="line">后序:</span><br><span class="line">1  3  7  8  4  9  0  5  2  6  None</span><br><span class="line"></span><br><span class="line">已知二叉树后序遍历和中序遍历，求前序:</span><br><span class="line">后序:</span><br><span class="line">7  8  3  9  4  1  5  6  2  0  None</span><br><span class="line">中序:</span><br><span class="line">7  3  8  1  9  4  0  5  2  6  None</span><br><span class="line">前序:</span><br><span class="line">0  1  3  7  8  4  9  6  2  5  None</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数据结构面试中经常会被问到篇二叉树相关的问题，那么这篇文章会研究下怎么用python来进行二叉树的构建和遍历。&lt;/p&gt;
    
    </summary>
    
      <category term="编程与系统" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B%E4%B8%8E%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="数据结构" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>回归分析之Sklearn实现电力预测</title>
    <link href="http://yoursite.com/2017/11/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90%E4%B9%8BSklearn%E5%AE%9E%E7%8E%B0%E7%94%B5%E5%8A%9B%E9%A2%84%E6%B5%8B/"/>
    <id>http://yoursite.com/2017/11/07/机器学习/回归分析之Sklearn实现电力预测/</id>
    <published>2017-11-07T05:39:15.000Z</published>
    <updated>2018-03-17T17:07:39.619Z</updated>
    
    <content type="html"><![CDATA[<p>参考原文：<a href="http://www.cnblogs.com/pinard/p/6016029.html" target="_blank" rel="external">http://www.cnblogs.com/pinard/p/6016029.html</a><br>这里进行了手动实现，增强记忆。<br><a id="more"></a></p><h1 id="1：数据集介绍"><a href="#1：数据集介绍" class="headerlink" title="1：数据集介绍"></a>1：数据集介绍</h1><p>使用的数据是UCI大学公开的机器学习数据</p><p>数据的介绍在这： <a href="http://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant" target="_blank" rel="external">http://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant</a></p><p>数据的下载地址在这：<a href="http://archive.ics.uci.edu/ml/machine-learning-databases/00294/" target="_blank" rel="external">http://archive.ics.uci.edu/ml/machine-learning-databases/00294/</a></p><p>里面是一个循环发电场的数据，共有9568个样本数据，每个数据有5列，分别是:AT（温度）, V（压力）, AP（湿度）, RH（压强）, PE（输出电力)。我们不用纠结于每项具体的意思。</p><p>我们的问题是得到一个线性的关系，对应PE是样本输出，而AT/V/AP/RH这4个是样本特征， 机器学习的目的就是得到一个线性回归模型，即:</p><script type="math/tex; mode=display">PE = \theta _{0} + \theta _{0} * AT + \theta _{0} * V +\theta _{0} * AP +\theta _{0}*RH</script><p>而需要学习的，就是θ0,θ1,θ2,θ3,θ4这5个参数。</p><hr><h1 id="2：准备数据"><a href="#2：准备数据" class="headerlink" title="2：准备数据"></a>2：准备数据</h1><p>下载源数据之后，解压会得到一个xlsx的文件，打开另存为csv文件，数据已经整理好，没有非法数据，但是数据并没有进行归一化，不过这里我们可以使用sklearn来帮我处理</p><p>sklearn的归一化处理参考：<a href="http://blog.csdn.net/gamer_gyt/article/details/77761884" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt/article/details/77761884</a></p><hr><h1 id="3：使用pandas来进行数据的读取"><a href="#3：使用pandas来进行数据的读取" class="headerlink" title="3：使用pandas来进行数据的读取"></a>3：使用pandas来进行数据的读取</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line"># pandas 读取数据</span><br><span class="line">data = pd.read_csv(&quot;Folds5x2_pp.csv&quot;)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><p>然后会看到如下结果，说明数据读取成功：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ATVAPRHPE</span><br><span class="line">08.3440.771010.8490.01480.48</span><br><span class="line">123.6458.491011.4074.20445.75</span><br><span class="line">229.7456.901007.1541.91438.76</span><br><span class="line">319.0749.691007.2276.79453.09</span><br><span class="line">411.8040.661017.1397.20464.43</span><br></pre></td></tr></table></figure><hr><h1 id="4：准备运行算法的数据"><a href="#4：准备运行算法的数据" class="headerlink" title="4：准备运行算法的数据"></a>4：准备运行算法的数据</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = data[[&quot;AT&quot;,&quot;V&quot;,&quot;AP&quot;,&quot;RH&quot;]]</span><br><span class="line">print X.shape</span><br><span class="line">y = data[[&quot;PE&quot;]]</span><br><span class="line">print y.shape</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(9568, 4)</span><br><span class="line">(9568, 1)</span><br></pre></td></tr></table></figure><p>说明有9658条数据，其中”AT”,”V”,”AP”,”RH” 四列作为样本特征，”PE”列作为样本输出。</p><hr><h1 id="5：划分训练集和测试集"><a href="#5：划分训练集和测试集" class="headerlink" title="5：划分训练集和测试集"></a>5：划分训练集和测试集</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cross_validation import train_test_split</span><br><span class="line"></span><br><span class="line"># 划分训练集和测试集</span><br><span class="line">X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=1)</span><br><span class="line">print X_train.shape</span><br><span class="line">print y_train.shape</span><br><span class="line">print X_test.shape</span><br><span class="line">print y_test.shape</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(7176, 4)</span><br><span class="line">(7176, 1)</span><br><span class="line">(2392, 4)</span><br><span class="line">(2392, 1)</span><br></pre></td></tr></table></figure><p>75%的数据被划分为训练集，25的数据划分为测试集。</p><hr><h1 id="6：运行sklearn-线性模型"><a href="#6：运行sklearn-线性模型" class="headerlink" title="6：运行sklearn 线性模型"></a>6：运行sklearn 线性模型</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line"></span><br><span class="line">linreg = LinearRegression()</span><br><span class="line">linreg.fit(X_train,y_train)</span><br><span class="line"></span><br><span class="line"># 训练模型完毕，查看结果</span><br><span class="line">print linreg.intercept_</span><br><span class="line">print linreg.coef_</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ 447.06297099]</span><br><span class="line">[[-1.97376045 -0.23229086  0.0693515  -0.15806957]]</span><br></pre></td></tr></table></figure><p>即我们得到的模型结果为：</p><script type="math/tex; mode=display">PE = 447.06297099 - 1.97376045*AT - 0.23229086*V + 0.0693515*AP -0.15806957*RH</script><hr><h1 id="7：模型评价"><a href="#7：模型评价" class="headerlink" title="7：模型评价"></a>7：模型评价</h1><p>我们需要评价模型的好坏，通常对于线性回归来讲，我么一般使用均方差（MSE，Mean Squared Error）或者均方根差（RMSE，Root Mean Squared Error）来评价模型的好坏</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y_pred = linreg.predict(X_test)</span><br><span class="line">from sklearn import metrics</span><br><span class="line"></span><br><span class="line"># 使用sklearn来计算mse和Rmse</span><br><span class="line">print &quot;MSE:&quot;,metrics.mean_squared_error(y_test, y_pred)</span><br><span class="line">print &quot;RMSE:&quot;,np.sqrt(metrics.mean_squared_error(y_test, y_pred))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MSE: 20.0804012021</span><br><span class="line">RMSE: 4.48111606657</span><br></pre></td></tr></table></figure><p>得到了MSE或者RMSE，如果我们用其他方法得到了不同的系数，需要选择模型时，就用MSE小的时候对应的参数。</p><hr><h1 id="8：交叉验证"><a href="#8：交叉验证" class="headerlink" title="8：交叉验证"></a>8：交叉验证</h1><p>我们可以通过交叉验证来持续优化模型，代码如下，我们采用10折交叉验证，即cross_val_predict中的cv参数为10：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 交叉验证</span><br><span class="line">from sklearn.model_selection import cross_val_predict</span><br><span class="line">predicted = cross_val_predict(linreg,X,y,cv=10)</span><br><span class="line">print &quot;MSE:&quot;,metrics.mean_squared_error(y, predicted)</span><br><span class="line">print &quot;RMSE:&quot;,np.sqrt(metrics.mean_squared_error(y, predicted))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MSE: 20.7955974619</span><br><span class="line">RMSE: 4.56021901469</span><br></pre></td></tr></table></figure><p>可以看出，采用交叉验证模型的MSE比第6节的大，主要原因是我们这里是对所有折的样本做测试集对应的预测值的MSE，而第6节仅仅对25%的测试集做了MSE。两者的先决条件并不同。</p><hr><h1 id="9：画图查看结果"><a href="#9：画图查看结果" class="headerlink" title="9：画图查看结果"></a>9：画图查看结果</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 画图查看结果</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.scatter(y, predicted)</span><br><span class="line">ax.plot([y.min(), y.max()], [y.min(), y.max()], &apos;k--&apos;, lw=4)</span><br><span class="line">ax.set_xlabel(&apos;Measured&apos;)</span><br><span class="line">ax.set_ylabel(&apos;Predicted&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://img.blog.csdn.net/20171107133222238?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><hr><center>    <img src="/img/gzh.jpg" weight="250px" height="250px">    <br>打开微信扫一扫，关注微信公众号【数据与算法联盟】</center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参考原文：&lt;a href=&quot;http://www.cnblogs.com/pinard/p/6016029.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://www.cnblogs.com/pinard/p/6016029.html&lt;/a&gt;&lt;br&gt;这里进行了手动实现，增强记忆。&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="回归分析" scheme="http://yoursite.com/tags/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/"/>
    
      <category term="sklearn" scheme="http://yoursite.com/tags/sklearn/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>回归分析之线性回归（N元线性回归）</title>
    <link href="http://yoursite.com/2017/09/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%88N%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%89/"/>
    <id>http://yoursite.com/2017/09/29/机器学习/回归分析之线性回归（N元线性回归）/</id>
    <published>2017-09-29T08:45:14.000Z</published>
    <updated>2018-03-17T17:09:27.139Z</updated>
    
    <content type="html"><![CDATA[<p>在上一篇文章中我们介绍了 <a href="http://blog.csdn.net/gamer_gyt/article/details/78008144" target="_blank" rel="external"> 回归分析之理论篇</a>，在其中我们有聊到线性回归和非线性回归，包括广义线性回归，这一篇文章我们来聊下回归分析中的线性回归。</p><a id="more"></a><h1 id="一元线性回归"><a href="#一元线性回归" class="headerlink" title="一元线性回归"></a>一元线性回归</h1><p>预测房价：<br>输入编号    | 平方米    | 价格<br>-|-|-<br>1 |    150 |    6450<br>2    | 200    | 7450<br>3|    250    |8450<br>4|    300    |9450<br>5|    350    |11450<br>6|    400    |15450<br>7|    600|    18450</p><p>针对上边这种一元数据来讲，我们可以构建的一元线性回归函数为</p><script type="math/tex; mode=display">H(x) = k*x + b</script><p>其中H(x)为平方米价格表，k是一元回归系数，b为常数。最小二乘法的公式：</p><script type="math/tex; mode=display">k =\frac{ \sum_{1}^{n} (x_{i} - \bar{x} )(y_{i} - \bar{y}) } { \sum_{1}^{n}(x_{i}-\bar{x})^{2} }</script><p>自己使用python代码实现为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def leastsq(x,y):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    x,y分别是要拟合的数据的自变量列表和因变量列表</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    meanX = sum(x) * 1.0 / len(x)      # 求x的平均值</span><br><span class="line">    meanY = sum(y) * 1.0 / len(y)     # 求y的平均值</span><br><span class="line"></span><br><span class="line">    xSum = 0.0</span><br><span class="line">    ySum = 0.0</span><br><span class="line"></span><br><span class="line">    for i in range(len(x)):</span><br><span class="line">        xSum += (x[i] - meanX) * (y[i] - meanY)</span><br><span class="line">        ySum += (x[i] - meanX) ** 2</span><br><span class="line"></span><br><span class="line">    k = ySum/xSum</span><br><span class="line">    b = ySum - k * meanX</span><br><span class="line"></span><br><span class="line">    return k,b</span><br></pre></td></tr></table></figure></p><p>使用python的scipy包进行计算:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">leastsq(func, x0, args=(), Dfun=None, full_output=0, col_deriv=0, ftol=1.49012e-08, xtol=1.49012e-08, gtol=0.0, maxfev=0, epsfcn=None, factor=100, diag=None)</span><br><span class="line"></span><br><span class="line">from scipy.optimize import leastsq</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def fun(p, x):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    定义想要拟合的函数</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    k,b = p    # 从参数p获得拟合的参数</span><br><span class="line">    return k*x + b</span><br><span class="line"></span><br><span class="line">def err(p, x, y):</span><br><span class="line">    return fun(p,x) - y</span><br><span class="line"></span><br><span class="line">#定义起始的参数 即从 y = 1*x+1 开始，其实这个值可以随便设，只不过会影响到找到最优解的时间</span><br><span class="line">p0 = [1,1]</span><br><span class="line"></span><br><span class="line">#将list类型转换为 numpy.ndarray 类型，最初我直接使用</span><br><span class="line">#list 类型,结果 leastsq函数报错，后来在别的blog上看到了，原来要将类型转</span><br><span class="line">#换为numpy的类型</span><br><span class="line"></span><br><span class="line">x1 = np.array([150,200,250,300,350,400,600])</span><br><span class="line">y1 = np.array([6450,7450,8450,9450,11450,15450,18450])</span><br><span class="line"></span><br><span class="line">xishu = leastsq(err, p0, args=(x1,y1))</span><br><span class="line"></span><br><span class="line">print xishu[0]</span><br></pre></td></tr></table></figure></p><p>当然python的leastsq函数不仅仅局限于一元一次的应用，也可以应用到一元二次，二元二次，多元多次等，具体可以看下这篇博客：<a href="http://www.cnblogs.com/NanShan2016/p/5493429.html" target="_blank" rel="external">http://www.cnblogs.com/NanShan2016/p/5493429.html</a></p><h1 id="多元线性回归"><a href="#多元线性回归" class="headerlink" title="多元线性回归"></a>多元线性回归</h1><p>总之：我们可以用python leastsq函数解决几乎所有的线性回归的问题了，比如说</p><script type="math/tex; mode=display">y = a * x^2 + b * x + c</script><script type="math/tex; mode=display">y = a * x_1^2 + b * x_1 + c * x_2 + d</script><script type="math/tex; mode=display">y = a * x_1^3 + b * x_1^2 + c * x_1 + d</script><p>在使用时只需把参数列表和 fun 函数中的return 换一下，拿以下函数举例</p><script type="math/tex; mode=display">y = a * x_1^2 + b * x_1 + c * x_2 + d</script><p>对应的python 代码是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">from scipy.optimize import leastsq</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def fun(p, x1, x2):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    定义想要拟合的函数</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    a,b,c,d = p    # 从参数p获得拟合的参数</span><br><span class="line">    return a * (x1**2) + b * x1 + c * x2 + d</span><br><span class="line"></span><br><span class="line">def err(p, x1, x2, y):</span><br><span class="line">    return fun(p,x1,x2) - y</span><br><span class="line"></span><br><span class="line">#定义起始的参数 即从 y = 1*x+1 开始，其实这个值可以随便设，只不过会影响到找到最优解的时间</span><br><span class="line">p0 = [1,1,1,1]</span><br><span class="line"></span><br><span class="line">#将list类型转换为 numpy.ndarray 类型，最初我直接使用</span><br><span class="line">#list 类型,结果 leastsq函数报错，后来在别的blog上看到了，原来要将类型转</span><br><span class="line">#换为numpy的类型</span><br><span class="line"></span><br><span class="line">x1 = np.array([150,200,250,300,350,400,600])    # 面积</span><br><span class="line">x2 = np.array([4,2,7,9,12,14,15])               # 楼层</span><br><span class="line">y1 = np.array([6450,7450,8450,9450,11450,15450,18450])   # 价格/平方米</span><br><span class="line"></span><br><span class="line">xishu = leastsq(err, p0, args=(x1,x2,y1))</span><br><span class="line"></span><br><span class="line">print xishu[0]</span><br></pre></td></tr></table></figure></p><h1 id="sklearn中的线性回归应用"><a href="#sklearn中的线性回归应用" class="headerlink" title="sklearn中的线性回归应用"></a>sklearn中的线性回归应用</h1><h2 id="普通最小二乘回归"><a href="#普通最小二乘回归" class="headerlink" title="普通最小二乘回归"></a>普通最小二乘回归</h2><p>这里我们使用的是sklearn中的linear_model来模拟<script type="math/tex">y=a * x_1 + b * x_2 + c</script></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">In [1]: from sklearn.linear_model import LinearRegression</span><br><span class="line"></span><br><span class="line">In [2]: linreg = LinearRegression()</span><br><span class="line"></span><br><span class="line">In [3]: linreg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])</span><br><span class="line"></span><br><span class="line">In [4]: linreg.coef_</span><br><span class="line">Out[4]: array([ 0.5,  0.5])</span><br><span class="line"></span><br><span class="line">In [5]: linreg.intercept_</span><br><span class="line">Out[5]: 1.1102230246251565e-16</span><br><span class="line"></span><br><span class="line">In [6]: linreg.predict([4,4])</span><br><span class="line">Out[6]: array([ 4.])</span><br><span class="line"></span><br><span class="line">In [7]: zip([&quot;x1&quot;,&quot;x2&quot;], linreg.coef_)</span><br><span class="line">Out[7]: [(&apos;x1&apos;, 0.5), (&apos;x2&apos;, 0.49999999999999989)]</span><br></pre></td></tr></table></figure><p>所以可得<script type="math/tex">y = 0.5 * x_1 + 0.5 * x_2 + 1.11e-16</script></p><p>linreg.coef_  为系数 a,b</p><p>linreg.intercept_ 为截距 c</p><p>缺点：因为系数矩阵x与它的转置矩阵相乘得到的矩阵不能求逆，导致最小二乘法得到的回归系数不稳定，方差很大。</p><h2 id="多项式回归：基函数扩展线性模型"><a href="#多项式回归：基函数扩展线性模型" class="headerlink" title="多项式回归：基函数扩展线性模型"></a>多项式回归：基函数扩展线性模型</h2><p>机器学习中一种常见的模式是使用线性模型训练数据的非线性函数。这种方法保持了一般快速的线性方法的性能，同时允许它们适应更广泛的数据范围。</p><p>例如，可以通过构造系数的多项式特征来扩展一个简单的线性回归。在标准线性回归的情况下，你可能有一个类似于二维数据的模型：</p><script type="math/tex; mode=display">y(w,x) = w_{0} + w_{1} x_{1} + w_{2} x_{2}</script><p>如果我们想把抛物面拟合成数据而不是平面，我们可以结合二阶多项式的特征，使模型看起来像这样:</p><script type="math/tex; mode=display">y(w,x) = w_{0} + w_{1} x_{1} + w_{2} x_{2} + w_{3} x_{1}x_{2} + w_{4} x_{1}^2 + w_{5} x_{2}^2</script><p>我们发现，这仍然是一个线性模型，想象着创建一个新变量：</p><script type="math/tex; mode=display">z = [x_{1},x_{2},x_{1} x_{2},x_{1}^2,x_{2}^2]</script><p>可以把线性回归模型写成下边这种形式：</p><script type="math/tex; mode=display">y(w,x) = w_{0} + w_{1} z_{1} + w_{2} z_{2} + w_{3} z_{3} + w_{4} z_{4} + w_{5} z_{5}</script><p>我们看到，所得的多项式回归与我们上面所考虑的线性模型相同（即模型在W中是线性的），可以用同样的方法来求解。通过考虑在用这些基函数建立的高维空间中的线性拟合，该模型具有灵活性，可以适应更广泛的数据范围。</p><p>使用如下代码，将二维数据进行二元转换,转换规则为：</p><script type="math/tex; mode=display">[x_1, x_2] => [1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">In [15]: from sklearn.preprocessing import PolynomialFeatures</span><br><span class="line"></span><br><span class="line">In [16]: import numpy as np</span><br><span class="line"></span><br><span class="line">In [17]: X = np.arange(6).reshape(3,2)</span><br><span class="line"></span><br><span class="line">In [18]: X</span><br><span class="line">Out[18]: </span><br><span class="line">array([[0, 1],</span><br><span class="line">       [2, 3],</span><br><span class="line">       [4, 5]])</span><br><span class="line"></span><br><span class="line">In [19]: poly = PolynomialFeatures(degree=2)</span><br><span class="line"></span><br><span class="line">In [20]: poly.fit_transform(X)</span><br><span class="line">Out[20]: </span><br><span class="line">array([[  1.,   0.,   1.,   0.,   0.,   1.],</span><br><span class="line">       [  1.,   2.,   3.,   4.,   6.,   9.],</span><br><span class="line">       [  1.,   4.,   5.,  16.,  20.,  25.]])</span><br></pre></td></tr></table></figure><p>验证：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">In [38]: from sklearn.preprocessing import PolynomialFeatures</span><br><span class="line"></span><br><span class="line">In [39]: from sklearn.linear_model import LinearRegression</span><br><span class="line"></span><br><span class="line">In [40]: from sklearn.pipeline import Pipeline</span><br><span class="line"></span><br><span class="line">In [41]: import numpy as np</span><br><span class="line"></span><br><span class="line">In [42]: </span><br><span class="line"></span><br><span class="line">In [42]: model = Pipeline( [ (&quot;poly&quot;,PolynomialFeatures(degree=3)),(&quot;linear&quot;,LinearRegression(fit_intercept=False)) ] )</span><br><span class="line"></span><br><span class="line">In [43]: model</span><br><span class="line">Out[43]: Pipeline(steps=[(&apos;poly&apos;, PolynomialFeatures(degree=3, include_bias=True, interaction_only=False)), (&apos;linear&apos;, LinearRegression(copy_X=True, fit_intercept=False, n_jobs=1, normalize=False))])</span><br><span class="line"></span><br><span class="line">In [44]: x = np.arange(5)</span><br><span class="line"></span><br><span class="line">In [45]: y = 3 - 2 * x + x ** 2 - x ** 3</span><br><span class="line"></span><br><span class="line">In [46]: y</span><br><span class="line">Out[46]: array([  3,   1,  -5, -21, -53])</span><br><span class="line"></span><br><span class="line">In [47]: model = model.fit(x[:,np.newaxis],y)</span><br><span class="line"></span><br><span class="line">In [48]: model.named_steps[&apos;linear&apos;].coef_</span><br><span class="line">Out[48]: array([ 3., -2.,  1., -1.])</span><br></pre></td></tr></table></figure></p><p>我们可以看出最后求出的参数和一元三次方程是一致的。</p><p>这里如果把degree改为2，y的方程也换一下，结果也是一致的<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">In [51]: from sklearn.linear_model import LinearRegression</span><br><span class="line"></span><br><span class="line">In [52]: from sklearn.preprocessing import PolynomialFeatures</span><br><span class="line"></span><br><span class="line">In [53]: from sklearn.pipeline import Pipeline</span><br><span class="line"></span><br><span class="line">In [54]: import numpy as np</span><br><span class="line"></span><br><span class="line">In [55]: model = Pipeline( [ (&quot;poly&quot;,PolynomialFeatures(degree=2)),(&quot;linear&quot;,LinearRegression(fit_intercept=False)) ] )</span><br><span class="line"></span><br><span class="line">In [56]: x = np.arange(5)</span><br><span class="line"></span><br><span class="line">In [57]: y = 3 + 2 * x + x ** 2</span><br><span class="line"></span><br><span class="line">In [58]: model = model.fit(x[:, np.newaxis], y)</span><br><span class="line"></span><br><span class="line">In [59]: model.named_steps[&apos;linear&apos;].coef_</span><br><span class="line">Out[59]: array([ 3., 2.,  1.])</span><br></pre></td></tr></table></figure></p><h2 id="线性回归的评测"><a href="#线性回归的评测" class="headerlink" title="线性回归的评测"></a>线性回归的评测</h2><p>在<a href="http://note.youdao.com/" target="_blank" rel="external">上一篇文章</a>中我们聊到了回归模型的评测方法，解下来我们详细聊聊如何来评价一个回归模型的好坏。</p><p>这里我们定义预测值和真实值分别为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">true = [10, 5, 3, 2]</span><br><span class="line">pred = [9, 5, 5, 3]</span><br></pre></td></tr></table></figure></p><p>1: 平均绝对误差（Mean Absolute Error, MAE）</p><script type="math/tex; mode=display">\frac{1}{N}(\sum_{1}^{n} |y_i - \bar{y}|)</script><p>2: 均方误差（Mean Squared Error, MSE）</p><script type="math/tex; mode=display">\frac{1}{N}\sum_{1}^{n}(y_i - \bar{y})^2</script><p>3: 均方根误差（Root Mean Squared Error, RMSE）</p><script type="math/tex; mode=display">\frac{1}{N} \sqrt{ \sum_{1}^{n}(y_i - \bar{y})^2 }</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">In [80]: from sklearn import metrics</span><br><span class="line"></span><br><span class="line">In [81]: import numpy as np</span><br><span class="line"></span><br><span class="line">In [82]: true = [10, 5, 3, 2]</span><br><span class="line"></span><br><span class="line">In [83]: pred = [9, 5, 5, 3]</span><br><span class="line"></span><br><span class="line">In [84]: print(&quot;MAE: &quot;, metrics.mean_absolute_error(true,pred))</span><br><span class="line">(&apos;MAE: &apos;, 1.0)</span><br><span class="line"></span><br><span class="line">In [85]: print(&quot;MAE By Hand: &quot;, (1+0+2+1)/4.)</span><br><span class="line">(&apos;MAE By Hand: &apos;, 1.0)</span><br><span class="line"></span><br><span class="line">In [86]: print(&quot;MSE: &quot;, metrics.mean_squared_error(true,pred))</span><br><span class="line">(&apos;MSE: &apos;, 1.5)</span><br><span class="line"></span><br><span class="line">In [87]: print(&quot;MSE By Hand: &quot;, (1 ** 2 + 0 ** 2 + 2 ** 2 + 1 ** 2 ) / 4.)</span><br><span class="line">(&apos;MSE By Hand: &apos;, 1.5)</span><br><span class="line"></span><br><span class="line">In [88]: print(&quot;RMSE: &quot;, np.sqrt(metrics.mean_squared_error(true,pred)))</span><br><span class="line">(&apos;RMSE: &apos;, 1.2247448713915889)</span><br><span class="line"></span><br><span class="line">In [89]: print(&quot;RMSE By Hand: &quot;, np.sqrt((1 ** 2 + 0 ** 2 + 2 ** 2 + 1 ** 2 ) / 4.))</span><br><span class="line">(&apos;RMSE By Hand: &apos;, 1.2247448713915889)</span><br></pre></td></tr></table></figure><hr><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>线性回归在现实中还是可以解决很多问题的，但是并不是万能的，后续我会继续整理逻辑回归，岭回归等相关回归的知识，如果你感觉有用，欢迎分享！</p><hr><center>    <img src="/img/gzh.jpg" weight="250px" height="250px">    <br>打开微信扫一扫，关注微信公众号【数据与算法联盟】</center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在上一篇文章中我们介绍了 &lt;a href=&quot;http://blog.csdn.net/gamer_gyt/article/details/78008144&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt; 回归分析之理论篇&lt;/a&gt;，在其中我们有聊到线性回归和非线性回归，包括广义线性回归，这一篇文章我们来聊下回归分析中的线性回归。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="回归分析" scheme="http://yoursite.com/tags/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>几种距离计算公式在数据挖掘中的应用场景分析</title>
    <link href="http://yoursite.com/2017/09/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%87%A0%E7%A7%8D%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F%E5%9C%A8%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/2017/09/20/机器学习/几种距离计算公式在数据挖掘中的应用场景分析/</id>
    <published>2017-09-20T02:23:39.000Z</published>
    <updated>2018-03-17T17:02:48.983Z</updated>
    
    <content type="html"><![CDATA[<p>本文涉及以下几种距离计算公式的分析，参考资料为《面向程序员的数据挖掘指南》</p><ul><li>曼哈顿距离</li><li>欧几里得距离</li><li>闵可夫斯基距离</li><li>皮尔逊相关系数</li><li>余弦相似度</li></ul><a id="more"></a><p>之前整理过一篇关于距离相关的文章：<a href="">机器学习算法中的距离和相似性计算公式，分析以及python实现</a></p><h1 id="闵可夫斯基距离"><a href="#闵可夫斯基距离" class="headerlink" title="闵可夫斯基距离"></a>闵可夫斯基距离</h1><p>两个n维变量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的闵可夫斯基距离定义为：</p><script type="math/tex; mode=display">\sqrt[p]{ \sum_{k=1}^{n} \left | x_{1k}-x_{2k} \right |^p}</script><p>其中p是一个变参数。</p><p>当p=1时，就是曼哈顿距离</p><p>当p=2时，就是欧氏距离</p><p>当p→∞时，就是切比雪夫距离</p><p>根据变参数的不同，闵氏距离可以表示一类的距离。</p><p>p值越大，单个维度的差值大小会对整体距离有更大的影响</p><h1 id="曼哈顿距离／欧几里得距离的瑕疵"><a href="#曼哈顿距离／欧几里得距离的瑕疵" class="headerlink" title="曼哈顿距离／欧几里得距离的瑕疵"></a>曼哈顿距离／欧几里得距离的瑕疵</h1><p>在《面向程序员的数据挖掘指南》中给出了这样一组样例数据, 下图为一个在线音乐网站的的用户评分情况，用户可以用1-5星来评价一个乐队，下边是8位用户对8个乐队的评价：<br><img src="http://img.blog.csdn.net/20170920102356159?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>表中的横线表示用户没有对乐队进行评价，我们在计算两个用户的距离时，只采用他们都评价过的乐队。</p><p>现在来求Angelica和Bill的距离，因为他们共同评分过的乐队有5个，所以使用其对该5个乐队的评分进行曼哈顿距离的计算为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Dis_1 = |3.5-2| + |2-3.5| + |5-2| + |1.5-3.5| + |2-3| = 9</span><br></pre></td></tr></table></figure><p>同样使用欧式距离计算为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Dis_2 = sqrt( (3.5-2)^2 + (2-3.5)^2 + (5-2)^2 + (1.5-3.5)^2 + (2-3)^2 ) = 4.3</span><br></pre></td></tr></table></figure></p><p>当对Angelica和Bill，Bill和Chan进行距离对比时，由于两者的共同评分过的乐队均为5，数据都在一个5维空间里，是公平的，如果现在要计算Angelica和Hailey与Bill的距离时，会发现，Angelica与Bill共同评分的有5个乐队，Hailey与Bill共同评分的有3个乐队，也就是说两者数据一个在5维空间里，一个在三维空间里，这样明显是不公平的。这将会对我们进行计算时产生不好的影响，所以曼哈顿距离和欧几里得距离在数据完整的情况下效果最好。</p><hr><h1 id="用户问题／皮尔逊相关系数／分数膨胀"><a href="#用户问题／皮尔逊相关系数／分数膨胀" class="headerlink" title="用户问题／皮尔逊相关系数／分数膨胀"></a>用户问题／皮尔逊相关系数／分数膨胀</h1><h2 id="现象——用户问题"><a href="#现象——用户问题" class="headerlink" title="现象——用户问题"></a>现象——用户问题</h2><p>仔细观察用户对乐队的评分数据，可以发现每个用户的评分标准不同：</p><ul><li>Bill没有打出极端的分数，都在2-4分之间</li><li>Jordyn似乎喜欢所有的乐队，打分都在4-5之间</li><li>Hailey是一个有趣的人，他的评分不是1就是4</li></ul><p>那么如何比较这些用户呢？比如说Hailey的4分是相当于Jordyn的4分还是5分呢？我觉得更接近5分，这样一来，就影响推荐系统的准确性了！</p><h2 id="解决该现象"><a href="#解决该现象" class="headerlink" title="解决该现象"></a>解决该现象</h2><p>解决该现象的办法之一就是 使用皮尔逊相关系数，例如下边这样的数据样例（Clara和Robert对五个乐队的评分）：</p><p><img src="http://img.blog.csdn.net/20170920111425007?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>这种现象在数据挖掘领域被称为“分数膨胀“。我们将其评分画成图，如下：<br><img src="http://img.blog.csdn.net/20170920112804525?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>一条直线-完全吻合，代表着Clara和Robert的喜好完全一致。</p><p>皮尔逊相关系数用于衡量两个变量之间的相关性，他的值在-1～1，1代表完全一致，-1代表完全相悖。所以我们可以利用皮尔逊相关系数来找到相似的用户。</p><p>皮尔逊相关系数的计算公式为：<br><img src="http://img.blog.csdn.net/20170920114015874?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br>该公式除了看起来比较复杂，另外需要对数据进行两次遍历，第一次遍历求出 x平均值和y平均值，第二次遍历才能出现结果，这里提供另外一个计算公式，能够计算皮尔逊相关系数的近似值：<br><img src="http://img.blog.csdn.net/20170920114326883?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><hr><h1 id="余弦相似度／稀疏数据"><a href="#余弦相似度／稀疏数据" class="headerlink" title="余弦相似度／稀疏数据"></a>余弦相似度／稀疏数据</h1><p>假设这样一个数据集，一个在线音乐网站，有10000w首音乐（这里不考虑音乐类型，年代等因素），每个用户常听的也就其中的几十首，这种情况下使用曼哈顿或者欧几里得或者皮尔逊相关系数进行计算用户之间相似性，计算相似值会非常小，因为用户之间的交集本来就很少，这样对于计算结果来讲是很不准确的，这个时候就需要余弦相似度了，余弦相似度进行计算时会自动略过这些非零值。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这里只是简答的介绍了这几种相似性距离度量的方法和场景，但是在实际环境中远比这个复杂许多。这里总结下：</p><ul><li>如果数据存在“分数膨胀“问题，就使用皮尔逊相关系数</li><li>如果数据比较密集，变量之间基本都存在共有值，且这些距离数据都是非常重要的，那就使用欧几里得或者曼哈顿距离</li><li>如果数据是稀疏的，就使用余弦相似度</li></ul><hr><center>    <img src="/img/gzh.jpg" weight="250px" height="250px">    <br>打开微信扫一扫，关注微信公众号【数据与算法联盟】</center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文涉及以下几种距离计算公式的分析，参考资料为《面向程序员的数据挖掘指南》&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;曼哈顿距离&lt;/li&gt;
&lt;li&gt;欧几里得距离&lt;/li&gt;
&lt;li&gt;闵可夫斯基距离&lt;/li&gt;
&lt;li&gt;皮尔逊相关系数&lt;/li&gt;
&lt;li&gt;余弦相似度&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="相似度计算" scheme="http://yoursite.com/tags/%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>回归分析之理论篇</title>
    <link href="http://yoursite.com/2017/09/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90%E4%B9%8B%E7%90%86%E8%AE%BA%E7%AF%87/"/>
    <id>http://yoursite.com/2017/09/17/机器学习/回归分析之理论篇/</id>
    <published>2017-09-17T00:10:27.000Z</published>
    <updated>2018-03-17T17:10:47.819Z</updated>
    
    <content type="html"><![CDATA[<p>2015年的机器学习博客其实都是看《机器学习实战》这本书时学到的，说实话当时也是知其然，不知其所以然，以至于对其理解不深刻，好多细节和理论知识都搞的是乱七八糟，自从工作之后再去看一个算法，思考的比之前多了点，查看资料也比之前多了点，生怕理解错误，影响其他人，当然在理解的程度上还是不够深刻，这也是一个学习的过程吧，记录一下，欢迎指正。</p><p>CSDN链接：<a href="http://blog.csdn.net/gamer_gyt/article/details/78008144" target="_blank" rel="external">点击阅读</a><br><a id="more"></a></p><h1 id="一：一些名词定义"><a href="#一：一些名词定义" class="headerlink" title="一：一些名词定义"></a>一：一些名词定义</h1><h2 id="1）指数分布族"><a href="#1）指数分布族" class="headerlink" title="1）指数分布族"></a>1）指数分布族</h2><p>指数分布族是指可以表示为指数形式的概率分布。</p><script type="math/tex; mode=display">f_X(x\mid\theta) = h(x) \exp \left (\eta(\theta) \cdot T(x) -A(\theta)\right )</script><p>其中，η为自然参数(nature parameter)，T(x)是充分统计量（sufficient statistic）。当参数A，h，T都固定以后，就定义了一个以η为参数的函数族。</p><p>伯努利分布与高斯分布是两个典型的指数分布族</p><h3 id="伯努利分布"><a href="#伯努利分布" class="headerlink" title="伯努利分布"></a>伯努利分布</h3><p>又名两点分布或者0-1分布，是一个离散型概率分布。假设1的概率为p，0的概率为q，则<br>其概率质量函数为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;\displaystyle f_&#123;X&#125;(x)=p^&#123;x&#125;(1-p)^&#123;1-x&#125;=\left\&#123;&#123;\begin&#123;matrix&#125;p&amp;&#123;\mbox&#123;if &#125;&#125;x=1,\\q\ &amp;&#123;\mbox&#123;if &#125;&#125;x=0.\\\end&#123;matrix&#125;&#125;\right.&#125;</span><br></pre></td></tr></table></figure></p><p>其期望值为：</p><script type="math/tex; mode=display">{\displaystyle \operatorname {E} [X]=\sum _{i=0}^{1}x_{i}f_{X}(x)=0+p=p}</script><p>其方差为：</p><script type="math/tex; mode=display">{\displaystyle \operatorname {var} [X]=\sum _{i=0}^{1}(x_{i}-E[X])^{2}f_{X}(x)=(0-p)^{2}(1-p)+(1-p)^{2}p=p(1-p)=pq}</script><h3 id="正态分布-高斯分布"><a href="#正态分布-高斯分布" class="headerlink" title="正态分布(高斯分布)"></a>正态分布(高斯分布)</h3><p>若随机变量X服从一个位置参数为 ${\displaystyle \mu }$ 、尺度参数为 ${\displaystyle \sigma } $ 的概率分布，记为：</p><script type="math/tex; mode=display">X \sim N(\mu,\sigma^2),</script><p>其概率密度函数为:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f(x) = &#123;1 \over \sigma\sqrt&#123;2\pi&#125; &#125;\,e^&#123;- &#123;&#123;(x-\mu )^2 \over 2\sigma^2&#125;&#125;&#125;</span><br></pre></td></tr></table></figure></p><p>正态分布的数学期望值或期望值$ {\displaystyle \mu } $ 等于位置参数，决定了分布的位置；其方差 $ {\displaystyle \sigma ^{2}} $ 的开平方或标准差$ {\displaystyle \sigma }$ 等于尺度参数，决定了分布的幅度。</p><h3 id="标准正态分布："><a href="#标准正态分布：" class="headerlink" title="标准正态分布："></a>标准正态分布：</h3><p>如果$ {\displaystyle \mu =0} $ 并且 $ {\displaystyle \sigma =1} $ 则这个正态分布称为标准正态分布。简化为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f(x) = \frac&#123;1&#125;&#123;\sqrt&#123;2\pi&#125;&#125; \, \exp\left(-\frac&#123;x^2&#125;&#123;2&#125; \right)</span><br></pre></td></tr></table></figure></p><p>如下图所示：</p><p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Normal_distribution_pdf.png/650px-Normal_distribution_pdf.png" alt="image"></p><p>正态分布中一些值得注意的量：</p><ul><li>密度函数关于平均值对称</li><li>平均值与它的众数（statistical mode）以及中位数（median）同一数值。</li><li>函数曲线下68.268949%的面积在平均数左右的一个标准差范围内。</li><li>95.449974%的面积在平均数左右两个标准差 $ {\displaystyle 2\sigma } $ 的范围内。</li><li>99.730020%的面积在平均数左右三个标准差$ {\displaystyle 3\sigma } $ 的范围内。</li><li>99.993666%的面积在平均数左右四个标准差$ {\displaystyle 4\sigma } $ 的范围内。</li><li>函数曲线的反曲点（inflection point）为离平均数一个标准差距离的位置。</li></ul><h2 id="2）多重共线性和完全共线性"><a href="#2）多重共线性和完全共线性" class="headerlink" title="2）多重共线性和完全共线性"></a>2）多重共线性和完全共线性</h2><p>多重共线性：指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。一般来说，由于经济数据的限制使得模型设计不当，导致设计矩阵中解释变量间存在普遍的相关关系。通俗点理解就是自变量里边有一些是打酱油的，可以由另外一些变量推导出来，当变量中存在大量的多重共线性变量就会导致模型误差很大，这个时候就需要从自变量中将“打酱油”的变量给剔除掉。</p><p>完全共线性：在多元回归中，一个自变量是一个或多个其他自变量的线性函数。</p><p>两者在某种特殊情况下是有交集的。</p><h2 id="3）T检验"><a href="#3）T检验" class="headerlink" title="3）T检验"></a>3）T检验</h2><p>T检验又叫student T 检验，主要用于样本含量小，总标准差 $\sigma$ 未知的正太分布数据。T检验是用于小样本的两个平均值差异程度的检查方法，他是用T分布理论值来推断事件发生的概率，从而判断两个平均数的差异是否显著。<br>参考: <a href="http://blog.csdn.net/shulixu/article/details/53354206" target="_blank" rel="external">http://blog.csdn.net/shulixu/article/details/53354206</a></p><h2 id="4）关系"><a href="#4）关系" class="headerlink" title="4）关系"></a>4）关系</h2><ul><li>函数关系<blockquote><p>确定性关系，y=3+2x</p></blockquote></li><li>相关关系<blockquote><p>非确定性关系，比如说高中时数学成绩好的人，一般物理成绩也好，这是因为它们背后使用的都是数学逻辑，这种酒叫做非确定性关系。</p></blockquote></li></ul><h2 id="5）虚拟变量"><a href="#5）虚拟变量" class="headerlink" title="5）虚拟变量"></a>5）虚拟变量</h2><p>定义：</p><blockquote><p>又称虚设变量、名义变量或哑变量，用以反映质的属性的一个人工变量，是量化了的自变量，通常取值为0或1。（通常为离散变量，因子变量）</p></blockquote><p>作用：</p><blockquote><p>引入哑变量可使线形回归模型变得更复杂，但对问题描述更简明，一个方程能达到两个方程的作用，而且接近现实。</p></blockquote><p>设置：</p><blockquote><p>例如：体重（w）和身高（h），性别（s）的关系，但这里性别并非连续的或者数字可以表示的变量，你并不能拿 1表示男，2表示女，这里的性别是离散变量，只能为男或者女，所以这里就需要引入哑变量来处理。<br>性别（s） =》 isman（男1，非男0），iswoman （因为只有两种可能，所以这里只需要引入一个哑变量即可），同理假设这里有另外一个变量肤色（有黑，白，黄三种可能），那么这里只需引入两个哑变量即可（isblack，iswhite），因为不是这两种的话那肯定是黄色皮肤了。</p></blockquote><p>例子：<br>针对上边所说的体重和身高，性别的关系。</p><p>构建模型：</p><ul><li>1）加法模型<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = a + b * h + c * isman</span><br></pre></td></tr></table></figure></li></ul><p>针对数据样本而言，性别是确定的，所以 c * isman 的结果不是c就是0，所以在加法模型下，影响的是模型在y轴上的截距。这说明的是针对不同的性别而言，回归方程是平衡的，只不过是截距不一样。</p><ul><li>2）乘法模型<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = a + b * h + c * isman * h + d * iswoman * h</span><br></pre></td></tr></table></figure></li></ul><p>同样针对数据样本而言，性别也是确定的，假设一个男性，isman 为1，iswoman 为0，则上述模型变成了 w = a + b<em>h + c </em> h =a + (b+c) * h，这个时候就是在y轴上的截距一样，而斜率不一致。</p><ul><li>3）混合模型<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = a + b * h + c * isman + d * iswoman + e * isman * h + f * iswoman * h</span><br></pre></td></tr></table></figure></li></ul><p>假设一个针对一个性别为男的样本数据，该模型变可以变成 w = a + b<em>h + c + e </em> h = a +c + (b+e)*h，这个时候斜率和截距都是不一样的。</p><h1 id="二：什么是回归（分析）"><a href="#二：什么是回归（分析）" class="headerlink" title="二：什么是回归（分析）"></a>二：什么是回归（分析）</h1><p>回归就是利用样本（已知数据），产生拟合方程，从而（对未知数据）进行预测。比如说我有一组随机变量X（X1，X2，X3…）和另外一组随机变量Y（Y1，Y2，Y3…）,那么研究变量X与Y之间的统计学方法就叫做回归分析。当然这里X和Y是单一对应的，所以这里是一元线性回归。</p><p>回归分为线性回归和非线性回归，其中一些非线性回归可以用线性回归的方法来进行分析的叫做==广义线性回归==，接下来我们来了解下每一种回归：</p><h2 id="1）线性回归"><a href="#1）线性回归" class="headerlink" title="1）线性回归"></a>1）线性回归</h2><p>线性回归可以分为一元线性回归和多元线性回归。当然线性回归中自变量的指数都是1，这里的线性并非真的是指用一条线将数据连起来，也可以是一个二维平面，三维平面等。</p><p>一元线性回归：自变量只有一个的回归，比如说北京二环的房子面积（Area）和房子总价（Money）的关系，随着面积（Area）的增大，房屋价格也是不断增长。这里的自变量只有面积，所以这里是一元线性回归。</p><p>多元线性回归：自变量大于等于两个，比如说北京二环的房子面积（Area），楼层（floor）和房屋价格（Money）的关系，这里自变量是两个，所以是二元线性回归，三元，多元同理。</p><h2 id="2）非线性回归"><a href="#2）非线性回归" class="headerlink" title="2）非线性回归"></a>2）非线性回归</h2><p>有一类模型，其回归参数不是线性的，也不能通过转换的方法将其变为线性的参数，这类模型称为非线性回归模型。非线性回归可以分为一元回归和多元回归。非线性回归中至少有一个自变量的指数不为1。回归分析中，当研究的因果关系只涉及因变量和一个自变量时，叫做一元回归分析；当研究的因果关系涉及因变量和两个或两个以上自变量时，叫做多元回归分析。</p><h2 id="3）广义线性回归"><a href="#3）广义线性回归" class="headerlink" title="3）广义线性回归"></a>3）广义线性回归</h2><p>一些非线性回归可以用线性回归的方法来进行分析叫做广义线性回归。<br>典型的代表是Logistic回归。</p><h2 id="4）如何衡量相关关系既判断适不适合使用线性回归模型？"><a href="#4）如何衡量相关关系既判断适不适合使用线性回归模型？" class="headerlink" title="4）如何衡量相关关系既判断适不适合使用线性回归模型？"></a>4）如何衡量相关关系既判断适不适合使用线性回归模型？</h2><p>使用相关系数（-1，1），绝对值越接近于1，相关系数越高，越适合使用线性回归模型（Rxy&gt;0,代表正相关，Rxy&lt;0,代表负相关）</p><script type="math/tex; mode=display">r_{XY} = \frac{ \sum (X_{i}-\bar{X})(Y_{i}-\bar{Y}) }{ \sqrt{ \sum (X_{i}-\bar{X})^2) \sum (Y_{i}-\bar{Y})^2) } }</script><h1 id="三：回归中困难点"><a href="#三：回归中困难点" class="headerlink" title="三：回归中困难点"></a>三：回归中困难点</h1><h2 id="1）选定变量"><a href="#1）选定变量" class="headerlink" title="1）选定变量"></a>1）选定变量</h2><blockquote><p>假设自变量特别多，有一些是和因变量相关的，有一些是和因变量不相关的，这里我们就需要筛选出有用的变量，如果筛选后变量还特别多的话，可以采用降维的方式进行变量缩减（可以参考之前的PCA降维的文章：<a href="http://blog.csdn.net/gamer_gyt/article/details/51418069" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt/article/details/51418069</a> ，基本是整理《机器学习实战》这本书的笔记）</p></blockquote><h2 id="2）发现多重共线性"><a href="#2）发现多重共线性" class="headerlink" title="2）发现多重共线性"></a>2）发现多重共线性</h2><p>(1).方差扩大因子法( VIF)</p><blockquote><p>一般认为如果最大的VIF超过10，常常表示存在多重共线性。</p></blockquote><p>(2).容差容忍定法</p><blockquote><p>如果容差（tolerance）&lt;=0.1，常常表示存在多重共线性。</p></blockquote><p>(3). 条件索引</p><blockquote><p>条件索引(condition index)&gt;10，可以说明存在比较严重的共线性</p></blockquote><h2 id="3）过拟合与欠拟合问题"><a href="#3）过拟合与欠拟合问题" class="headerlink" title="3）过拟合与欠拟合问题"></a>3）过拟合与欠拟合问题</h2><p>过拟合和欠拟合其实对每一个模型来讲都是存在的，过拟合就是模型过于符合训练数据的趋势，欠拟合就是模型对于训练数据和测试数据都表现出不好的情况。针对于欠拟合来讲，是很容易发现的，通常不被讨论。</p><p>在进行模型训练的时候，算法要进行不断的学习，模型在训练数据和测试数据上的错误都在不断下降，但是，如果学习的时间过长的话，模型在训练数据集上的表现将会继续下降，这是因为模型已经过拟合，并且学习到了训练数据集中不恰当的细节和噪音，同时，测试集上的错误率开始上升，也是模型泛化能力在下降。</p><p>这个完美的临界点就在于测试集中的错误率在上升时，此时训练集和测试集上都有良好的表现。通常有两种手段可以帮助你找到这个完美的临界点：重采样方法和验证集方法。</p><h3 id="如何限制过拟合？"><a href="#如何限制过拟合？" class="headerlink" title="如何限制过拟合？"></a>如何限制过拟合？</h3><blockquote><p>过拟合和欠拟合可以导致很差的模型表现。但是到目前为止大部分机器学习实际应用时的问题都是过拟合。<br>过拟合是个问题因为训练数据上的机器学习算法的评价方法与我们最关心的实际上的评价方法，也就是算法在位置数据上的表现是不一样的。<br>当评价机器学习算法时我们有两者重要的技巧来限制过拟合<br>使用重采样来评价模型效能<br>保留一个验证数据集<br>最流行的重采样技术是k折交叉验证。指的是在训练数据的子集上训练和测试模型k次，同时建立对于机器学习模型在未知数据上表现的评估。<br>验证集只是训练数据的子集，你把它保留到你进行机器学习算法的最后才使用。在训练数据上选择和调谐机器学习算法之后，我们在验证集上在对于模型进行评估，以便得到一些关于模型在未知数据上的表现的认知。</p></blockquote><h2 id="4）检验模型是否合理"><a href="#4）检验模型是否合理" class="headerlink" title="4）检验模型是否合理"></a>4）检验模型是否合理</h2><p>验证目前主要采用如下三类办法：<br>1、拟合优度检验<br>主要有R^2，t检验，f检验等等<br>这三种检验为常规验证，只要在95%的置信度内满足即可说明拟合效果良好。<br>2、预测值和真实值比较<br>主要是差值和比值，一般差值和比值都不超过5%。<br>3、另外的办法<br>GEH方法最为常用。GEH是Geoffrey E. Havers于1970年左右提出的一种模型验证方法，其巧妙的运用一个拟定的公式和标准界定模型的拟合优劣。<br>GEH=(2(M-C)^2/(M+C))^(1/2)<br>其中M是预测值，C是实际观测值<br>如果GEH小于5，认为模型拟合效果良好，如果GEH在5-10之间，必须对数据不可靠需要进行检查，如果GEH大于10，说明数据存在问题的几率很高。<br><a href="http://blog.sina.com.cn/s/blog_66188c300100hl45.html" target="_blank" rel="external">http://blog.sina.com.cn/s/blog_66188c300100hl45.html</a></p><h2 id="5）线性回归的模型评判"><a href="#5）线性回归的模型评判" class="headerlink" title="5）线性回归的模型评判"></a>5）线性回归的模型评判</h2><ul><li>误差平方和（残差平方和）</li></ul><p>例如二维平面上的一点（x1，y1），经过线性回归模型预测其值为 y_1，那么预测模型的好与坏就是计算预测结果到直线的距离的大小，由于是一组数据，那么便是这一组数据的和。</p><p>点到直线的距离公式为： </p><script type="math/tex; mode=display"> \frac{\left | A_{x_{0}}+B_{y_{0}} +C \right |}{\sqrt{A^2 + B^2 }}</script><p>由于涉及到开方，在计算过程中十分不方便，所以这里转换为纵轴上的差值，即利用预测值与真实值的差进行累加求和，最小时即为最佳的线性回归模型，但是这里涉及到预测值与真实值的差可能为负数，所以这里用平方，所以最终的误差平方和为：</p><script type="math/tex; mode=display">RSS = \sum_{i=1}^{n}(y_{i}- \hat{y_{i}} )^2 = \sum_{i=1}^{n}[y_{i} - (\alpha +\beta x_{i})]^2</script><ul><li>AIC准则（赤池信息准则）<script type="math/tex; mode=display">AIC=n ln (RSSp/n)+2p</script>n为变量总个数，p为选出的变量个数，AIC越小越好</li></ul><hr><center>    <img src="/img/gzh.jpg" weight="250px" height="250px">    <br>打开微信扫一扫，关注微信公众号【数据与算法联盟】</center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;2015年的机器学习博客其实都是看《机器学习实战》这本书时学到的，说实话当时也是知其然，不知其所以然，以至于对其理解不深刻，好多细节和理论知识都搞的是乱七八糟，自从工作之后再去看一个算法，思考的比之前多了点，查看资料也比之前多了点，生怕理解错误，影响其他人，当然在理解的程度上还是不够深刻，这也是一个学习的过程吧，记录一下，欢迎指正。&lt;/p&gt;
&lt;p&gt;CSDN链接：&lt;a href=&quot;http://blog.csdn.net/gamer_gyt/article/details/78008144&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;点击阅读&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="回归分析" scheme="http://yoursite.com/tags/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/"/>
    
      <category term="正态分布" scheme="http://yoursite.com/tags/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>数据归一化和其在sklearn中的处理</title>
    <link href="http://yoursite.com/2017/09/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%95%B0%E6%8D%AE%E5%BD%92%E4%B8%80%E5%8C%96%E5%92%8C%E5%85%B6%E5%9C%A8sklearn%E4%B8%AD%E7%9A%84%E5%A4%84%E7%90%86/"/>
    <id>http://yoursite.com/2017/09/01/机器学习/数据归一化和其在sklearn中的处理/</id>
    <published>2017-09-01T03:33:50.000Z</published>
    <updated>2018-03-17T17:06:11.807Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：数据归一化"><a href="#一：数据归一化" class="headerlink" title="一：数据归一化"></a>一：数据归一化</h1><p>数据归一化（标准化）处理是数据挖掘的一项基础工作，不同评价指标往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果，为了消除指标之间的量纲影响，需要进行数据标准化处理，以解决数据指标之间的可比性。原始数据经过数据标准化处理后，各指标处于同一数量级，适合进行综合对比评价。<br><a id="more"></a><br>归一化方法有两种形式，一种是把数变为（0，1）之间的小数，一种是把有量纲表达式变为无量纲表达式。在机器学习中我们更关注的把数据变到0～1之间，接下来我们讨论的也是第一种形式。</p><h2 id="1）min-max标准化"><a href="#1）min-max标准化" class="headerlink" title="1）min-max标准化"></a>1）min-max标准化</h2><p>min-max标准化也叫做离差标准化，是对原始数据的线性变换，使结果落到[0,1]区间，其对应的数学公式如下：</p><script type="math/tex; mode=display">X_{scale} = \frac{x-min}{max-min}</script><p>对应的python实现为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># x为数据 比如说 [1,2,1,3,2,4,1]</span><br><span class="line">def Normalization(x):</span><br><span class="line">    return [(float(i)-min(x))/float(max(x)-min(x)) for i in x]</span><br></pre></td></tr></table></figure></p><p>如果要将数据转换到[-1,1]之间，可以修改其数学公式为：</p><script type="math/tex; mode=display">X_{scale} = \frac{x-x_{mean}}{max-min}</script><p>x_mean 表示平均值。</p><p>对应的python实现为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># x为数据 比如说 [1,2,1,3,2,4,1]</span><br><span class="line">def Normalization(x):</span><br><span class="line">    return [(float(i)-np.mean(x))/float(max(x)-min(x)) for i in x]</span><br></pre></td></tr></table></figure></p><p>其中max为样本数据的最大值，min为样本数据的最小值。这种方法有个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义。</p><p>该标准化方法有一个缺点就是，如果数据中有一些偏离正常数据的异常点，就会导致标准化结果的不准确性。比如说一个公司员工（A，B，C，D）的薪水为6k,8k,7k,10w,这种情况下进行归一化对每个员工来讲都是不合理的。</p><p>当然还有一些其他的办法也能实现数据的标准化。</p><h2 id="2）z-score标准化"><a href="#2）z-score标准化" class="headerlink" title="2）z-score标准化"></a>2）z-score标准化</h2><p>z-score标准化也叫标准差标准化，代表的是分值偏离均值的程度，经过处理的数据符合标准正态分布，即均值为0，标准差为1。其转化函数为</p><script type="math/tex; mode=display">X_{scale} = \frac{x-\mu }{\sigma }</script><p>其中μ为所有样本数据的均值，σ为所有样本数据的标准差。</p><p>其对应的python实现为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">#x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br><span class="line">def z_score(x):</span><br><span class="line">    return (x - np.mean(x) )/np.std(x, ddof = 1)</span><br></pre></td></tr></table></figure></p><p>z-score标准化方法同样对于离群异常值的影响。接下来看一种改进的z-score标准化方法。</p><h2 id="3）改进的z-score标准化"><a href="#3）改进的z-score标准化" class="headerlink" title="3）改进的z-score标准化"></a>3）改进的z-score标准化</h2><p>将标准分公式中的均值改为中位数，将标准差改为绝对偏差。</p><script type="math/tex; mode=display">X_{scale} = \frac{x-x_{center} }{\sigma_{1} }</script><p>中位数是指将所有数据进行排序，取中间的那个值，如数据量是偶数，则取中间两个数据的平均值。</p><p>σ1为所有样本数据的绝对偏差,其计算公式为：</p><script type="math/tex; mode=display">\frac{1}{N} \sum_{1}^{n}|x_{i} - x_{center}|</script><hr><h1 id="二：sklearn中的归一化"><a href="#二：sklearn中的归一化" class="headerlink" title="二：sklearn中的归一化"></a>二：sklearn中的归一化</h1><p>sklearn.preprocessing 提供了一些实用的函数 用来处理数据的维度，以供算法使用。</p><h2 id="1）均值-标准差缩放"><a href="#1）均值-标准差缩放" class="headerlink" title="1）均值-标准差缩放"></a>1）均值-标准差缩放</h2><p>即我们上边对应的z-score标准化。<br>在sklearn的学习中，数据集的标准化是很多机器学习模型算法的常见要求。如果个别特征看起来不是很符合正态分布，那么他们可能为表现不好。</p><p>实际上，我们经常忽略分布的形状，只是通过减去整组数据的平均值，使之更靠近数据中心分布，然后通过将非连续数特征除以其标准偏差进行分类。</p><p>例如，用于学习算法（例如支持向量机的RBF内核或线性模型的l1和l2正则化器）的目标函数中使用的许多元素假设所有特征都以零为中心并且具有相同顺序的方差。如果特征的方差大于其他数量级，则可能主导目标函数，使估计器无法按预期正确地学习其他特征。</p><p>例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from sklearn import preprocessing</span><br><span class="line">&gt;&gt;&gt; import numpy as np</span><br><span class="line">&gt;&gt;&gt; X_train = np.array([[ 1., -1.,  2.],</span><br><span class="line">...                     [ 2.,  0.,  0.],</span><br><span class="line">...                     [ 0.,  1., -1.]])</span><br><span class="line">&gt;&gt;&gt; X_scaled = preprocessing.scale(X_train)</span><br><span class="line">&gt;&gt;&gt; X_scaled</span><br><span class="line">array([[ 0.        , -1.22474487,  1.33630621],</span><br><span class="line">       [ 1.22474487,  0.        , -0.26726124],</span><br><span class="line">       [-1.22474487,  1.22474487, -1.06904497]])</span><br></pre></td></tr></table></figure></p><p>标准化后的数据符合标准正太分布<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; X_scaled.mean(axis=0)</span><br><span class="line">array([ 0.,  0.,  0.])</span><br><span class="line">&gt;&gt;&gt; X_scaled.std(axis=0)</span><br><span class="line">array([ 1.,  1.,  1.])</span><br></pre></td></tr></table></figure></p><p>预处理模块还提供了一个实用程序级StandardScaler，它实现了Transformer API来计算训练集上的平均值和标准偏差，以便能够稍后在测试集上重新应用相同的变换。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; scaler = preprocessing.StandardScaler().fit(X_train)</span><br><span class="line">&gt;&gt;&gt; scaler</span><br><span class="line">StandardScaler(copy=True, with_mean=True, with_std=True)</span><br><span class="line">&gt;&gt;&gt; scaler.mean_</span><br><span class="line">array([ 1.        ,  0.        ,  0.33333333])</span><br><span class="line">&gt;&gt;&gt; scaler.scale_</span><br><span class="line">array([ 0.81649658,  0.81649658,  1.24721913])</span><br><span class="line">&gt;&gt;&gt; scaler.transform(X_train)</span><br><span class="line">array([[ 0.        , -1.22474487,  1.33630621],</span><br><span class="line">       [ 1.22474487,  0.        , -0.26726124],</span><br><span class="line">       [-1.22474487,  1.22474487, -1.06904497]])</span><br></pre></td></tr></table></figure></p><p>使用转换器可以对新数据进行转换<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; X_test = [[-1., 1., 0.]]</span><br><span class="line">&gt;&gt;&gt; scaler.transform(X_test)</span><br><span class="line">array([[-2.44948974,  1.22474487, -0.26726124]])</span><br></pre></td></tr></table></figure></p><h2 id="2）min-max标准化"><a href="#2）min-max标准化" class="headerlink" title="2）min-max标准化"></a>2）min-max标准化</h2><p>X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt;&gt;&gt; X_train = np.array([[ 1., -1.,  2.],</span><br><span class="line">...                      [ 2.,  0.,  0.],</span><br><span class="line">...                      [ 0.,  1., -1.]])</span><br><span class="line">&gt;&gt;&gt; min_max_scaler = preprocessing.MinMaxScaler()</span><br><span class="line">&gt;&gt;&gt; X_train_minmax = min_max_scaler.fit_transform(X_train)</span><br><span class="line">&gt;&gt;&gt; X_train_minmax</span><br><span class="line">array([[ 0.5       ,  0.        ,  1.        ],</span><br><span class="line">       [ 1.        ,  0.5       ,  0.33333333],</span><br><span class="line">       [ 0.        ,  1.        ,  0.        ]])</span><br></pre></td></tr></table></figure><p>上边我们创建的min_max_scaler 同样适用于新的测试数据<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; X_test = np.array([[ -3., -1.,  4.]])</span><br><span class="line">&gt;&gt;&gt; X_test_minmax = min_max_scaler.transform(X_test)</span><br><span class="line">&gt;&gt;&gt; X_test_minmax</span><br><span class="line">array([[-1.5       ,  0.        ,  1.66666667]])</span><br></pre></td></tr></table></figure></p><p>可以通过scale_和min方法查看标准差和最小值<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; min_max_scaler.scale_ </span><br><span class="line">array([ 0.5       ,  0.5       ,  0.33333333])</span><br><span class="line">&gt;&gt;&gt; min_max_scaler.min_</span><br><span class="line">array([ 0.        ,  0.5       ,  0.33333333])</span><br></pre></td></tr></table></figure></p><h2 id="3）最大值标准化"><a href="#3）最大值标准化" class="headerlink" title="3）最大值标准化"></a>3）最大值标准化</h2><p>对于每个数值／每个维度的最大值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; X_train</span><br><span class="line">array([[ 1., -1.,  2.],</span><br><span class="line">       [ 2.,  0.,  0.],</span><br><span class="line">       [ 0.,  1., -1.]])</span><br><span class="line">&gt;&gt;&gt; max_abs_scaler = preprocessing.MaxAbsScaler()</span><br><span class="line">&gt;&gt;&gt; X_train_maxabs = max_abs_scaler.fit_transform(X_train)</span><br><span class="line">&gt;&gt;&gt; X_train_maxabs</span><br><span class="line">array([[ 0.5, -1. ,  1. ],</span><br><span class="line">       [ 1. ,  0. ,  0. ],</span><br><span class="line">       [ 0. ,  1. , -0.5]])</span><br><span class="line">&gt;&gt;&gt; X_test = np.array([[ -3., -1.,  4.]])</span><br><span class="line">&gt;&gt;&gt; X_test_maxabs = max_abs_scaler.transform(X_test)</span><br><span class="line">&gt;&gt;&gt; X_test_maxabs                 </span><br><span class="line">array([[-1.5, -1. ,  2. ]])</span><br><span class="line">&gt;&gt;&gt; max_abs_scaler.scale_         </span><br><span class="line">array([ 2.,  1.,  2.])</span><br></pre></td></tr></table></figure><h2 id="4）规范化"><a href="#4）规范化" class="headerlink" title="4）规范化"></a>4）规范化</h2><p>规范化是文本分类和聚类中向量空间模型的基础</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; X = [[ 1., -1.,  2.],</span><br><span class="line">...      [ 2.,  0.,  0.],</span><br><span class="line">...      [ 0.,  1., -1.]]</span><br><span class="line">&gt;&gt;&gt; X_normalized = preprocessing.normalize(X, norm=&apos;l2&apos;)</span><br><span class="line">&gt;&gt;&gt; X_normalized</span><br><span class="line">array([[ 0.40824829, -0.40824829,  0.81649658],</span><br><span class="line">       [ 1.        ,  0.        ,  0.        ],</span><br><span class="line">       [ 0.        ,  0.70710678, -0.70710678]])</span><br></pre></td></tr></table></figure><p>解释：norm 该参数是可选的，默认值是l2（向量各元素的平方和然后求平方根），用来规范化每个非零向量，如果axis参数设置为0，则表示的是规范化每个非零的特征维度。</p><p>机器学习中的范数规则：<a href="http://blog.csdn.net/zouxy09/article/details/24971995/" target="_blank" rel="external">点击阅读</a><br><br>其他对应参数：<a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html#sklearn.preprocessing.normalize" target="_blank" rel="external">点击查看</a></p><p>preprocessing模块提供了训练种子的功能，我们可通过以下方式得到一个新的种子，并对新数据进行规范化处理。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; normalizer = preprocessing.Normalizer().fit(X)</span><br><span class="line">&gt;&gt;&gt; normalizer</span><br><span class="line">Normalizer(copy=True, norm=&apos;l2&apos;)</span><br><span class="line">&gt;&gt;&gt; normalizer.transform(X)</span><br><span class="line">array([[ 0.40824829, -0.40824829,  0.81649658],</span><br><span class="line">       [ 1.        ,  0.        ,  0.        ],</span><br><span class="line">       [ 0.        ,  0.70710678, -0.70710678]])</span><br><span class="line">&gt;&gt;&gt; normalizer.transform([[-1,1,0]])</span><br><span class="line">array([[-0.70710678,  0.70710678,  0.        ]])</span><br></pre></td></tr></table></figure></p><h2 id="5）二值化"><a href="#5）二值化" class="headerlink" title="5）二值化"></a>5）二值化</h2><p>将数据转换到0-1 之间<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; X</span><br><span class="line">[[1.0, -1.0, 2.0], [2.0, 0.0, 0.0], [0.0, 1.0, -1.0]]</span><br><span class="line">&gt;&gt;&gt; binarizer = preprocessing.Binarizer().fit(X)</span><br><span class="line">&gt;&gt;&gt; binarizer</span><br><span class="line">Binarizer(copy=True, threshold=0.0)</span><br><span class="line">&gt;&gt;&gt; binarizer.transform(X)</span><br><span class="line">array([[ 1.,  0.,  1.],</span><br><span class="line">       [ 1.,  0.,  0.],</span><br><span class="line">       [ 0.,  1.,  0.]])</span><br></pre></td></tr></table></figure></p><p>可以调整二值化的门阀<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; binarizer = preprocessing.Binarizer(threshold=1.1)</span><br><span class="line">&gt;&gt;&gt; binarizer.transform(X)</span><br><span class="line">array([[ 0.,  0.,  1.],</span><br><span class="line">       [ 1.,  0.,  0.],</span><br><span class="line">       [ 0.,  0.,  0.]])</span><br></pre></td></tr></table></figure></p><h2 id="6）编码的分类特征"><a href="#6）编码的分类特征" class="headerlink" title="6）编码的分类特征"></a>6）编码的分类特征</h2><p>通常情况下，特征不是作为连续值给定的。例如一个人可以有<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&quot;male&quot;, &quot;female&quot;], [&quot;from Europe&quot;, &quot;from US&quot;, &quot;from Asia&quot;], [&quot;uses Firefox&quot;, &quot;uses Chrome&quot;, &quot;uses Safari&quot;, &quot;uses Internet Explorer&quot;]</span><br></pre></td></tr></table></figure></p><p>这些特征可以被有效的编码为整数，例如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[&quot;male&quot;, &quot;from US&quot;, &quot;uses Internet Explorer&quot;] =&gt; [0, 1, 3]</span><br><span class="line">[&quot;female&quot;, &quot;from Asia&quot;, &quot;uses Chrome&quot;] would be [1, 2, 1].</span><br></pre></td></tr></table></figure></p><p>这样的整数不应该直接应用到scikit的算法中，可以通过one-of-k或者独热编码（OneHotEncorder），该种处理方式会把每个分类特征的m中可能值转换成m个二进制值。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; enc = preprocessing.OneHotEncoder()</span><br><span class="line">&gt;&gt;&gt; enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])</span><br><span class="line">OneHotEncoder(categorical_features=&apos;all&apos;, dtype=&lt;class &apos;numpy.float64&apos;&gt;,</span><br><span class="line">       handle_unknown=&apos;error&apos;, n_values=&apos;auto&apos;, sparse=True)</span><br><span class="line">&gt;&gt;&gt; enc.transform([[0,1,3]]).toarray()</span><br><span class="line">array([[ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.]])</span><br></pre></td></tr></table></figure><p>默认情况下，从数据集中自动推断出每个特征可以带多少个值。可以明确指定使用的参数n_values。在我们的数据集中有两种性别，三种可能的大陆和四种Web浏览器。然后，我们拟合估计量，并转换一个数据点。在结果中，前两个数字编码性别，下一组三个数字的大陆和最后四个Web浏览器。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; enc = preprocessing.OneHotEncoder(n_values=[2,3,4])</span><br><span class="line">&gt;&gt;&gt; enc.fit([[1,2,3],[0,2,0]])</span><br><span class="line">OneHotEncoder(categorical_features=&apos;all&apos;, dtype=&lt;class &apos;numpy.float64&apos;&gt;,</span><br><span class="line">       handle_unknown=&apos;error&apos;, n_values=[2, 3, 4], sparse=True)</span><br><span class="line">&gt;&gt;&gt; enc.transform([[1,0,0]]).toarray()</span><br><span class="line">array([[ 0.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,  0.]])</span><br></pre></td></tr></table></figure></p><h2 id="7）填补缺失值"><a href="#7）填补缺失值" class="headerlink" title="7）填补缺失值"></a>7）填补缺失值</h2><p>由于各种原因，真实数据中存在大量的空白值，这样的数据集，显然是不符合scikit的要求的，那么preprocessing模块提供这样一个功能，利用已知的数据来填补这些空白。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import numpy as np</span><br><span class="line">&gt;&gt;&gt; from sklearn.preprocessing import Imputer</span><br><span class="line">&gt;&gt;&gt; imp = Imputer(missing_values=&apos;NaN&apos;,strategy=&apos;mean&apos;,verbose=0)</span><br><span class="line">&gt;&gt;&gt; imp.fit([[1, 2], [np.nan, 3], [7, 6]])</span><br><span class="line">Imputer(axis=0, copy=True, missing_values=&apos;NaN&apos;, strategy=&apos;mean&apos;, verbose=0)</span><br><span class="line">&gt;&gt;&gt; X = [[np.nan, 2], [6, np.nan], [7, 6]]</span><br><span class="line">&gt;&gt;&gt; print(imp.transform(X))                           </span><br><span class="line">[[ 4.          2.        ]</span><br><span class="line"> [ 6.          3.66666667]</span><br><span class="line"> [ 7.          6.        ]]</span><br></pre></td></tr></table></figure></p><p>Imputer同样支持稀疏矩阵<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import scipy.sparse as sp</span><br><span class="line">&gt;&gt;&gt; X = sp.csc_matrix([[1,2],[0,3],[7,6]])</span><br><span class="line">&gt;&gt;&gt; imp = Imputer(missing_values=0,strategy=&apos;mean&apos;,axis=0)</span><br><span class="line">&gt;&gt;&gt; imp.fit(X)</span><br><span class="line">Imputer(axis=0, copy=True, missing_values=0, strategy=&apos;mean&apos;, verbose=0)</span><br><span class="line">&gt;&gt;&gt; X_test = sp.csc</span><br><span class="line">sp.csc          sp.csc_matrix(  </span><br><span class="line">&gt;&gt;&gt; X_test = sp.csc_matrix([[0,2],[6,0],[7,6]])</span><br><span class="line">&gt;&gt;&gt; print(imp.transform(X_test))</span><br><span class="line">[[ 4.          2.        ]</span><br><span class="line"> [ 6.          3.66666667]</span><br><span class="line"> [ 7.          6.        ]]</span><br></pre></td></tr></table></figure></p><h2 id="8）生成多项式特征"><a href="#8）生成多项式特征" class="headerlink" title="8）生成多项式特征"></a>8）生成多项式特征</h2><p>通常，通过考虑输入数据的非线性特征来增加模型的复杂度是很有用的。一个简单而常用的方法是多项式特征，它可以得到特征的高阶和相互作用项。</p><p>其遵循的原则是 </p><script type="math/tex; mode=display">(X_1, X_2) -> (1, X_1, X_2, X_1^2, X_1X_2, X_2^2)</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import numpy as np</span><br><span class="line">&gt;&gt;&gt; from sklearn.preprocessing import PolynomialFeatures</span><br><span class="line">&gt;&gt;&gt; X = np.arange(6).reshape(3, 2)</span><br><span class="line">&gt;&gt;&gt; X                                                 </span><br><span class="line">array([[0, 1],</span><br><span class="line">       [2, 3],</span><br><span class="line">       [4, 5]])</span><br><span class="line">&gt;&gt;&gt; poly = PolynomialFeatures(2)</span><br><span class="line">&gt;&gt;&gt; poly.fit_transform(X)                             </span><br><span class="line">array([[  1.,   0.,   1.,   0.,   0.,   1.],</span><br><span class="line">       [  1.,   2.,   3.,   4.,   6.,   9.],</span><br><span class="line">       [  1.,   4.,   5.,  16.,  20.,  25.]])</span><br></pre></td></tr></table></figure><p>有些情况下，有相互关系的标签才是必须的，这个时候可以通过设置 interaction_only=True 来进行多项式特征的生成<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; X = np.arange(9).reshape(3, 3)</span><br><span class="line">&gt;&gt;&gt; X                                                 </span><br><span class="line">array([[0, 1, 2],</span><br><span class="line">       [3, 4, 5],</span><br><span class="line">       [6, 7, 8]])</span><br><span class="line">&gt;&gt;&gt; poly = PolynomialFeatures(degree=3, interaction_only=True)</span><br><span class="line">&gt;&gt;&gt; poly.fit_transform(X)                             </span><br><span class="line">array([[   1.,    0.,    1.,    2.,    0.,    0.,    2.,    0.],</span><br><span class="line">       [   1.,    3.,    4.,    5.,   12.,   15.,   20.,   60.],</span><br><span class="line">       [   1.,    6.,    7.,    8.,   42.,   48.,   56.,  336.]])</span><br></pre></td></tr></table></figure></p><p>其遵循的规则是：</p><script type="math/tex; mode=display">(X_1, X_2, X_3) -> (1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)</script><hr><p>对应的scikit-learn资料为： <a href="http://scikit-learn.org/stable/modules/preprocessing.html" target="_blank" rel="external">http://scikit-learn.org/stable/modules/preprocessing.html</a></p><hr><center>    <img src="/img/gzh.jpg" weight="250px" height="250px">    <br>打开微信扫一扫，关注微信公众号【数据与算法联盟】</center>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一：数据归一化&quot;&gt;&lt;a href=&quot;#一：数据归一化&quot; class=&quot;headerlink&quot; title=&quot;一：数据归一化&quot;&gt;&lt;/a&gt;一：数据归一化&lt;/h1&gt;&lt;p&gt;数据归一化（标准化）处理是数据挖掘的一项基础工作，不同评价指标往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果，为了消除指标之间的量纲影响，需要进行数据标准化处理，以解决数据指标之间的可比性。原始数据经过数据标准化处理后，各指标处于同一数量级，适合进行综合对比评价。&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="sklearn" scheme="http://yoursite.com/tags/sklearn/"/>
    
      <category term="数据归一化" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BD%92%E4%B8%80%E5%8C%96/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
